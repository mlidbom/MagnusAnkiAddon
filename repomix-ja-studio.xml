This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*py, **/*.pyi
- Files matching these patterns are excluded: _lib, user_files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
__init__.py
anki_extentions/card_ex.py
anki_extentions/config_manager_ex.py
anki_extentions/deck_configdict_ex.py
anki_extentions/deck_ex.py
anki_extentions/note_ex.py
anki_extentions/notetype_ex/note_type_ex.py
anki_extentions/notetype_ex/note_type_field.py
anki_extentions/notetype_ex/note_type_template.py
anki_extentions/sheduling_states_ex.py
ankiutils/anki_module_import_issues_fix_just_import_this_module_before_any_other_anki_modules.py
ankiutils/app.py
ankiutils/audio_suppressor.py
ankiutils/query_builder.py
ankiutils/search_executor.py
ankiutils/ui_utils_interface.py
ankiutils/ui_utils.py
batches/local_note_updater.py
configuration/configuration_value.py
configuration/configuration.py
configuration/readings_mapping_dialog.py
ex_autoslot.py
language_services/conjugator.py
language_services/english_dictionary/english_dict_search.py
language_services/hiragana_chart.py
language_services/jamdict_ex/dict_entry.py
language_services/jamdict_ex/dict_lookup.py
language_services/jamdict_ex/priority_spec.py
language_services/janome_ex/tokenizing/inflection_forms.py
language_services/janome_ex/tokenizing/inflection_types.py
language_services/janome_ex/tokenizing/jn_parts_of_speech.py
language_services/janome_ex/tokenizing/jn_token.py
language_services/janome_ex/tokenizing/jn_tokenized_text.py
language_services/janome_ex/tokenizing/jn_tokenizer.py
language_services/janome_ex/word_extraction/analysis_constants.py
language_services/janome_ex/word_extraction/candidate_word_variant.py
language_services/janome_ex/word_extraction/candidate_word.py
language_services/janome_ex/word_extraction/matches/dictionary_match.py
language_services/janome_ex/word_extraction/matches/match.py
language_services/janome_ex/word_extraction/matches/missing_match.py
language_services/janome_ex/word_extraction/matches/requirements/forbids_state.py
language_services/janome_ex/word_extraction/matches/requirements/requirement.py
language_services/janome_ex/word_extraction/matches/requirements/requires_forbids_requirement.py
language_services/janome_ex/word_extraction/matches/requirements/requires_state.py
language_services/janome_ex/word_extraction/matches/state_tests/another_match_owns_the_form.py
language_services/janome_ex/word_extraction/matches/state_tests/head/has_a_stem.py
language_services/janome_ex/word_extraction/matches/state_tests/head/has_e_stem.py
language_services/janome_ex/word_extraction/matches/state_tests/head/has_past_tense_stem.py
language_services/janome_ex/word_extraction/matches/state_tests/head/has_te_form_stem.py
language_services/janome_ex/word_extraction/matches/state_tests/head/is_sentence_start.py
language_services/janome_ex/word_extraction/matches/state_tests/head/prefix_is_in.py
language_services/janome_ex/word_extraction/matches/state_tests/is_configured_hidden.py
language_services/janome_ex/word_extraction/matches/state_tests/is_configured_incorrect.py
language_services/janome_ex/word_extraction/matches/state_tests/is_exact_match.py
language_services/janome_ex/word_extraction/matches/state_tests/is_poison_word.py
language_services/janome_ex/word_extraction/matches/state_tests/is_shadowed.py
language_services/janome_ex/word_extraction/matches/state_tests/is_single_token.py
language_services/janome_ex/word_extraction/matches/state_tests/match_state_test.py
language_services/janome_ex/word_extraction/matches/state_tests/surface_is_in.py
language_services/janome_ex/word_extraction/matches/state_tests/tail/has_overlapping_following_compound.py
language_services/janome_ex/word_extraction/matches/state_tests/tail/is_sentence_end.py
language_services/janome_ex/word_extraction/matches/state_tests/tail/suffix_is_in.py
language_services/janome_ex/word_extraction/matches/state_tests/vocab_match_state_test.py
language_services/janome_ex/word_extraction/matches/vocab_match.py
language_services/janome_ex/word_extraction/text_analysis.py
language_services/janome_ex/word_extraction/text_location.py
language_services/janome_ex/word_extraction/word_exclusion.py
language_services/katakana_chart.py
line_profiling_hacks.py
manually_copied_in_libraries/autoslot.py
mylog.py
note/cardutils.py
note/collection/backend_facade.py
note/collection/cache_runner.py
note/collection/jp_collection.py
note/collection/kanji_collection.py
note/collection/note_cache.py
note/collection/sentence_collection.py
note/collection/vocab_collection.py
note/difficulty_calculator.py
note/jpnote.py
note/kanjinote_mnemonic_maker.py
note/kanjinote.py
note/note_constants.py
note/note_flush_guard.py
note/notefields/audio_field.py
note/notefields/auto_save_wrappers/field_wrapper.py
note/notefields/auto_save_wrappers/set_wrapper.py
note/notefields/auto_save_wrappers/value_wrapper.py
note/notefields/comma_separated_strings_list_field_de_duplicated.py
note/notefields/comma_separated_strings_list_field.py
note/notefields/comma_separated_strings_set_field.py
note/notefields/fallback_string_field.py
note/notefields/integer_field.py
note/notefields/json_object_field.py
note/notefields/mutable_string_field.py
note/notefields/require_forbid_flag_field.py
note/notefields/sentence_question_field.py
note/notefields/strip_html_on_read_fallback_string_field.py
note/notefields/tag_flag_field.py
note/noteutils.py
note/queue_manager.py
note/sentences/caching_sentence_configuration_field.py
note/sentences/parsed_word.py
note/sentences/parsing_result.py
note/sentences/sentence_configuration.py
note/sentences/sentencenote.py
note/sentences/serialization/parsed_word_serializer.py
note/sentences/serialization/parsing_result_serializer.py
note/sentences/serialization/sentence_configuration_serializer.py
note/sentences/user_fields.py
note/sentences/word_exclusion_set.py
note/vocabnote_cloner.py
note/vocabulary/related_vocab/Antonyms.py
note/vocabulary/related_vocab/ergative_twin.py
note/vocabulary/related_vocab/perfect_synonyms.py
note/vocabulary/related_vocab/related_vocab_data_serializer.py
note/vocabulary/related_vocab/related_vocab_data.py
note/vocabulary/related_vocab/related_vocab.py
note/vocabulary/related_vocab/SeeAlso.py
note/vocabulary/related_vocab/Synonyms.py
note/vocabulary/serialization/matching_rules_serializer.py
note/vocabulary/vocabnote_audio.py
note/vocabulary/vocabnote_conjugator.py
note/vocabulary/vocabnote_factory.py
note/vocabulary/vocabnote_forms.py
note/vocabulary/vocabnote_generated_data.py
note/vocabulary/vocabnote_kanji.py
note/vocabulary/vocabnote_matching_rules_is_inflecting_word.py
note/vocabulary/vocabnote_matching_rules_yield_last_token_to_next_compound.py
note/vocabulary/vocabnote_matching_rules.py
note/vocabulary/vocabnote_meta_tag.py
note/vocabulary/vocabnote_metadata.py
note/vocabulary/vocabnote_parts_of_speech.py
note/vocabulary/vocabnote_question.py
note/vocabulary/vocabnote_sentences.py
note/vocabulary/vocabnote_sorting.py
note/vocabulary/vocabnote_usercompoundparts.py
note/vocabulary/vocabnote_userfields.py
note/vocabulary/vocabnote.py
qt_utils/ex_qmenu.py
qt_utils/task_runner_progress_dialog.py
sysutils/app_thread_pool.py
sysutils/collections/default_dict_case_insensitive.py
sysutils/collections/recent_items.py
sysutils/debug_repr_builder.py
sysutils/ex_assert.py
sysutils/ex_gc.py
sysutils/ex_lambda.py
sysutils/ex_str.py
sysutils/ex_thread.py
sysutils/json/ex_json.py
sysutils/json/json_library_shim_builtin.py
sysutils/json/json_library_shim_orjson.py
sysutils/json/json_library_shim.py
sysutils/json/json_reader.py
sysutils/kana_utils.py
sysutils/lazy.py
sysutils/object_instance_tracker.py
sysutils/progress_display_runner.py
sysutils/simple_string_builder.py
sysutils/simple_string_list_builder.py
sysutils/standard_type_aliases.py
sysutils/timeutil.py
sysutils/typed.py
sysutils/weak_ref.py
testutils/ex_pytest.py
ui/__init__.py
ui/english_dict/find_english_words_dialog.py
ui/garbage_collection_fixes.py
ui/hooks/__init__.py
ui/hooks/clear_studying_cache_on_card_suspend_unsuspend.py
ui/hooks/copy_sort_field_to_clipboard.py
ui/hooks/custom_auto_advance_timings.py
ui/hooks/custom_short_term_scheduling.py
ui/hooks/custom_timebox_lengths.py
ui/hooks/global_shortcuts.py
ui/hooks/history_navigator.py
ui/hooks/no_accidental_double_click.py
ui/hooks/note_view_shortcuts.py
ui/hooks/timebox_end_sound.py
ui/menus/__init__.py
ui/menus/browser/__init__.py
ui/menus/browser/main.py
ui/menus/common.py
ui/menus/menu_utils/ex_qmenu.py
ui/menus/menu_utils/shortcutfinger.py
ui/menus/notes/__init__.py
ui/menus/notes/kanji/__init__.py
ui/menus/notes/kanji/main.py
ui/menus/notes/kanji/string_menu.py
ui/menus/notes/sentence/__init__.py
ui/menus/notes/sentence/main.py
ui/menus/notes/sentence/string_menu.py
ui/menus/notes/vocab/__init__.py
ui/menus/notes/vocab/common.py
ui/menus/notes/vocab/create_note_menu.py
ui/menus/notes/vocab/main.py
ui/menus/notes/vocab/matching_settings_menu.py
ui/menus/notes/vocab/string_menu.py
ui/menus/open_in_anki.py
ui/menus/web_search.py
ui/open_note/open_note_dialog.py
ui/timing_hacks.py
ui/tools_menu.py
ui/web/__init__.py
ui/web/kanji/__init__.py
ui/web/kanji/dependencies.py
ui/web/kanji/kanji_list.py
ui/web/kanji/mnemonic.py
ui/web/kanji/readings.py
ui/web/kanji/vocab_list.py
ui/web/sentence/__init__.py
ui/web/sentence/candidate_word_variant_viewmodel.py
ui/web/sentence/compound_part_viewmodel.py
ui/web/sentence/match_viewmodel.py
ui/web/sentence/question.py
ui/web/sentence/sentence_viewmodel.py
ui/web/sentence/text_analysis_viewmodel.py
ui/web/sentence/ud_sentence_breakdown.py
ui/web/vocab/__init__.py
ui/web/vocab/compound_parts.py
ui/web/vocab/related_vocabs.py
ui/web/vocab/vocab_kanji_list.py
ui/web/vocab/vocab_sentences_vocab_sentence_view_model.py
ui/web/vocab/vocab_sentences.py
ui/web/web_utils/content_renderer.py
viewmodels/kanji_list/kanji_list_viewmodel.py
viewmodels/kanji_list/sentence_kanji_list_viewmodel.py
viewmodels/kanji_list/sentence_kanji_viewmodel.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="__init__.py">
from __future__ import annotations

import os
import sys

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
import _lib # noqa NOTE: this line sets up lib paths, lib imports before here do not work when running in anki  # pyright: ignore[reportUnusedImport]

import ui # noqa
from ankiutils import app # noqa
if app.config().enable_automatic_garbage_collection.get_value():
    import gc
    gc.enable()

ui.init()
</file>

<file path="anki_extentions/card_ex.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from anki import consts
from anki_extentions.deck_ex import DeckEx
from ex_autoslot import AutoSlots
from queryablecollections.collections.q_list import QList
from sysutils.timeutil import StopWatch
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from anki.dbproxy import Row
    from anki.decks import DeckManager
    from anki.scheduler.v3 import Scheduler  # pyright: ignore[reportMissingTypeStubs]
    from anki_extentions.notetype_ex.note_type_template import NoteTemplateEx
    from note.jpnote import JPNote

import anki.cards
import anki.cards_pb2
from ankiutils import app
from aqt.reviewer import AnswerAction
from sysutils import timeutil, typed


def _latest_day_cutoff_timestamp() -> int:
    from aqt import mw
    return non_optional(mw.col).sched.day_cutoff - timeutil.SECONDS_PER_DAY

def _get_answers_since_last_day_cutoff_for_card(card_id: int) -> QList[int]:
    with StopWatch.log_warning_if_slower_than(0.01):
        reviews: list[Row] = app.anki_db().all("SELECT ease FROM revlog WHERE cid = ? AND id > ? ORDER BY id DESC", card_id, _latest_day_cutoff_timestamp() * timeutil.MILLISECONDS_PER_SECOND)
        return QList(typed.int_(review[0]) for review in reviews)  # pyright: ignore[reportAny]

class CardEx(AutoSlots):
    def __init__(self, card:anki.cards.Card) -> None:
        self.card:anki.cards.Card = card

    @staticmethod
    def _scheduler() -> Scheduler:
        return app.anki_scheduler()

    def is_suspended(self) -> bool:
        return self.card.queue == consts.QUEUE_TYPE_SUSPENDED

    def suspend(self) -> None:
        if not self.is_suspended():
            self._scheduler().suspend_cards([self.card.id])

    def un_suspend(self) -> None:
        if self.is_suspended():
            self._scheduler().unsuspend_cards([self.card.id])

    def type(self) -> NoteTemplateEx:
        from anki_extentions.notetype_ex.note_type_template import NoteTemplateEx
        return NoteTemplateEx.from_dict(self.card.template())

    def sequential_again_answers_today(self) -> int:
        answers = _get_answers_since_last_day_cutoff_for_card(self.card.id)
        return answers.take_while(lambda x: AnswerAction(x) == AnswerAction.ANSWER_AGAIN).qcount() # len(list(ex_iterable.take_while(lambda x: AnswerAction(x) == AnswerAction.ANSWER_AGAIN, answers)))

    def note(self) -> JPNote:
        from note.jpnote import JPNote
        return JPNote.note_from_card(self.card)

    @staticmethod
    def _deck_manager() -> DeckManager:
        return app.anki_collection().decks

    def get_deck(self) -> DeckEx:
        return DeckEx(non_optional(self._deck_manager().get(self.card.current_deck_id())))


class Card2Ex(AutoSlots):
    def __init__(self, card:anki.cards_pb2.Card) -> None:
        self.card:anki.cards_pb2.Card = card

    def sequential_again_answers_today(self) -> int:
        answers = _get_answers_since_last_day_cutoff_for_card(self.card.id)
        return answers.take_while(lambda x: AnswerAction(x) == AnswerAction.ANSWER_AGAIN).qcount()  #len(list(ex_iterable.take_while(lambda x: AnswerAction(x) == AnswerAction.ANSWER_AGAIN, answers)))

    def last_answer_today_was_fail_db_call(self) -> bool:
        return self.sequential_again_answers_today() > 0
</file>

<file path="anki_extentions/config_manager_ex.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from anki.config import ConfigManager


class ConfigManagerEx(AutoSlots):
    def __init__(self, config:ConfigManager) -> None:
        self.config: ConfigManager = config

    def set_timebox_seconds(self, seconds:int) -> None:
        self.config.set("timeLim", seconds)
</file>

<file path="anki_extentions/deck_configdict_ex.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from sysutils.typed import float_

if TYPE_CHECKING:
    from anki.decks import DeckConfigDict


class DeckConfigDictEx(AutoSlots):
    def __init__(self, deck_config_dict:DeckConfigDict) -> None:
        self._dict:DeckConfigDict = deck_config_dict

    def get_seconds_to_show_question(self) -> float:
        return float_(self._dict.get("secondsToShowQuestion"))
</file>

<file path="anki_extentions/deck_ex.py">
from __future__ import annotations

from typing import cast

from anki.decks import DeckDict, DeckId
from anki_extentions.deck_configdict_ex import DeckConfigDictEx
from ankiutils import app
from ex_autoslot import AutoSlots
from sysutils.typed import str_


class DeckEx(AutoSlots):
    def __init__(self, deck_dict: DeckDict) -> None:
        self.deck_dict: DeckDict = deck_dict
        self.name: str = str_(deck_dict["name"])  # pyright: ignore[reportAny]
        self.id: DeckId = cast(DeckId, deck_dict["id"])

    def get_config(self) -> DeckConfigDictEx:
        return DeckConfigDictEx(app.anki_collection().decks.config_dict_for_deck_id(self.id))
</file>

<file path="anki_extentions/note_ex.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from anki.models import FieldDict, NotetypeDict, NotetypeId
from anki.notes import Note, NoteId
from line_profiling_hacks import profile_lines
from sysutils import typed
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from anki.collection import Collection
    from anki.dbproxy import Row
    from qt_utils.task_runner_progress_dialog import ITaskRunner

class NoteBulkLoader:
    @staticmethod
    @profile_lines
    def load_all_notes_of_type(col: Collection, note_type_name: str, task_runner: ITaskRunner) -> list[Note]:
        note_type: NotetypeDict = typed.non_optional(col.models.by_name(note_type_name))
        field_map: dict[str, tuple[int, FieldDict]] = col.models.field_map(non_optional(note_type))
        field_count = len(note_type["flds"]) # pyright: ignore[reportAny]
        note_type_id: NotetypeId = NotetypeId(note_type["id"]) # pyright: ignore[reportAny]
        col_weak_ref: Collection = col.weakref()

        query = """
                SELECT notes.id,  -- 0
                       notes.guid,-- 1
                       notes.mid, -- 2
                       notes.mod, -- 3
                       notes.usn, -- 4
                       notes.tags,-- 5
                       notes.flds -- 6
                FROM notes
                WHERE notes.mid = ?
                """

        rows = non_optional(col.db).all(query, note_type_id)

        def note_bulkloader_note_constructor(row: Row) -> Note:
            return NoteBulkLoader._NoteEx(col_weak_ref, row, field_map, field_count)

        return task_runner.process_with_progress(rows, note_bulkloader_note_constructor, f"Loading {note_type_name} notes from Anki db")

    # noinspection PyMissingConstructor
    class _NoteEx(Note):
        # noinspection PyMissingConstructor this is very much intentional. We do NOT want the base class to go fetching the note's data from the db. Avoiding that is the whole point of this class and improves performance tenfold or more.
        def __init__(self, collection_weak_ref: Collection, db_row: Row, field_map: dict[str, tuple[int, FieldDict]], field_count: int) -> None: # pyright: ignore[reportMissingSuperCall]
            self.col: Collection = collection_weak_ref
            self.id: NoteId = NoteId(db_row[0]) # pyright: ignore[reportAny]
            self.guid: str = db_row[1]
            self.mid: NotetypeId = NotetypeId(db_row[2]) # pyright: ignore[reportAny]
            self.mod: int = db_row[3]
            self.usn: int = db_row[4]
            self.tags: list[str] = db_row[5].split() if db_row[5] else [] # pyright: ignore[reportAny]

            field_values = db_row[6].split("\x1f") if db_row[6] else [] # pyright: ignore
            while len(field_values) < field_count: field_values.append("") # pyright: ignore
            self.fields:list[str] = field_values
            self._fmap:dict[str, tuple[int, FieldDict]] = field_map
</file>

<file path="anki_extentions/notetype_ex/note_type_ex.py">
from __future__ import annotations

from typing import cast

from anki.models import NotetypeDict, NotetypeId
from anki_extentions.notetype_ex.note_type_field import NoteFieldEx
from anki_extentions.notetype_ex.note_type_template import NoteTemplateEx
from ex_autoslot import AutoSlots
from sysutils import ex_assert, typed


class NoteTypeEx(AutoSlots):
    def __init__(self, name: str, fields: list[NoteFieldEx], templates: list[NoteTemplateEx]) -> None:
        self.name: str = name
        self.id: NotetypeId = NotetypeId(0)
        self.flds:list[NoteFieldEx] = fields
        self.tmpls:list[NoteTemplateEx] = templates
        self.type:int = 0
        self.mod:int = 0
        self.usn:int = 0
        self.sortf:int = 0
        self.did:int = 0
        self.css:str = ""
        self.latexPre:str = ""
        self.latexPost:str = ""
        self.latexsvg:bool = False
        self.req: list[object] = []
        self.vers: list[object] = []
        self.tags: list[object] = []

        for index, field in enumerate(self.flds):
            field.ord = index

        for index, template in enumerate(self.tmpls):
            template.ord = index

    def to_dict(self) -> NotetypeDict:
        return {
            "id": self.id,
            "name": self.name,
            "type": self.type,
            "mod": self.mod,
            "usn": self.usn,
            "sortf": self.sortf,
            "did": self.did,
            "tmpls": [t.to_dict() for t in self.tmpls],
            "flds": [f.to_dict() for f in self.flds],
            "css": self.css,
            "latexPre": self.latexPre,
            "latexPost": self.latexPost,
            "latexsvg": self.latexsvg,
            "req": self.req,
            "vers": self.vers,
            "tags": self.tags
        }

    def assert_schema_matches(self, other: NoteTypeEx) -> None:
        ex_assert.equal(len(self.flds), len(other.flds), "same number of fields")
        for index in range(len(self.flds)):
            ex_assert.equal(self.flds[index].ord, other.flds[index].ord, "same order")
            ex_assert.equal(self.flds[index].name, other.flds[index].name, "same name")

    @classmethod
    def from_dict(cls, note_type_dict: NotetypeDict) -> NoteTypeEx:
        created = NoteTypeEx(note_type_dict["name"], [], [])  # pyright: ignore[reportAny]
        created.flds = [NoteFieldEx.from_dict(d) for d in note_type_dict["flds"]]  # pyright: ignore[reportAny]
        created.tmpls = [NoteTemplateEx.from_dict(t) for t in note_type_dict["tmpls"]]  # pyright: ignore[reportAny]

        created.id = cast(NotetypeId, typed.int_(note_type_dict["id"]))  # pyright: ignore[reportAny]
        created.type = typed.int_(note_type_dict["type"])  # pyright: ignore[reportAny]
        created.mod = typed.int_(note_type_dict["mod"])  # pyright: ignore[reportAny]
        created.usn = typed.int_(note_type_dict["usn"])  # pyright: ignore[reportAny]
        created.sortf = typed.int_(note_type_dict["sortf"])  # pyright: ignore[reportAny]
        # None for some reason# created.did = typed.int_(note_type_dict['did'])
        created.css = typed.str_(note_type_dict["css"])  # pyright: ignore[reportAny]
        created.latexPre = typed.str_(note_type_dict["latexPre"])  # pyright: ignore[reportAny]
        created.latexPost = typed.str_(note_type_dict["latexPost"])  # pyright: ignore[reportAny]
        created.latexsvg = typed.bool_(note_type_dict["latexsvg"])  # pyright: ignore[reportAny]
        created.req = note_type_dict["req"]
        # missing for som reason# created.vers = note_type_dict['vers']
        # missing for som reason# created.tags = note_type_dict['tags']

        return created
</file>

<file path="anki_extentions/notetype_ex/note_type_field.py">
from __future__ import annotations

from ex_autoslot import AutoSlots
from sysutils import typed


class NoteFieldEx(AutoSlots):
    def __init__(self, name: str) -> None:
        self.name: str = name
        self.ord: int = 0
        self.sticky: bool = False
        self.rtl: bool = False
        self.font: str = "Arial"
        self.size: int = 20
        self.description: str = ""
        self.plainText: bool = False
        self.collapsed: bool = False
        self.excludeFromSearch: bool = False
        self.media: list[object] = []

    def to_dict(self) -> dict[str, object]:
        return {
            "name": self.name,
            "ord": self.ord,
            "sticky": self.sticky,
            "rtl": self.rtl,
            "font": self.font,
            "size": self.size,
            "description": self.description,
            "plainText": self.plainText,
            "collapsed": self.collapsed,
            "excludeFromSearch": self.excludeFromSearch,
            "media": self.media}

    @classmethod
    def from_dict(cls, d: dict[str, object]) -> NoteFieldEx:
        instance = NoteFieldEx(typed.str_(d["name"]))

        instance.ord = typed.int_(d["ord"])
        instance.sticky = typed.bool_(d["sticky"])
        instance.rtl = typed.bool_(d["rtl"])
        instance.font = typed.str_(d["font"])
        instance.size = typed.int_(d["size"])
        instance.description = typed.str_(d["description"])
        instance.plainText = typed.bool_(d["plainText"])
        instance.collapsed = typed.bool_(d["collapsed"])
        instance.excludeFromSearch = typed.bool_(d["excludeFromSearch"])
        # missing for som reason# instance.media = d['media']

        return instance
</file>

<file path="anki_extentions/notetype_ex/note_type_template.py">
from __future__ import annotations

from ex_autoslot import AutoSlots
from sysutils import typed


class NoteTemplateEx(AutoSlots):
    def __init__(self, name: str) -> None:
        self.name: str = name
        self.ord: int = 0
        self.qfmt: str = "{{Tags}}"
        self.afmt: str = "{{Tags}}"
        self.bqfmt: str = ""
        self.bafmt: str = ""
        self.did: int = 0
        self.bfont: str = "Arial"
        self.bsize: int = 30

    def to_dict(self) -> dict[str, object]:
        return {"name": self.name,
                "ord": self.ord,
                "qfmt": self.qfmt,
                "afmt": self.afmt,
                "bqfmt": self.bqfmt,
                "bafmt": self.bafmt,
                "did": self.did,
                "bfont": self.bfont,
                "bsize": self.bsize}

    @classmethod
    def from_dict(cls, d: dict[str, str | int | None]) -> NoteTemplateEx:
        instance = NoteTemplateEx(typed.str_(d["name"]))

        instance.ord = typed.int_(d["ord"])
        instance.qfmt = typed.str_(d["qfmt"])
        instance.afmt = typed.str_(d["afmt"])
        instance.bqfmt = typed.str_(d["bqfmt"])
        instance.bafmt = typed.str_(d["bafmt"])
        # missing for som reason# instance.did = typed.int_(d['did'])
        instance.bfont = typed.str_(d["bfont"])
        instance.bsize = typed.int_(d["bsize"])
        return instance
</file>

<file path="anki_extentions/sheduling_states_ex.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from anki.scheduler.v3 import SchedulingState, SchedulingStates  # pyright: ignore[reportMissingTypeStubs]

class SchedulingStatesEx(AutoSlots):
    def __init__(self, states: SchedulingStates) -> None:
        self._states: SchedulingStates = states
        self.again: SchedulingStateEx = SchedulingStateEx(states.again)

class SchedulingStateEx(AutoSlots):
    def __init__(self, state: SchedulingState) -> None:
        self._state: SchedulingState = state

    def is_relearning(self) -> bool: return self._state.normal.relearning.learning.scheduled_secs > 0
    def is_learning(self) -> bool: return self._state.normal.learning.scheduled_secs > 0
    def is_filtered_learning(self) -> bool: return self._state.filtered.rescheduling.original_state.learning.scheduled_secs > 0
    def is_filtered_relearning(self) -> bool: return self._state.filtered.rescheduling.original_state.relearning.learning.scheduled_secs > 0

    def set_seconds(self, seconds: int) -> None:
        if self.is_relearning():
            self._state.normal.relearning.learning.scheduled_secs = seconds
        elif self.is_learning():
            self._state.normal.learning.scheduled_secs = seconds
        elif self.is_filtered_relearning():
            self._state.filtered.rescheduling.original_state.relearning.learning.scheduled_secs = seconds
        elif self.is_filtered_learning():
            self._state.filtered.rescheduling.original_state.learning.scheduled_secs = seconds
        else:
            raise Exception("May only be called on relearning or learning cards")
</file>

<file path="ankiutils/anki_module_import_issues_fix_just_import_this_module_before_any_other_anki_modules.py">
from anki import collection # noqa fixes anki module loading issues  # pyright: ignore[reportUnusedImport]
</file>

<file path="ankiutils/app.py">
from __future__ import annotations

import os
from typing import TYPE_CHECKING

import mylog
from sysutils.typed import checked_cast, non_optional
from testutils import ex_pytest

is_testing = ex_pytest.is_testing

if TYPE_CHECKING:
    from collections.abc import Callable

    from anki.collection import Collection
    from anki.dbproxy import DBProxy
    from anki.scheduler.v3 import Scheduler  # pyright: ignore[reportMissingTypeStubs]
    from anki_extentions.config_manager_ex import ConfigManagerEx
    from ankiutils.ui_utils_interface import IUIUtils
    from aqt import AnkiQt  # type: ignore[attr-defined]  # pyright: ignore[reportPrivateImportUsage]
    from configuration.configuration_value import JapaneseConfig
    from note.collection.jp_collection import JPCollection

_collection: JPCollection | None = None

_init_hooks: set[Callable[[], None]] = set()
_collection_closed_hooks: set[Callable[[], None]] = set()

def _call_init_hooks() -> None:
    global _init_hooks
    for hook in _init_hooks: hook()

def add_init_hook(hook: Callable[[], None]) -> None:
    _init_hooks.add(hook)

def config() -> JapaneseConfig:
    from configuration import configuration_value
    return configuration_value.config()

def _init(delay_seconds: float = 1.0) -> None:
    mylog.info(f"_init delay= {delay_seconds}")

    from aqt import mw
    from note.collection.jp_collection import JPCollection
    global _collection
    if _collection is not None:
        _collection.reshchedule_init_for(delay_seconds)

    _collection = JPCollection(non_optional(mw.col), delay_seconds)
    _call_init_hooks()

def is_initialized() -> bool:
    return _collection is not None and _collection.is_initialized

def reset(delay_seconds: float = 0) -> None:
    mylog.info(f"reset: delay= {delay_seconds}")
    _destruct()
    _init(delay_seconds)

def _reset(_col: object | None = None) -> None:
    mylog.info("_reset")
    reset()

def _destruct() -> None:
    mylog.info("_destruct_ja_app")
    from sysutils.timeutil import StopWatch
    with StopWatch.log_warning_if_slower_than(0.2):
        global _collection
        if _collection is not None:
            _collection.destruct_sync()

        _collection = None

def _collection_is_being_invalidated(_col: object | None = None) -> None:
    mylog.info("_collection_is_being_invalidated")
    _destruct()
    reset(delay_seconds=9999)  # Unless forced by the user we don't actually want to run an initialization here

def _profile_closing() -> None:
    mylog.info("anki_profile_closing")
    from aqt import gui_hooks
    gui_hooks.sync_will_start.remove(_collection_is_being_invalidated)
    _collection_closed_hooks.remove(_destruct)
    gui_hooks.sync_did_finish.remove(_init)
    gui_hooks.collection_did_load.remove(_reset)  # pyright: ignore[reportUnknownMemberType]
    _destruct()

def _profile_opened() -> None:
    mylog.info("profile_opened")
    from aqt import gui_hooks
    gui_hooks.sync_will_start.append(_collection_is_being_invalidated)
    _collection_closed_hooks.add(_destruct)
    gui_hooks.sync_did_finish.append(_init)
    gui_hooks.collection_did_load.append(_reset)  # pyright: ignore[reportUnknownMemberType]
    _init(delay_seconds=1.0)

def anki_config() -> ConfigManagerEx:
    from anki_extentions.config_manager_ex import ConfigManagerEx
    from aqt import mw
    return ConfigManagerEx(non_optional(mw.col).conf)

def col() -> JPCollection:
    if _collection is None: raise AssertionError("Collection not initialized")
    return _collection

def anki_collection() -> Collection: return col().anki_collection

def anki_db() -> DBProxy: return non_optional(col().anki_collection.db)

def anki_scheduler() -> Scheduler:
    from anki.scheduler.v3 import Scheduler  # pyright: ignore[reportMissingTypeStubs]
    return checked_cast(Scheduler, col().anki_collection.sched)

def main_window() -> AnkiQt:
    from aqt import mw
    return non_optional(mw)

def get_ui_utils() -> IUIUtils:
    from ankiutils.ui_utils import UIUtils
    return UIUtils(main_window())

def _collection_closed(_collection: Collection, _downgrade: bool = False) -> None:
    for hook in _collection_closed_hooks:
        hook()

def _wrap_collection_close() -> None:
    import anki
    from anki.collection import Collection
    Collection.close = anki.hooks.wrap(Collection.close, _collection_closed, "before")  # type: ignore  # pyright: ignore[reportAttributeAccessIssue, reportUnknownMemberType]

_wrap_collection_close()

def _setup_gui_hooks() -> None:
    from aqt import gui_hooks
    gui_hooks.profile_will_close.append(_profile_closing)
    gui_hooks.profile_did_open.append(_profile_opened)

user_files_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..\\user_files")

_setup_gui_hooks()
</file>

<file path="ankiutils/audio_suppressor.py">
from __future__ import annotations

import threading
from typing import TYPE_CHECKING

from aqt.sound import av_player
from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from anki.sound import AVTag


class AudioSuppressor(AutoSlots):
    def __init__(self) -> None:
        self._av_player_play_tags_method = av_player.play_tags  # pyright: ignore[reportUnannotatedClassAttribute]

    def restore_play_tags_method(self) -> None:
        av_player.play_tags = self._av_player_play_tags_method  # type: ignore

    def suppress_for_seconds(self, time: float) -> None:
        def null_op(_tags: list[AVTag]) -> None:
            pass

        av_player.play_tags = null_op  # type: ignore  # pyright: ignore[reportAttributeAccessIssue]
        threading.Timer(time, self.restore_play_tags_method).start()

audio_suppressor: AudioSuppressor = AudioSuppressor()
</file>

<file path="ankiutils/query_builder.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis
from note.note_constants import Builtin, MyNoteFields, NoteFields, NoteTypes, SentenceNoteFields
from queryablecollections.collections.q_set import QSet
from sysutils import kana_utils

if TYPE_CHECKING:
    from collections.abc import Iterable

    from anki.cards import CardId
    from anki.notes import NoteId
    from note.jpnote import JPNote
    from note.kanjinote import KanjiNote
    from note.vocabulary.vocabnote import VocabNote

f_question = MyNoteFields.question
f_reading = NoteFields.Vocab.Reading
f_answer = MyNoteFields.answer
f_forms = NoteFields.Vocab.Forms
f_reading_on = NoteFields.Kanji.Reading_On
f_reading_kun = NoteFields.Kanji.Reading_Kun

card_read = f"{Builtin.Card}:{NoteFields.VocabNoteType.Card.Reading}"

note_kanji = f"{Builtin.Note}:{NoteTypes.Kanji}"
note_vocab = f"{Builtin.Note}:{NoteTypes.Vocab}"
note_sentence = f"{Builtin.Note}:{NoteTypes.Sentence}"
excluded_deck_substring = "*Excluded*"
deck_excluded = f"""{Builtin.Deck}:{excluded_deck_substring}"""

note_vocab = note_vocab

def _or_clauses(clauses: list[str]) -> str:
    return clauses[0] if len(clauses) == 1 else f"""({" OR ".join(clauses)})"""

def field_contains_word(field: str, *words: str) -> str:
    return _or_clauses([f'''"{field}:re:\\b{query}\\b"''' for query in words])

def field_contains_string(field: str, *words: str) -> str:
    return _or_clauses([f'''"{field}:*{query}*"''' for query in words])

def sentence_search(word: str, exact: bool = False) -> str:
    result = f"""{note_sentence} """

    def form_query(form: str) -> str:
        return f"""(-{field_contains_word(SentenceNoteFields.user_excluded_vocab, form)} AND ({f_question}:*{form}* OR {field_contains_word(SentenceNoteFields.parsing_result, form)} OR {field_contains_word(SentenceNoteFields.user_extra_vocab, form)}))"""

    if not exact:
        vocabs = app.col().vocab.with_form(word)
        if vocabs:
            forms: set[str] = vocabs.select_many(lambda voc: voc.forms.all_list()).to_set()  # set(ex_sequence.flatten([v.forms.all_list() for v in vocabs]))
            return result + "(" + "　OR ".join([form_query(form) for form in forms]) + ")"

    return result + f"""({form_query(word)})"""

def potentially_matching_sentences_for_vocab(word: VocabNote) -> str:
    if word.matching_configuration.requires_forbids.exact_match.is_required:
        search_strings = word.forms.all_list()
    else:
        search_strings = word.conjugator.get_stems_for_all_forms() + word.forms.all_list()
        # we'd much rather catch too much than not enough here. False positives only slow things down a bit,
        # false negatives on the other hand, can totally confuse things, giving the user entirely the wrong idea about the value of a word when no/few matches are found.
        search_strings += [form[:-1] for form in word.forms.all_list() if (len(form) > 2 or kana_utils.contains_kanji(form)) and kana_utils.character_is_kana(form[-1])]
    return f"""{note_sentence} {field_contains_string(f_question, *search_strings)}"""

def sentences_with_question_substring(substring: str) -> str:
    return f"""{note_sentence} {field_contains_string(SentenceNoteFields.active_question, substring)}"""

def notes_lookup(notes: Iterable[JPNote]) -> str:
    return notes_by_id([note.get_id() for note in notes])

def notes_by_id(note_ids: list[NoteId]) -> str:
    return f"""{NoteFields.note_id}:{",".join([str(note_id) for note_id in note_ids])}""" if note_ids else ""

def single_vocab_wildcard(query: str) -> str: return f"{note_vocab} ({f_forms}:*{query}* OR {f_reading}:*{query}* OR {f_answer}:*{query}*)"

def single_vocab_by_question_reading_or_answer_exact(query: str) -> str:
    return f"{note_vocab} ({field_contains_word(f_forms, query)} OR {field_contains_word(f_reading, kana_utils.katakana_to_hiragana(query))} OR {field_contains_word(f_answer, query)})"

def single_vocab_by_form_exact(query: str) -> str: return f"{note_vocab} {field_contains_word(f_forms, query)}"

def single_vocab_by_form_exact_read_card_only(query: str) -> str: return f"({single_vocab_by_form_exact(query)}) {card_read}"

def kanji_in_string(query: str) -> str: return f"{note_kanji} ( {' OR '.join([f'{f_question}:{char}' for char in query])} )"

def vocab_dependencies_lookup_query(vocab: VocabNote) -> str:
    def single_vocab_clause(voc: str) -> str:
        return f"{field_contains_word(f_forms, voc)}"

    def create_vocab_clause(text: str) -> str:
        dictionary_forms = TextAnalysis.from_text(text).all_words_strings()
        return f"({note_vocab} ({' OR '.join([single_vocab_clause(voc) for voc in dictionary_forms])})) OR " if dictionary_forms else ""

    def create_vocab_vocab_clause() -> str:
        return create_vocab_clause(vocab.get_question())

    def create_kanji_clause() -> str:
        return f"{note_kanji} ( {' OR '.join([f'{f_question}:{char}' for char in vocab.get_question()])} )"

    return f"""{create_vocab_vocab_clause()} ({create_kanji_clause()})"""

def vocab_with_kanji(note: KanjiNote) -> str: return f"{note_vocab} {f_forms}:*{note.get_question()}*"

def vocab_clause(voc: str) -> str:
    return f"""{field_contains_word(f_forms, voc)}"""

def text_vocab_lookup(text: str) -> str:
    dictionary_forms = TextAnalysis.from_text(text).all_words_strings()
    return vocabs_lookup(dictionary_forms)

def vocabs_lookup(dictionary_forms: list[str]) -> str:
    return f"{note_vocab} ({' OR '.join([vocab_clause(voc) for voc in dictionary_forms])})"

def vocabs_lookup_strings(words: list[str]) -> str:
    return f'''{note_vocab} ({' OR '.join([f"""{field_contains_word(f_forms, voc)}""" for voc in words])})'''

def vocabs_lookup_strings_read_card(words: list[str]) -> str:
    return f"""{vocabs_lookup_strings(words)} {card_read}"""

def kanji_with_reading_part(reading_part: str) -> str:
    hiragana_reading_part = kana_utils.anything_to_hiragana(reading_part)
    return f"""note:{NoteTypes.Kanji} ({f_reading_on}:*{hiragana_reading_part}* OR {f_reading_kun}:*{hiragana_reading_part}*)"""

def exact_matches(question: str) -> str:
    return f"""{f_question}:"{question}" OR {field_contains_word(f_forms, question)}"""

def exact_matches_no_sentences(question: str) -> str:
    return f"""({exact_matches(question)}) -{note_sentence}"""

def exact_matches_no_sentences_reading_cards(question: str) -> str:
    return f"""({exact_matches_no_sentences(question)}) {card_read} -{deck_excluded}"""

def immersion_kit_sentences() -> str:
    return f'''"{Builtin.Note}:{NoteTypes.immersion_kit}"'''

def kanji_with_radicals_in_string(search: str) -> str:
    radicals = QSet(search.strip().replace(",", "").replace(" ", ""))
    def kanji_contails_all_radicals(kanji: KanjiNote) -> bool: return not any(rad for rad in radicals if rad not in kanji.get_radicals())
    return (radicals
            .select_many(lambda radical: app.col().kanji.with_radical(radical))
            .where(kanji_contails_all_radicals)
            .pipe(notes_lookup))

def open_card_by_id(card_id: CardId) -> str:
    return f"cid:{card_id}"

def kanji_with_meaning(search: str) -> str:
    return f"""{note_kanji} ({f_answer}:*{search}*)"""
</file>

<file path="ankiutils/search_executor.py">
from __future__ import annotations

from typing import TYPE_CHECKING

import aqt
from ankiutils import app
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from collections.abc import Callable

    from aqt.browser import Browser  # type: ignore[attr-defined]  # pyright: ignore[reportPrivateImportUsage]


def do_lookup_and_show_previewer(text: str) -> None:
    do_lookup(text)
    app.get_ui_utils().activate_preview()

def do_lookup(text: str) -> None:
    browser: Browser = aqt.dialogs.open("Browser", aqt.mw)  # pyright: ignore[reportAny]
    non_optional(browser.form.searchEdit.lineEdit()).setText(text)
    browser.onSearchActivated()  # pyright: ignore[reportUnknownMemberType]
    app.get_ui_utils().activate_preview()

def lookup_promise(search: Callable[[], str]) -> Callable[[], None]: return lambda: do_lookup(search())

def lookup_and_show_previewer_promise(search: Callable[[], str]) -> Callable[[], None]: return lambda: do_lookup_and_show_previewer(search())
</file>

<file path="ankiutils/ui_utils_interface.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from collections.abc import Callable


class IUIUtils(AutoSlots):
    def is_edit_current_open(self) -> bool: raise NotImplementedError()
    def refresh(self, refresh_browser:bool = True) -> None: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter]
    def run_ui_action(self, callback: Callable[[],None]) -> None: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter]
    def activate_preview(self) -> None: raise NotImplementedError()
    def tool_tip(self, message:str, milliseconds:int = 3000) -> None: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter]
</file>

<file path="ankiutils/ui_utils.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ex_autoslot import AutoSlots
from PyQt6.QtWidgets import QApplication

if TYPE_CHECKING:
    from collections.abc import Callable

    from anki.notes import Note
    from note.jpnote import JPNote

import aqt
from ankiutils.audio_suppressor import audio_suppressor
from ankiutils.ui_utils_interface import IUIUtils
from aqt import AnkiQt  # type: ignore[attr-defined]  # pyright: ignore[reportPrivateImportUsage]
from aqt.browser import Browser  # type: ignore[attr-defined]  # pyright: ignore[reportPrivateImportUsage]
from aqt.browser.previewer import Previewer
from aqt.clayout import CardLayout
from aqt.editcurrent import EditCurrent
from aqt.editor import Editor
from aqt.reviewer import RefreshNeeded
from aqt.utils import tooltip
from aqt.webview import AnkiWebView, AnkiWebViewKind
from sysutils import app_thread_pool, timeutil
from sysutils.typed import checked_cast, non_optional

_ANSWER_DISPLAY_TYPES = {"reviewAnswer", "previewAnswer", "clayoutAnswer"}

def main_window() -> AnkiQt: return non_optional(aqt.mw)

def is_displaytype_displaying_answer(display_type: str) -> bool:
    return display_type in _ANSWER_DISPLAY_TYPES

def is_displaytype_displaying_review_question(display_type: str) -> bool:
    return display_type == "reviewQuestion"

def is_displaytype_displaying_review_answer(display_type: str) -> bool:
    return display_type == "reviewAnswer"

def is_reviewer_display_type(display_type: str) -> bool:
    return display_type.startswith("review")

def get_note_from_web_view(view: AnkiWebView) -> JPNote | None:
    inner_note: Note | None

    if view.kind == AnkiWebViewKind.MAIN:
        card = main_window().reviewer.card
        if card:
            inner_note = non_optional(main_window().reviewer.card).note()
        else:
            return None
    elif view.kind == AnkiWebViewKind.EDITOR:
        # noinspection PyProtectedMember
        editor = checked_cast(Editor, view._bridge_context)  # pyright: ignore[reportPrivateUsage]
        if not editor.card: return None
        card = non_optional(editor.card)
        inner_note = non_optional(card.note())
    elif view.kind == AnkiWebViewKind.PREVIEWER:
        inner_note = non_optional([window for window in main_window().app.topLevelWidgets() if isinstance(window, Previewer)][0].card()).note()
    elif view.kind == AnkiWebViewKind.CARD_LAYOUT:
        inner_note = non_optional([window for window in main_window().app.topLevelWidgets() if isinstance(window, CardLayout)][0].note)
    else:
        return None

    from note.jpnote import JPNote
    return JPNote.note_from_note(inner_note)

class UIUtils(IUIUtils, AutoSlots):
    def __init__(self, mw: AnkiQt) -> None:
        self._mw: AnkiQt = mw

    @override
    def is_edit_current_open(self) -> bool:
        edit_current = [window for window in self._mw.app.topLevelWidgets() if isinstance(window, EditCurrent)]
        return len(edit_current) > 0

    @override
    def run_ui_action(self, callback: Callable[[], None]) -> None:
        time = timeutil.time_execution(callback)
        self.refresh()
        tooltip(f"done in {time}")

    @override
    def refresh(self, refresh_browser: bool = True) -> None:
        if not app.is_initialized():
            return

        def force_previewer_rerender() -> None:
            previewers: list[Previewer] = [window for window in self._mw.app.topLevelWidgets() if isinstance(window, Previewer)]
            if len(previewers) > 0:
                previewer = previewers[0]
                # noinspection PyProtectedMember
                previewer._last_state = (previewer._state, non_optional(previewer.card()).id, 0)  # pyright: ignore[reportPrivateUsage]
                previewer.render_card()

        def force_reviewer_rerender() -> None:
            if self._mw.reviewer.card:
                self._mw.reviewer._refresh_needed = RefreshNeeded.NOTE_TEXT  # pyright: ignore[reportPrivateUsage]
                self._mw.reviewer.refresh_if_needed()

        def force_browser_rerender() -> None:
            browser: list[Browser] = [window for window in self._mw.app.topLevelWidgets() if isinstance(window, Browser)]
            if len(browser) > 0:
                browser[0].onSearchActivated()  # pyright: ignore[reportUnknownMemberType]

        app.col().flush_cache_updates()
        audio_suppressor.suppress_for_seconds(.3)
        force_reviewer_rerender()
        force_previewer_rerender()

        if refresh_browser:
            force_browser_rerender()

    @override
    def activate_preview(self) -> None:
        browser: Browser = aqt.dialogs.open('Browser', self._mw)  # noqa  # pyright: ignore[reportAny]
        self._mw.app.processEvents()
        if browser._previewer is None:  # noqa  # pyright: ignore[reportPrivateUsage]
            browser.onTogglePreview()
        else:
            browser._previewer.activateWindow()  # noqa  # pyright: ignore[reportPrivateUsage]

    @override
    def tool_tip(self, message: str, milliseconds: int = 3000) -> None:
        def show_tooltip() -> None:
            tooltip(message, milliseconds)
            QApplication.processEvents()

        app_thread_pool.run_on_ui_thread_fire_and_forget(show_tooltip)

def try_get_review_note() -> JPNote | None:
    from note.jpnote import JPNote
    return JPNote.note_from_card(non_optional(main_window().reviewer.card)) if main_window().reviewer.card else None
</file>

<file path="batches/local_note_updater.py">
from __future__ import annotations

import gc
import re
from typing import TYPE_CHECKING

from ankiutils import app, query_builder
from note.note_constants import CardTypes, Tags
from note.sentences.sentencenote import SentenceNote
from sysutils import ex_str

if TYPE_CHECKING:
    from anki.notes import NoteId
    from note.kanjinote import KanjiNote
    from note.vocabulary.vocabnote import VocabNote

def update_all() -> None:
    update_sentences()
    update_kanji()
    update_vocab()
    tag_note_metadata()
    # update_vocab_parsed_parts_of_speech()

def full_rebuild() -> None:
    update_all()
    reparse_all_sentences()

def update_sentences() -> None:
    from sysutils import progress_display_runner
    def update_sentence(sentence: SentenceNote) -> None:
        sentence.update_generated_data()

    progress_display_runner.process_with_progress(app.col().sentences.all(), update_sentence, "Updating sentences")

def update_kanji() -> None:
    from sysutils import progress_display_runner
    def _update_kanji(kanji: KanjiNote) -> None:
        kanji.update_generated_data()

    progress_display_runner.process_with_progress(app.col().kanji.all(), _update_kanji, "Updating kanji")

def update_vocab() -> None:
    from sysutils import progress_display_runner
    def _update_vocab(vocab: VocabNote) -> None:
        vocab.update_generated_data()

    progress_display_runner.process_with_progress(app.col().vocab.all(), _update_vocab, "Updating vocab")

def convert_immersion_kit_sentences() -> None:
    from sysutils import progress_display_runner
    def convert_note(note_id: NoteId) -> None:
        immersion_kit_note = app.anki_collection().get_note(note_id)
        SentenceNote.import_immersion_kit_sentence(immersion_kit_note)
        app.anki_collection().remove_notes([note_id])

    immersion_kit_sences = list(app.anki_collection().find_notes(query_builder.immersion_kit_sentences()))
    progress_display_runner.process_with_progress(immersion_kit_sences, convert_note, "Converting immersion kit sentences")

def tag_note_metadata() -> None:
    tag_kanji_metadata()
    tag_vocab_metadata()
    tag_sentence_metadata()

def tag_sentence_metadata() -> None:
    from sysutils import progress_display_runner
    def tag_sentence(sentence: SentenceNote) -> None:
        sentence.toggle_tag(Tags.Sentence.Uses.incorrect_matches, any(sentence.configuration.incorrect_matches.get()))
        sentence.toggle_tag(Tags.Sentence.Uses.hidden_matches, any(sentence.configuration.hidden_matches.get()))

    progress_display_runner.process_with_progress(app.col().sentences.all(), tag_sentence, "Tag sentence notes")

def tag_vocab_metadata() -> None:
    from sysutils import progress_display_runner
    def tag_note(vocab: VocabNote) -> None:
        vocab.toggle_tag(Tags.Vocab.has_no_studying_sentences, not any(vocab.sentences.studying()))
        vocab.toggle_tag(Tags.Vocab.Matching.Uses.required_prefix, not vocab.matching_configuration.configurable_rules.required_prefix.none())
        vocab.toggle_tag(Tags.Vocab.Matching.Uses.prefix_is_not, not vocab.matching_configuration.configurable_rules.prefix_is_not.none())
        vocab.toggle_tag(Tags.Vocab.Matching.Uses.suffix_is_not, not vocab.matching_configuration.configurable_rules.suffix_is_not.none())
        vocab.toggle_tag(Tags.Vocab.Matching.Uses.surface_is_not, not vocab.matching_configuration.configurable_rules.surface_is_not.none())

    progress_display_runner.process_with_progress(app.col().vocab.all(), tag_note, "Tag vocab notes")

def tag_kanji_metadata() -> None:
    from sysutils import progress_display_runner
    primary_reading = re.compile(r"<primary>(.*?)</primary>")

    known_kanji = {kanji.get_question() for kanji in app.col().kanji.all() if kanji.is_studying()}

    def tag_kanji(kanji: KanjiNote) -> None:
        vocab_with_kanji_in_main_form = app.col().vocab.with_kanji_in_main_form(kanji)
        vocab_with_kanji_in_any_form = app.col().vocab.with_kanji_in_any_form(kanji)

        is_radical = any(app.col().kanji.with_radical(kanji.get_question()))
        kanji.toggle_tag(Tags.Kanji.in_vocab_main_form, any(vocab_with_kanji_in_main_form))
        kanji.toggle_tag(Tags.Kanji.in_any_vocab_form, any(vocab_with_kanji_in_any_form))

        studying_reading_vocab = [voc for voc in vocab_with_kanji_in_main_form if voc.is_studying(CardTypes.reading)]
        kanji.toggle_tag(Tags.Kanji.with_studying_vocab, any(studying_reading_vocab))

        primary_readings: list[str] = primary_reading.findall(f"{kanji.get_reading_on_html()} {kanji.get_reading_kun_html()} {kanji.get_reading_nan_html()}")
        kanji.toggle_tag(Tags.Kanji.with_no_primary_readings, not primary_readings)

        primary_on_readings: list[str] = primary_reading.findall(kanji.get_reading_on_html())
        non_primary_on_readings: list[str] = [reading for reading in kanji.get_readings_on() if reading not in primary_readings]

        kanji.toggle_tag(Tags.Kanji.with_no_primary_on_readings, not primary_on_readings)

        def reading_is_in_vocab_readings(kanji_reading: str, voc: VocabNote) -> bool: return any(vocab_reading for vocab_reading in voc.readings.get() if reading_in_vocab_reading(kanji, kanji_reading, vocab_reading, voc.get_question()))
        def has_vocab_with_reading(kanji_reading: str) -> bool: return any(voc for voc in vocab_with_kanji_in_main_form if any(vocab_reading for vocab_reading in voc.readings.get() if reading_in_vocab_reading(kanji, kanji_reading, vocab_reading, voc.get_question())))

        def vocab_has_only_known_kanji(voc: VocabNote) -> bool: return not any(kan for kan in voc.kanji.extract_all_kanji() if kan not in known_kanji)
        def has_vocab_with_reading_and_no_unknown_kanji(kanji_reading: str) -> bool: return any(voc for voc in vocab_with_kanji_in_main_form if reading_is_in_vocab_readings(kanji_reading, voc) and vocab_has_only_known_kanji(voc))

        kanji.toggle_tag(Tags.Kanji.with_vocab_with_primary_on_reading, any(primary_on_readings) and has_vocab_with_reading(primary_on_readings[0]))

        def has_studying_vocab_with_reading(kanji_reading: str) -> bool: return any(voc for voc in studying_reading_vocab if any(vocab_reading for vocab_reading in voc.readings.get() if reading_in_vocab_reading(kanji, kanji_reading, vocab_reading, voc.get_question())))
        kanji.toggle_tag(Tags.Kanji.with_studying_vocab_with_primary_on_reading, any(primary_on_readings) and has_studying_vocab_with_reading(primary_on_readings[0]))
        kanji.toggle_tag(Tags.Kanji.has_studying_vocab_for_each_primary_reading, any(primary_readings) and not any(reading for reading in primary_readings if not has_studying_vocab_with_reading(reading)))
        kanji.toggle_tag(Tags.Kanji.has_primary_reading_with_no_studying_vocab, any(primary_readings) and any(studying_reading_vocab) and any(reading for reading in primary_readings if not has_studying_vocab_with_reading(reading)))
        kanji.toggle_tag(Tags.Kanji.has_non_primary_on_reading_vocab, any(reading for reading in non_primary_on_readings if has_vocab_with_reading(reading)))
        kanji.toggle_tag(Tags.Kanji.has_non_primary_on_reading_vocab_with_only_known_kanji, any(reading for reading in non_primary_on_readings if has_vocab_with_reading_and_no_unknown_kanji(reading)))

        all_readings = kanji.get_readings_clean()

        def vocab_matches_primary_reading(_vocab: VocabNote) -> bool:
            return any(_primary_reading for _primary_reading in primary_readings if any(_vocab_reading for _vocab_reading in _vocab.readings.get() if _primary_reading in _vocab_reading))

        def vocab_matches_reading(_vocab: VocabNote) -> bool:
            return any(_reading for _reading in all_readings if any(_vocab_reading for _vocab_reading in _vocab.readings.get() if _reading in _vocab_reading))

        kanji.toggle_tag(Tags.Kanji.has_studying_vocab_with_no_matching_primary_reading, any(_vocab for _vocab in studying_reading_vocab if (not vocab_matches_primary_reading(_vocab) and vocab_matches_reading(_vocab))))

        kanji.toggle_tag(Tags.Kanji.is_radical, is_radical)
        kanji.toggle_tag(Tags.Kanji.is_radical_purely, is_radical and not any(vocab_with_kanji_in_any_form))
        kanji.toggle_tag(Tags.Kanji.is_radical_silent, is_radical and not any(primary_readings))

    def tag_has_single_kanji_vocab_with_reading_different_from_kanji_primary_reading(kanji: KanjiNote) -> None:
        vocabs = app.col().vocab.with_kanji_in_main_form(kanji)
        single_kanji_vocab = [v for v in vocabs if v.get_question() == kanji.get_question()]
        kanji.toggle_tag(Tags.Kanji.with_single_kanji_vocab, any(single_kanji_vocab))
        if single_kanji_vocab:
            primary_readings: list[str] = primary_reading.findall(f"{kanji.get_reading_on_html()} {kanji.get_reading_kun_html()} {kanji.get_reading_nan_html()}")

            kanji.remove_tag(Tags.Kanji.with_single_kanji_vocab_with_different_reading)
            kanji.remove_tag(Tags.Kanji.with_studying_single_kanji_vocab_with_different_reading)

            for vocab in single_kanji_vocab:
                for reading in vocab.readings.get():
                    if reading not in primary_readings:
                        kanji.set_tag(Tags.Kanji.with_single_kanji_vocab_with_different_reading)
                        if vocab.is_studying(reading):  # todo: Bug: this code looks nuts. You cannot pass a reading to is_studying.
                            kanji.set_tag(Tags.Kanji.with_studying_single_kanji_vocab_with_different_reading)

    all_kanji = app.col().kanji.all()
    progress_display_runner.process_with_progress(all_kanji, tag_kanji, "Tagging kanji with studying metadata")
    progress_display_runner.process_with_progress(all_kanji, tag_has_single_kanji_vocab_with_reading_different_from_kanji_primary_reading, "Tagging kanji with single kanji vocab")

def reparse_all_sentences() -> None:
    reparse_sentences(app.col().sentences.all())

def reading_in_vocab_reading(kanji: KanjiNote, kanji_reading: str, vocab_reading: str, vocab_form: str) -> bool:
    vocab_form = ex_str.strip_html_and_bracket_markup_and_noise_characters(vocab_form)
    if vocab_form.startswith(kanji.get_question()):
        return vocab_reading.startswith(kanji_reading)
    if vocab_form.endswith(kanji.get_question()):
        return vocab_reading.endswith(kanji_reading)
    return kanji_reading in vocab_reading[1:-1]

def reparse_sentences(sentences: list[SentenceNote]) -> None:
    from sysutils import progress_display_runner
    def reparse_sentence(sentence: SentenceNote) -> None:
        sentence.update_parsed_words(force=True)

    progress_display_runner.process_with_progress(sentences, reparse_sentence, "Reparsing sentences.")

def print_gc_status_and_collect() -> None:
    from sysutils import object_instance_tracker
    object_instance_tracker.print_instance_counts()

    app.get_ui_utils().tool_tip(f"Gc.isenabled(): {gc.isenabled()}, Collecting ...", 10000)

    instances = gc.collect()
    app.get_ui_utils().tool_tip(f"collected: {instances} instances", 10000)
    object_instance_tracker.print_instance_counts()

def reparse_sentences_for_vocab(vocab: VocabNote) -> None:
    query = query_builder.potentially_matching_sentences_for_vocab(vocab)
    sentences: set[SentenceNote] = set(app.col().sentences.search(query))
    # noinspection PyAugmentAssignment
    sentences = sentences | set(vocab.sentences.all())
    reparse_sentences(list(sentences))

def reparse_matching_sentences(question_substring: str) -> None:
    sentences_to_update = app.col().sentences.search(query_builder.sentences_with_question_substring(question_substring))
    reparse_sentences(sentences_to_update)
</file>

<file path="configuration/configuration_value.py">
from __future__ import annotations

import os
from typing import TYPE_CHECKING, cast

from ankiutils import app
from aqt import mw
from ex_autoslot import AutoSlots
from sysutils.lazy import Lazy
from sysutils.typed import non_optional
from sysutils.weak_ref import WeakRefable

if TYPE_CHECKING:
    from collections.abc import Callable

_addon_dir = os.path.dirname(os.path.dirname(__file__))
_addon_name = os.path.basename(_addon_dir)

def _get_config_dict() -> dict[str, object]:
    return mw.addonManager.getConfig(_addon_name) or {} if not app.is_testing else {}

_config_dict = Lazy(_get_config_dict)

def _write_config_dict() -> None:
    if not app.is_testing:
        mw.addonManager.writeConfig(_addon_name, _config_dict())  # pyright: ignore[reportUnknownMemberType]

class ConfigurationValue[T](WeakRefable, AutoSlots):
    def __init__(self, name: str, title: str, default: T, feature_toggler: Callable[[T], None] | None = None) -> None:
        self.title: str = title
        self.feature_toggler: Callable[[T], None] | None = feature_toggler
        self.name: str = name
        self._value: T = cast(T, _config_dict().get(name, default))

        if self.feature_toggler:
            app.add_init_hook(self.toggle_feature)

        self._change_callbacks: list[Callable[[T], None]] = []

    def get_value(self) -> T:
        return self._value

    def set_value(self, value: T) -> None:
        self._value = value
        _config_dict()[self.name] = value
        self.toggle_feature()
        _write_config_dict()
        for callback in self._change_callbacks:
            callback(self.get_value())

    def on_change(self, callback: Callable[[T], None]) -> None:
        self._change_callbacks.append(callback)

    def toggle_feature(self) -> None:
        if self.feature_toggler is not None:
            self.feature_toggler(self._value)

ConfigurationValueInt = ConfigurationValue[int]
ConfigurationValueFloat = ConfigurationValue[float]
ConfigurationValueBool = ConfigurationValue[bool]

class JapaneseConfig(AutoSlots):
    def __init__(self) -> None:
        self.boost_failed_card_allowed_time_by_factor: ConfigurationValueFloat = ConfigurationValueFloat("boost_failed_card_allowed_time_by_factor", "Boost Failed Card Allowed Time Factor", 1.5)
        self.boost_failed_card_allowed_time: ConfigurationValueBool = ConfigurationValueBool("boost_failed_card_allowed_time", "Boost failed card allowed time", True)

        def set_enable_fsrs_short_term_with_steps(toggle: bool) -> None:
            # noinspection PyProtectedMember, PyArgumentList
            non_optional(mw.col)._set_enable_fsrs_short_term_with_steps(toggle)  # pyright: ignore[reportPrivateUsage]

        self.autoadvance_vocab_starting_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_vocab_starting_seconds", "Starting Seconds", 3.0)
        self.autoadvance_vocab_hiragana_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_vocab_hiragana_seconds", "Hiragana Seconds", 0.7)
        self.autoadvance_vocab_katakana_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_vocab_katakana_seconds", "Katakana Seconds", 0.7)
        self.autoadvance_vocab_kanji_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_vocab_kanji_seconds", "Kanji Seconds", 1.5)

        self.autoadvance_sentence_starting_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_sentence_starting_seconds", "Starting Seconds", 3.0)
        self.autoadvance_sentence_hiragana_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_sentence_hiragana_seconds", "Hiragana Seconds", 0.7)
        self.autoadvance_sentence_katakana_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_sentence_katakana_seconds", "Katakana Seconds", 0.7)
        self.autoadvance_sentence_kanji_seconds: ConfigurationValueFloat = ConfigurationValueFloat("autoadvance_sentence_kanji_seconds", "Kanji Seconds", 1.5)

        self.timebox_vocab_read: ConfigurationValueInt = ConfigurationValueInt("time_box_length_vocab_read", "Vocab Read", 15)
        self.timebox_vocab_listen: ConfigurationValueInt = ConfigurationValueInt("time_box_length_vocab_listen", "Vocab Listen", 15)
        self.timebox_sentence_read: ConfigurationValueInt = ConfigurationValueInt("time_box_length_sentence_read", "Sentence Read", 15)
        self.timebox_sentence_listen: ConfigurationValueInt = ConfigurationValueInt("time_box_length_sentence_listen", "Sentence Listen", 15)
        self.timebox_kanji_read: ConfigurationValueInt = ConfigurationValueInt("time_box_length_kanji", "Kanji", 15)
        self.yomitan_integration_copy_answer_to_clipboard: ConfigurationValueBool = ConfigurationValueBool("yomitan_integration_copy_answer_to_clipboard", "Yomitan integration: Copy reviewer answer to clipboard", False)

        self.anki_internal_fsrs_set_enable_fsrs_short_term_with_steps: ConfigurationValueBool = ConfigurationValueBool("fsrs_set_enable_fsrs_short_term_with_steps",
                                                                                                                       "FSRS: Enable short term scheduler with steps",
                                                                                                                       default=False,
                                                                                                                       feature_toggler=set_enable_fsrs_short_term_with_steps)

        self.decrease_failed_card_intervals: ConfigurationValueBool = ConfigurationValueBool("decrease_failed_card_intervals", "Decrease failed card intervals", False)

        self.prevent_double_clicks: ConfigurationValueBool = ConfigurationValueBool("prevent_double_clicks", "Prevent double clicks", True)
        self.prefer_default_mnemonics_to_source_mnemonics: ConfigurationValueBool = ConfigurationValueBool("prefer_default_mnemocs_to_source_mnemonics", "Prefer default mnemonics to source mnemonics", False)

        self.show_compound_parts_in_sentence_breakdown: ConfigurationValueBool = ConfigurationValueBool("show_compound_parts_in_sentence_breakdown", "Show compound parts in sentence breakdown", True)
        self.show_kanji_in_sentence_breakdown: ConfigurationValueBool = ConfigurationValueBool("show_kanji_in_sentence_breakdown", "Show kanji in sentence breakdown", True)
        self.show_sentence_breakdown_in_edit_mode: ConfigurationValueBool = ConfigurationValueBool("show_sentence_breakdown_in_edit_mode", "Show sentence breakdown in edit mode", False)
        self.automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound: ConfigurationValueBool = ConfigurationValueBool("automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound", "Automatically yield last token in suru verb compounds to overlapping compounds (Ctrl+Shift+Alt+s)", True)
        self.automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound: ConfigurationValueBool = ConfigurationValueBool("automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound", "Automatically yield last token in passive verb compounds to overlapping compounds (Ctrl+Shift+Alt+h)", True)
        self.automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound: ConfigurationValueBool = ConfigurationValueBool("automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound", "Automatically yield last token in causative verb compounds to overlapping compounds (Ctrl+Shift+Alt+t)", True)

        self.decrease_failed_card_intervals_interval: ConfigurationValueInt = ConfigurationValueInt("decrease_failed_card_intervals_interval", "Failed card again seconds for next again", 60)

        self.minimum_time_viewing_question: ConfigurationValueFloat = ConfigurationValueFloat("minimum_time_viewing_question", "Minimum time viewing question", 0.5)
        self.minimum_time_viewing_answer: ConfigurationValueFloat = ConfigurationValueFloat("minimum_time_viewing_answer", "Minimum time viewing answer", 0.5)

        self.sentence_view_toggles: list[ConfigurationValueBool] = [self.automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound,
                                                                    self.automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound,
                                                                    self.automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound,
                                                                    self.show_compound_parts_in_sentence_breakdown,
                                                                    self.show_sentence_breakdown_in_edit_mode,
                                                                    self.show_kanji_in_sentence_breakdown]

        # performance
        self.load_jamdict_db_into_memory: ConfigurationValueBool = ConfigurationValueBool("load_jamdict_db_into_memory", "Load Jamdict DB into memory [Requires restart]", False)
        self.pre_cache_card_studying_status: ConfigurationValueBool = ConfigurationValueBool("pre_cache_card_studying_status", "Cache card studying status on startup. Only disable for dev/testing purposes. [Requires restart]", False)
        self.prevent_anki_from_garbage_collecting_every_time_a_window_closes: ConfigurationValueBool = ConfigurationValueBool("prevent_anki_from_garbage_collecting_every_time_a_window_closes", "Prevent Anki from garbage collecting every time a window closes, causing a short hang every time. [Requires restart]", True)
        self.load_studio_in_foreground: ConfigurationValueBool = ConfigurationValueBool("load_studio_in_foreground", "Load Studio in foreground. Makes it clear when done. Anki will be responsive when done. But you can't use anki while loading.", True)

        # memory
        self.enable_garbage_collection_during_batches: ConfigurationValueBool = ConfigurationValueBool("enable_garbage_collection_during_batches", "Enable Batch GC. [Requires restart]", True)
        self.enable_automatic_garbage_collection: ConfigurationValueBool = ConfigurationValueBool("enable_automatic_garbage_collection", "Enable automatic GC. [Requires restart. Reduces memory usage the most but slows Anki down and may cause crashes due to Qt incompatibility.]", False)
        self.track_instances_in_memory: ConfigurationValueBool = ConfigurationValueBool("track_instances_in_memory", "Track instances in memory. [Requires restart.. Only useful to developers and will use extra memory.]", False)

        self.feature_toggles: list[tuple[str, list[ConfigurationValueBool]]] = \
            [("Sentence Display", self.sentence_view_toggles),
             ("Misc", [self.yomitan_integration_copy_answer_to_clipboard,
                       self.anki_internal_fsrs_set_enable_fsrs_short_term_with_steps,
                       self.decrease_failed_card_intervals,
                       self.prevent_double_clicks,
                       self.boost_failed_card_allowed_time,
                       self.prefer_default_mnemonics_to_source_mnemonics]),
             ("Performance and memory usage", [self.load_studio_in_foreground,
                                               self.load_jamdict_db_into_memory,
                                               self.pre_cache_card_studying_status,
                                               self.prevent_anki_from_garbage_collecting_every_time_a_window_closes,
                                               self.enable_garbage_collection_during_batches,
                                               self.enable_automatic_garbage_collection,
                                               self.track_instances_in_memory])]

        self.readings_mappings_dict: dict[str, str] = self._read_reading_mappings_from_file()

    def toggle_all_sentence_display_auto_yield_flags(self, value: bool | None = None) -> None:
        value = value if value is not None else not self.automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound.get_value()
        self.automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound.set_value(value)
        self.automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound.set_value(value)
        self.automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound.set_value(value)

    def save_mappings(self, mappings: str) -> None:
        mappings_file_path = self._mappings_file_path()
        with open(mappings_file_path, "w", encoding="utf-8") as f:
            f.write(mappings)

        self.readings_mappings_dict = self._read_reading_mappings_from_file()

    def set_readings_mappings_for_testing(self, mappings: str) -> None:
        self.readings_mappings_dict = self._parse_mappings_from_string(mappings)

    @classmethod
    def read_readings_mappings_file(cls) -> str:
        with open(cls._mappings_file_path(), encoding="utf-8") as f:
            return f.read()

    @classmethod
    def _read_reading_mappings_from_file(cls) -> dict[str, str]:
        return cls._parse_mappings_from_string(cls.read_readings_mappings_file())

    @staticmethod
    def _parse_mappings_from_string(mappings_string: str) -> dict[str, str]:
        def parse_value_part(value_part: str) -> str:
            if "<read>" in value_part:
                return value_part
            if ":" in value_part:
                parts = value_part.split(":", 1)
                return f"""<read>{parts[0].strip()}</read>{parts[1]}"""
            return f"<read>{value_part}</read>"

        return {
            line.split(":", 1)[0].strip(): parse_value_part(line.split(":", 1)[1].strip())
            for line in mappings_string.strip().splitlines()
            if ":" in line
        }

    @staticmethod
    def _mappings_file_path() -> str:
        return os.path.join(app.user_files_dir, "readings_mappings.txt")

config: Lazy[JapaneseConfig] = Lazy(lambda: JapaneseConfig())
</file>

<file path="configuration/configuration.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from PyQt6.QtCore import pyqtBoundSignal
from PyQt6.QtWidgets import QDialog, QDialogButtonBox, QDoubleSpinBox, QGridLayout, QGroupBox, QLabel, QSpinBox, QVBoxLayout, QWidget
from sysutils.typed import checked_cast

if TYPE_CHECKING:
    from configuration.configuration_value import ConfigurationValueFloat, ConfigurationValueInt, JapaneseConfig

class JapaneseOptionsDialog(QDialog): # Cannot inherit Slots for some QT internal reason
    def __init__(self, parent: QWidget | None = None) -> None:
        super().__init__(parent)
        self.config: JapaneseConfig = app.config()

        # for some reason the dead code detector breaks some logic in pycharm here. This method is just fine
        # noinspection PyUnresolvedReferences
        self.setWindowTitle("Japanese Options")
        self.setMinimumWidth(300)

        window_layout = QVBoxLayout()

        def add_number_spinner_value(grid: QGridLayout, row: int, config_value: ConfigurationValueInt) -> None:
            label = QLabel(config_value.title)
            grid.addWidget(label, row, 0)

            # noinspection PyArgumentList
            number_input = QSpinBox()
            number_input.setRange(0, 99999)
            number_input.setValue(config_value.get_value())
            checked_cast(pyqtBoundSignal, number_input.valueChanged).connect(config_value.set_value)  # pyright: ignore[reportUnknownMemberType]
            grid.addWidget(number_input, row, 1)

        def add_double_spinner_value(grid: QGridLayout, row: int, config_value: ConfigurationValueFloat) -> None:
            label = QLabel(config_value.title)
            grid.addWidget(label, row, 0)

            # Create a QDoubleSpinBox for floating-point input
            double_input = QDoubleSpinBox()
            double_input.setRange(0.0, 100.0)  # Set the range for the floating-point values
            double_input.setDecimals(2)  # Set precision to 2 decimal places
            double_input.setValue(config_value.get_value())  # Set initial value
            double_input.setSingleStep(0.05)  # Set step size for increment/decrementb

            # Connect the valueChanged signal to update the config value
            checked_cast(pyqtBoundSignal, double_input.valueChanged).connect(config_value.set_value)  # pyright: ignore[reportUnknownMemberType]

            grid.addWidget(double_input, row, 1)

        def setup_decrease_failed_card_interval_section() -> None:
            failed_card_group = QGroupBox("Decrease failed card intervals")
            # noinspection PyArgumentList
            failed_card_layout = QGridLayout()
            failed_card_layout.setColumnStretch(0, 1)  # Make the label column expandable
            failed_card_layout.setColumnStretch(1, 0)  # Keep the spinner column fixed width

            add_number_spinner_value(failed_card_layout, 0, self.config.decrease_failed_card_intervals_interval)

            failed_card_group.setLayout(failed_card_layout)
            window_layout.addWidget(failed_card_group)

        def setup_boost_failed_card_allowed_time_section() -> None:
            failed_card_allowed_time_group = QGroupBox("Boost failed card allowed time")
            # noinspection PyArgumentList
            failed_card_allowed_time_layout = QGridLayout()
            failed_card_allowed_time_layout.setColumnStretch(0, 1)  # Make the label column expandable
            failed_card_allowed_time_layout.setColumnStretch(1, 0)  # Keep the spinner column fixed width

            add_double_spinner_value(failed_card_allowed_time_layout, 0, self.config.boost_failed_card_allowed_time_by_factor)

            failed_card_allowed_time_group.setLayout(failed_card_allowed_time_layout)
            window_layout.addWidget(failed_card_allowed_time_group)

        def setup_vocab_autoadvance_timings() -> None:
            vocab_autoadvance_timings_group = QGroupBox("Vocab autoadvance timings")
            # noinspection PyArgumentList
            vocab_autoadvance_timings_layout = QGridLayout()
            vocab_autoadvance_timings_layout.setColumnStretch(0, 1)  # Make the label column expandable
            vocab_autoadvance_timings_layout.setColumnStretch(1, 0)  # Keep the spinner column fixed width

            add_double_spinner_value(vocab_autoadvance_timings_layout, 0, self.config.autoadvance_vocab_starting_seconds)
            add_double_spinner_value(vocab_autoadvance_timings_layout, 1, self.config.autoadvance_vocab_hiragana_seconds)
            add_double_spinner_value(vocab_autoadvance_timings_layout, 2, self.config.autoadvance_vocab_katakana_seconds)
            add_double_spinner_value(vocab_autoadvance_timings_layout, 3, self.config.autoadvance_vocab_kanji_seconds)

            vocab_autoadvance_timings_group.setLayout(vocab_autoadvance_timings_layout)
            window_layout.addWidget(vocab_autoadvance_timings_group)

        def setup_sentence_autoadvance_timings() -> None:
            sentence_autoadvance_timings_group = QGroupBox("Sentence autoadvance timings")
            # noinspection PyArgumentList
            sentence_autoadvance_timings_layout = QGridLayout()
            sentence_autoadvance_timings_layout.setColumnStretch(0, 1)  # Make the label column expandable
            sentence_autoadvance_timings_layout.setColumnStretch(1, 0)  # Keep the spinner column fixed width

            add_double_spinner_value(sentence_autoadvance_timings_layout, 0, self.config.autoadvance_sentence_starting_seconds)
            add_double_spinner_value(sentence_autoadvance_timings_layout, 1, self.config.autoadvance_sentence_hiragana_seconds)
            add_double_spinner_value(sentence_autoadvance_timings_layout, 2, self.config.autoadvance_sentence_katakana_seconds)
            add_double_spinner_value(sentence_autoadvance_timings_layout, 3, self.config.autoadvance_sentence_kanji_seconds)

            sentence_autoadvance_timings_group.setLayout(sentence_autoadvance_timings_layout)
            window_layout.addWidget(sentence_autoadvance_timings_group)

        def setup_timeboxes_section() -> None:
            number_group = QGroupBox("Timeboxes")
            # noinspection PyArgumentList
            number_layout = QGridLayout()
            number_layout.setColumnStretch(0, 1)  # Make the label column expandable
            number_layout.setColumnStretch(1, 0)  # Keep the spinner column fixed width

            add_number_spinner_value(number_layout, 0, self.config.timebox_sentence_read)
            add_number_spinner_value(number_layout, 1, self.config.timebox_sentence_listen)
            add_number_spinner_value(number_layout, 2, self.config.timebox_vocab_read)
            add_number_spinner_value(number_layout, 3, self.config.timebox_vocab_listen)
            add_number_spinner_value(number_layout, 4, self.config.timebox_kanji_read)

            number_group.setLayout(number_layout)
            window_layout.addWidget(number_group)

        def setup_debounce_section() -> None:
            number_group = QGroupBox("Prevent accidental clicks")
            # noinspection PyArgumentList
            no_accidental_clicks_layout = QGridLayout()
            no_accidental_clicks_layout.setColumnStretch(0, 1)  # Make the label column expandable
            no_accidental_clicks_layout.setColumnStretch(1, 0)  # Keep the spinner column fixed width

            add_double_spinner_value(no_accidental_clicks_layout, 0, self.config.minimum_time_viewing_question)
            add_double_spinner_value(no_accidental_clicks_layout, 1, self.config.minimum_time_viewing_answer)

            number_group.setLayout(no_accidental_clicks_layout)
            window_layout.addWidget(number_group)

        setup_vocab_autoadvance_timings()
        setup_sentence_autoadvance_timings()
        setup_boost_failed_card_allowed_time_section()
        setup_decrease_failed_card_interval_section()
        setup_timeboxes_section()
        setup_debounce_section()

        # checkbox_group = QGroupBox("Feature toggles")
        # checkbox_layout = QVBoxLayout()
        # self.checkbox = QCheckBox(self.config.yomitan_integration_copy_answer_to_clipboard.title)
        # self.checkbox.setChecked(self.config.yomitan_integration_copy_answer_to_clipboard.get_value())
        # qconnect(self.checkbox.toggled, self.config.yomitan_integration_copy_answer_to_clipboard.set_value)
        # checkbox_layout.addWidget(self.checkbox)
        # checkbox_group.setLayout(checkbox_layout)
        # layout.addWidget(checkbox_group)

        self.button_box: QDialogButtonBox = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok)
        checked_cast(pyqtBoundSignal, self.button_box.clicked).connect(self.accept)  # pyright: ignore[reportUnknownMemberType]
        window_layout.addWidget(self.button_box)

        self.setLayout(window_layout)

def show_japanese_options() -> None:
    from aqt import mw
    JapaneseOptionsDialog(checked_cast(QWidget, mw)).exec()
</file>

<file path="configuration/readings_mapping_dialog.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from PyQt6.QtCore import pyqtBoundSignal
from PyQt6.QtGui import QColor, QKeySequence, QShortcut, QTextBlock, QTextCharFormat, QTextCursor
from PyQt6.QtWidgets import QDialog, QDialogButtonBox, QHBoxLayout, QLabel, QLineEdit, QTextEdit, QVBoxLayout, QWidget
from sysutils.ex_str import newline
from sysutils.typed import checked_cast, non_optional

if TYPE_CHECKING:
    from configuration.configuration_value import JapaneseConfig

class ReadingsOptionsDialog(QDialog): # Cannot inherit Slots for some QT internal reason
    def __init__(self, parent: QWidget | None = None) -> None:
        super().__init__(parent)
        self.config: JapaneseConfig = app.config()

        # for some reason the dead code detector breaks some logic in pycharm here. This method is just fine
        # noinspection PyUnresolvedReferences
        self.setWindowTitle("Readings Mappings")
        self.setMinimumWidth(500)

        mappings_text = newline + self.config.read_readings_mappings_file()

        # Create search field
        search_layout = QHBoxLayout()
        search_label = QLabel("Search:")
        self.search_edit: QLineEdit = QLineEdit(self)
        self.search_edit.setPlaceholderText("Type to search...")
        search_layout.addWidget(search_label)
        search_layout.addWidget(self.search_edit)

        # Connect search field to search function
        checked_cast(pyqtBoundSignal, self.search_edit.textChanged).connect(self.search_text)  # pyright: ignore[reportUnknownMemberType]

        self.text_edit: QTextEdit = QTextEdit(self)
        self.text_edit.setPlainText(mappings_text)

        window_layout = QVBoxLayout()
        window_layout.addLayout(search_layout)
        window_layout.addWidget(self.text_edit)

        self.button_box: QDialogButtonBox = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok)
        shortcut = "Alt+Return"
        self.button_box.setToolTip(f"Save ({shortcut})")
        save_shortcut = QShortcut(QKeySequence(shortcut), self)
        checked_cast(pyqtBoundSignal, save_shortcut.activated).connect(self.save)  # pyright: ignore[reportUnknownMemberType]
        checked_cast(pyqtBoundSignal, self.button_box.clicked).connect(self.save)  # pyright: ignore[reportUnknownMemberType]
        window_layout.addWidget(self.button_box)
        self.setLayout(window_layout)

        self.center_on_screen()

    def search_text(self, text: str) -> None:
        """Search for lines beginning with the search text and scroll to first match."""

        def reset_cursor_to_start() -> None:
            cursor_ = self.text_edit.textCursor()
            cursor_.movePosition(QTextCursor.MoveOperation.Start)
            self.text_edit.setTextCursor(cursor_)

        def remove_higlighting() -> None:
            format_ = QTextCharFormat()
            cursor_ = QTextCursor(document)
            cursor_.select(QTextCursor.SelectionType.Document)
            cursor_.setCharFormat(format_)

        def highlight_block(block_to_highlight: QTextBlock) -> None:
            cursor = QTextCursor(block_to_highlight)
            self.text_edit.setTextCursor(cursor)
            highlight_format = QTextCharFormat()
            highlight_format.setBackground(QColor(255, 255, 0, 70))  # Light yellow highlight
            cursor.select(QTextCursor.SelectionType.BlockUnderCursor)
            cursor.setCharFormat(highlight_format)

        def scroll_cursor_to_top() -> None:
            scrollbar = self.text_edit.verticalScrollBar()
            if scrollbar:
                scrollbar.setValue(scrollbar.maximum())
                self.text_edit.ensureCursorVisible()
                scrollbar_position_with_cursor_at_top = scrollbar.value()

                if scrollbar_position_with_cursor_at_top != scrollbar.maximum():
                    five_lines_height = self.text_edit.fontMetrics().height() * 5
                    scrollbar.setValue(scrollbar_position_with_cursor_at_top - five_lines_height)

        def find_block_starting_with_text() -> QTextBlock | None:
            for block_num_ in range(document.blockCount()):
                block_ = document.findBlockByNumber(block_num_)
                line_text_ = block_.text().strip()

                if line_text_.startswith(text):
                    return block_
            return None

        def find_block_containing_text() -> QTextBlock | None:
            for block_num_ in range(document.blockCount()):
                block_ = document.findBlockByNumber(block_num_)
                line_text_ = block_.text().strip()

                if text in line_text_:
                    return block_
            return None

        document = non_optional(self.text_edit.document())

        reset_cursor_to_start()
        remove_higlighting()
        block_starting_with_text = find_block_starting_with_text()
        if block_starting_with_text:
            highlight_block(block_starting_with_text)
            scroll_cursor_to_top()
            return

        block_containing_text = find_block_containing_text()
        if block_containing_text:
            highlight_block(block_containing_text)
            scroll_cursor_to_top()
            return

    def center_on_screen(self) -> None:
        available = non_optional(self.screen()).availableGeometry()
        self.setGeometry(available.x() + (available.width() - self.width()),
                         available.y() + 30,
                         self.width(),
                         300)

    def save(self) -> None:
        def sorted_value_lines_without_duplicates_or_blank_lines() -> str:
            lines = self.text_edit.toPlainText().splitlines()
            lines.reverse()  # the top latest lines are now the last lines and will overwrite earlier lines

            readings_mappings = {
                line.split(":", 1)[0].strip(): line.split(":", 1)[1].strip()
                for line in lines
                if ":" in line
            }

            new_lines = [f"{line[0]}:{line[1]}" for line in readings_mappings.items()]

            return "\n".join(sorted(new_lines))

        self.config.save_mappings(sorted_value_lines_without_duplicates_or_blank_lines())
        self.accept()

        app.get_ui_utils().refresh()

def show_readings_mappings() -> None:
    from aqt import mw
    dialog = ReadingsOptionsDialog(checked_cast(QWidget, mw))
    dialog.exec()
</file>

<file path="ex_autoslot.py">
from __future__ import annotations

import line_profiling_hacks
from manually_copied_in_libraries.autoslot import Slots, SlotsPlusDict

# when running line profiling we have to have __dict__ in our classes, when not running line profiling we do not want that overhead
if line_profiling_hacks.is_running_line_profiling:
    class AutoSlots(SlotsPlusDict):  # pyright: ignore [reportRedeclaration]
        pass
else:
    class AutoSlots(Slots):
        pass
</file>

<file path="language_services/conjugator.py">
from __future__ import annotations

from typing import TYPE_CHECKING

import mylog

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote

_ichidan_endings = ["", "ろ", "な"]

_godan_ru_endings = ["り", "ら", "れ", "っ"]
_godan_ru_or_ichidan_endings = _godan_ru_endings + _ichidan_endings

godan_potential_verb_ending_to_dictionary_form_endings: dict[str, str] = {"える": "う", "ける": "く", "げる": "ぐ", "せる": "す", "てる": "つ", "ねる": "ぬ", "べる": "ぶ", "める": "む", "れる": "る"}

e_stem_hiragana: set[str] = {"え", "け", "げ", "せ", "て", "ね", "べ", "め", "れ"}
e_stem_katakana: set[str] = {"エ", "ケ", "ゲ", "セ", "テ", "ネ", "ベ", "メ", "レ"}
e_stem_characters: set[str] = e_stem_hiragana | e_stem_katakana

a_stem_hiragana: set[str] = {"わ", "か", "が", "さ", "た", "な", "ば", "ま", "ら", }
a_stem_katakana: set[str] = {"ワ", "カ", "ガ", "サ", "タ", "ナ", "バ", "マ", "ラ"}
a_stem_characters: set[str] = a_stem_hiragana | a_stem_katakana

# o_row_hiragana: set[str] = {"を", "こ", "ご", "そ", "と", "の", "ぼ", "も", "ろ"}
# o_row_katakana: set[str] = {"オ", "コ", "ゴ", "ソ", "ト", "ノ", "ボ", "モ", "ロ"}
# passive_form_stem_characters: set[str] = a_stem_characters - {"な", "ナ"} # in practice な only brings false positives when searching for passive since passive ぬ verb usages are non-existent

_i_stem_index = 0
_a_stem_index = 1
_e_stem_index = 2
_te_stem_index = 3
_1_character_mappings: dict[str, list[str]] = {"う": ["い", "わ", "え", "っ"],
                                               "く": ["き", "か", "け", "い"],
                                               "ぐ": ["ぎ", "が", "げ", "い"],
                                               "す": ["し", "さ", "せ", "し"],
                                               "つ": ["ち", "た", "て", "っ"],
                                               "ぬ": ["に", "な", "ね", "ん"],
                                               "ぶ": ["び", "ば", "べ", "ん"],
                                               "む": ["み", "ま", "め", "ん"],
                                               "る": _godan_ru_endings,
                                               "い": ["く", "け", "か"]}

_2_character_mappings: dict[str, list[str]] = {"する": ["し", "さ", "すれ", "し", "せ"],
                                               "くる": ["き", "こ", "くれ", "き"],
                                               "いく": ["いき", "いか", "いけ", "いっ", "いこ"],
                                               "行く": ["行き", "行か", "行け", "行っ", "行こ"],
                                               "ます": ["まし", "ませ"],
                                               "いい": ["よく", "よけ", "よか", "よかっ"]}

_masu_forms_by_index = ["まし", "ませ", "まし", "まし"]  # not to sure about these. To say the least....

_aru_verbs: set[str] = {"なさる", "くださる", "おっしゃる", "ござる", "らっしゃる", "下さる", "為さる"}

_aru_mappings: dict[str, list[str]] = {"さる": ["さい", "さら", "され", "さっ"],
                                       "ざる": ["ざい", "ざら", "ざれ", "ざっ"],
                                       "ゃる": ["ゃい", "ゃら", "れば", "ゃっ"]}

def construct_root_verb_for_possibly_potential_godan_verb_dictionary_form(potential_verb_form: str) -> str:
    return potential_verb_form[:-2] + godan_potential_verb_ending_to_dictionary_form_endings[potential_verb_form[-2:]]

def _is_aru_verb(word: str) -> bool:
    return any(aru_ending for aru_ending in _aru_verbs if word.endswith(aru_ending))

def get_word_stems(word: str, is_ichidan_verb: bool = False, is_godan: bool = False) -> list[str]:
    try:
        if _is_aru_verb(word):
            return [word[:-2] + end for end in _aru_mappings[word[-2:]]]
        if is_ichidan_verb:
            return [word[:-1] + end for end in _ichidan_endings]
        if is_godan:
            return [word[:-1] + end for end in _1_character_mappings[word[-1]]]
        if word[-2:] in _2_character_mappings:
            return [word[:-2] + end for end in _2_character_mappings[word[-2:]]]
        if word[-1] in _1_character_mappings:
            if word[-1] == "る":
                return [word[:-1] + end for end in _godan_ru_or_ichidan_endings]
            return [word[:-1] + end for end in _1_character_mappings[word[-1]]]
    except (KeyError, IndexError):
        mylog.warning(f"get_word_stems failed to handle {word}, returning empty list ")
    return [word]

def _get_stem(word: str, stem_index: int, is_ichidan_verb: bool = False, is_godan: bool = False) -> str:
    try:
        if _is_aru_verb(word):
            return word[:-2] + _aru_mappings[word[-2:]][stem_index]
        if is_ichidan_verb:
            return word[:-1]
        if is_godan:
            return word[:-1] + _1_character_mappings[word[-1]][stem_index]
        if word[-2:] == "ます":
            return word[:-2] + _masu_forms_by_index[stem_index]
        if word[-2:] in _2_character_mappings:
            return word[:-2] + _2_character_mappings[word[-2:]][stem_index]
        if word[-1] in _1_character_mappings:
            if word[-1] != "る":
                return word[:-1] + _1_character_mappings[word[-1]][stem_index]
            return word[:-1] + _1_character_mappings[word[-1]][stem_index]
    except (KeyError, IndexError):
        mylog.warning(f"_get_stem failed to handle {word}, returning empty list ")
    return word

def get_i_stem(word: str, is_ichidan: bool = False, is_godan: bool = False) -> str:
    return _get_stem(word, _i_stem_index, is_ichidan, is_godan)

def get_a_stem(word: str, is_ichidan: bool = False, is_godan: bool = False) -> str:
    return _get_stem(word, _a_stem_index, is_ichidan, is_godan)

def get_e_stem(word: str, is_ichidan: bool = False, is_godan: bool = False) -> str:
    if is_ichidan: return word[:-1] + "れ"
    return _get_stem(word, _e_stem_index, is_ichidan, is_godan)

def get_te_stem(word: str, is_ichidan: bool = False, is_godan: bool = False) -> str:
    return _get_stem(word, _te_stem_index, is_ichidan, is_godan)

def get_i_stem_vocab(vocab: VocabNote, form: str = "") -> str:
    return get_i_stem(form if form else vocab.get_question(), vocab.parts_of_speech.is_ichidan(), vocab.parts_of_speech.is_godan())

def get_e_stem_vocab(vocab: VocabNote, form: str = "") -> str:
    return get_e_stem(form if form else vocab.get_question(), vocab.parts_of_speech.is_ichidan(), vocab.parts_of_speech.is_godan())

def get_a_stem_vocab(vocab: VocabNote, form: str = "") -> str:
    return get_a_stem(form if form else vocab.get_question(), vocab.parts_of_speech.is_ichidan(), vocab.parts_of_speech.is_godan())

def get_te_stem_vocab(vocab: VocabNote, form: str = "") -> str:
    return get_te_stem(form if form else vocab.get_question(), vocab.parts_of_speech.is_ichidan(), vocab.parts_of_speech.is_godan())

def get_imperative(word: str, is_ichidan: bool = False, is_godan: bool = False) -> str:
    if is_godan or (not is_ichidan and word[-1] != "る"):
        return get_e_stem(word, is_ichidan, is_godan)

    return word[:-1] + "ろ"
</file>

<file path="language_services/english_dictionary/english_dict_search.py">
from __future__ import annotations

import os

from ex_autoslot import AutoSlots
from sysutils.lazy import Lazy


class WordSense(AutoSlots):
    def __init__(self, definition: str, pos: str) -> None:
        self.definition: str = definition
        self.pos: str = pos

class EnglishWord(AutoSlots):
    def __init__(self, word: str, definition: str = "", pos: str = "") -> None:
        self.word: str = word
        self.lower_case_word: str = word.lower()
        self.senses: list[WordSense] = []

        # Add the initial sense if provided
        if definition or pos:
            self.add_sense(definition, pos)

    def add_sense(self, definition: str, pos: str) -> None:
        self.senses.append(WordSense(definition, pos))

class EnglishDictionary(AutoSlots):
    def __init__(self) -> None:
        self.words: list[EnglishWord] = []
        self.word_map: dict[str, EnglishWord] = {}  # Map words to their EnglishWord objects

        current_dir = os.path.dirname(os.path.abspath(__file__))
        data_file_path = os.path.join(current_dir, "data", "english_words.csv")

        with open(data_file_path, encoding="utf-8") as file:
            next(file)
            for line in file:
                parts = line.strip().split(",", 2)

                if parts and parts[0]:
                    word = parts[0]
                    pos = parts[1] if len(parts) > 1 else ""
                    definition = parts[2] if len(parts) > 2 else ""

                    # Check if the word already exists in our dictionary
                    lower_word = word.lower()
                    if lower_word in self.word_map:
                        # Add a new sense to the existing word
                        self.word_map[lower_word].add_sense(definition, pos)
                    else:
                        # Create a new EnglishWord
                        english_word = EnglishWord(word, definition, pos)
                        self.words.append(english_word)
                        self.word_map[lower_word] = english_word

        raw_word_list_path = os.path.join(current_dir, "data", "raw_word_list.txt")
        with open(raw_word_list_path, encoding="utf-8") as raw_file:
            for line in raw_file:
                word = line.strip()
                if word:
                    lower_word = word.lower()
                    if lower_word not in self.word_map:
                        english_word = EnglishWord(word, "no-definition", "")
                        self.words.append(english_word)
                        self.word_map[lower_word] = english_word

    @staticmethod
    def _shortest_word_first(word: EnglishWord) -> int:
        return len(word.lower_case_word)

    @staticmethod
    def _starting_with_first_then_shortest_first(word: EnglishWord, search_string: str) -> tuple[bool, int, str]:
        return not word.lower_case_word.startswith(search_string), len(word.lower_case_word), word.lower_case_word

    def words_containing_starting_with_first_then_by_shortest_first(self, search_string: str) -> list[EnglishWord]:
        search_string = search_string.lower()
        hits = [word for word in self.words if search_string in word.lower_case_word]

        def starting_with_first_then_shortest_first(word: EnglishWord) -> tuple[bool, int, str]:
            return self._starting_with_first_then_shortest_first(word, search_string)

        return sorted(hits, key=starting_with_first_then_shortest_first)

dictionary: Lazy[EnglishDictionary] = Lazy(lambda: EnglishDictionary())
</file>

<file path="language_services/hiragana_chart.py">
from __future__ import annotations

from ex_autoslot import AutoSlots


# noinspection PyUnusedName,PyUnusedClass
class HiraganaChart(AutoSlots):
    k_index: int = 1
    s_index: int = 2
    t_index: int = 3
    n_index: int = 4
    h_index: int = 5
    m_index: int = 6
    y_index: int = 7
    r_index: int = 8
    w_index: int = 9

    a_row_1: list[str] = ["あ", "か", "さ", "た", "な", "は", "ま", "や", "ら", "わ"]
    a_row_2: list[str] = ["　", "が", "ざ", "だ", "　", "ば", "　", "　", "　", "　"]
    a_row_3: list[str] = ["　", "　", "　", "　", "　", "ぱ", "　", "　", "　", "　"]

    i_row_1: list[str] = ["い", "き", "し", "ち", "に", "ひ", "み", "　", "り", "　"]
    i_row_2: list[str] = ["　", "ぎ", "じ", "ぢ", "　", "び", "　", "　", "　", "　"]
    i_row_3: list[str] = ["　", "　", "　", "　", "　", "ぴ", "　", "　", "　", "　"]

    u_row_1: list[str] = ["う", "く", "す", "つ", "ぬ", "ふ", "む", "ゆ", "る", "　"]
    u_row_2: list[str] = ["　", "ぐ", "ず", "づ", "　", "ぶ", "　", "　", "　", "　"]
    u_row_3: list[str] = ["　", "　", "　", "　", "　", "ぷ", "　", "　", "　", "　"]

    e_row_1: list[str] = ["え", "け", "せ", "て", "ね", "へ", "め", "　", "れ", "　"]
    e_row_2: list[str] = ["　", "げ", "ぜ", "で", "　", "べ", "　", "　", "　", "　"]
    e_row_3: list[str] = ["　", "　", "　", "　", "　", "ぺ", "　", "　", "　", "　"]

    o_row_1: list[str] = ["お", "こ", "そ", "と", "の", "ほ", "も", "よ", "ろ", "を"]
    o_row_2: list[str] = ["　", "ご", "ぞ", "ど", "　", "ぼ", "　", "　", "　", "　"]
    o_row_3: list[str] = ["　", "　", "　", "　", "　", "ぽ", "　", "　", "　", "　"]
</file>

<file path="language_services/jamdict_ex/dict_entry.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final

from ex_autoslot import AutoSlots
from queryablecollections.collections.q_list import QList
from queryablecollections.q_iterable import query
from sysutils import kana_utils
from sysutils.typed import checked_cast_generics, str_

if TYPE_CHECKING:
    from collections.abc import Sequence

    from jamdict.jmdict import JMDEntry, Sense  # pyright: ignore[reportMissingTypeStubs]

def _sense_is_transitive_verb(sense: Sense) -> bool:
    return any(pos_item == "transitive verb" for pos_item in sense.pos)  # pyright: ignore[reportUnknownVariableType, reportUnknownArgumentType, reportUnknownMemberType]

def _sense_is_intransitive_verb(sense: Sense) -> bool:
    return any(pos_item == "intransitive verb" for pos_item in sense.pos)  # pyright: ignore[reportUnknownVariableType, reportUnknownArgumentType, reportUnknownMemberType]

@final
class DictEntry(AutoSlots):
    def __init__(self, entry: JMDEntry, lookup_word: str, lookup_readings: list[str]) -> None:
        self.entry: JMDEntry = entry
        self.lookup_word: str = lookup_word
        self.lookup_readings: list[str] = lookup_readings

    def is_kana_only(self) -> bool:
        return not self.entry.kanji_forms or any(sense for sense
                                                 in self.entry.senses
                                                 if "word usually written using kana alone" in sense.misc)  # pyright: ignore[reportUnknownMemberType]

    @staticmethod
    def create(entries: Sequence[JMDEntry], lookup_word: str, lookup_reading: list[str]) -> QList[DictEntry]:
        return QList(DictEntry(entry, lookup_word, lookup_reading) for entry in entries)

    def has_matching_kana_form(self, search: str) -> bool:
        search = kana_utils.katakana_to_hiragana(search)  # todo: this converting to hiragana is worrisome. Is this really the behavior we want? What false positives might we run into?
        return any(search == kana_utils.katakana_to_hiragana(form) for form in self.kana_forms())

    def has_matching_kanji_form(self, search: str) -> bool:
        search = kana_utils.katakana_to_hiragana(search)  # todo: this converting to hiragana is worrisome. Is this really the behavior we want? What false positives might we run into?
        return any(search == kana_utils.katakana_to_hiragana(form) for form in self.kanji_forms())

    def kana_forms(self) -> list[str]: return [ent.text for ent in self.entry.kana_forms]  # pyright: ignore[reportUnknownMemberType]
    def kanji_forms(self) -> list[str]: return [ent.text for ent in self.entry.kanji_forms]  # pyright: ignore[reportUnknownMemberType]
    def valid_forms(self, force_allow_kana_only: bool = False) -> set[str]:
        return set(self.kana_forms()) | set(self.kanji_forms()) if self.is_kana_only() or force_allow_kana_only else set(self.kanji_forms())

    def priority_tags(self) -> set[str]:
        kanji_priorities: QList[str] = (query(self.entry.kanji_forms)
                                        .where(lambda form: str_(form.text) == self.lookup_word)  # pyright: ignore [reportUnknownArgumentType, reportUnknownMemberType]
                                        .select_many(lambda form: checked_cast_generics(list[str], form.pri))  # pyright: ignore [reportUnknownArgumentType, reportUnknownMemberType]
                                        .to_list())
        kana_priorities: list[str] = (query(self.entry.kana_forms)
                                      .where(lambda form: str_(form.text) in self.lookup_readings)  # pyright: ignore [reportUnknownArgumentType, reportUnknownMemberType]
                                      .select_many(lambda form: checked_cast_generics(list[str], form.pri))  # pyright: ignore [reportUnknownArgumentType, reportUnknownMemberType]
                                      .to_list())  # ex_sequence.flatten([str_(form.pri) for form in self.entry.kana_forms if str_(form.text) in self.lookup_readings])  # pyright: ignore [reportUnknownArgumentType, reportUnknownMemberType]

        return set(kanji_priorities + kana_priorities)

    def _is_transitive_verb(self) -> bool:
        return all(_sense_is_transitive_verb(sense) for sense in self.entry.senses)

    def _is_intransitive_verb(self) -> bool:
        return all(_sense_is_intransitive_verb(sense) for sense in self.entry.senses)

    def _is_verb(self) -> bool:
        return (any(_sense_is_intransitive_verb(sense) for sense in self.entry.senses)
                or any(_sense_is_transitive_verb(sense) for sense in self.entry.senses)
                or any(" verb " in s.pos[0] for s in self.entry.senses if len(s.pos) > 0))  # pyright: ignore[reportUnknownArgumentType, reportUnknownMemberType]

    def _answer_prefix(self) -> str:
        if self._is_verb():
            if self._is_intransitive_verb():
                return "to: "
            if self._is_transitive_verb():
                return "to{} "
            if self._is_to_be_verb():
                return "to: be: "

            return "to? "

        return ""

    def _is_to_be_verb(self) -> bool:
        if not self._is_verb():
            return False

        return (query(self.entry.senses)
                .select_many(lambda sense: sense.gloss)
                .select(lambda gloss: str_(gloss.text))  # pyright: ignore [reportUnknownArgumentType, reportUnknownMemberType]
                .all(lambda gloss: gloss.startswith("to be ")))

    def format_sense(self, sense: Sense) -> str:
        glosses_text = [str(gloss.text) for gloss in sense.gloss]  # pyright: ignore[reportUnknownArgumentType]
        glosses_text = [gloss.replace(" ", "-") for gloss in glosses_text]
        if self._is_verb():
            if all(gloss[:6] == "to-be-" for gloss in glosses_text):  # noqa: SIM108
                glosses_text = [gloss[6:] for gloss in glosses_text]
            else:
                glosses_text = [gloss[3:] if gloss[:3] == "to-" else gloss for gloss in glosses_text]
        return "/".join(glosses_text)

    def generate_answer(self) -> str:
        prefix = self._answer_prefix()
        senses = list(self.entry.senses)
        formatted_senses = [self.format_sense(sense) for sense in senses]
        return prefix + " | ".join(formatted_senses)

    _parts_of_speech_map = {
        "transitive verb": ["transitive"],
        "intransitive verb": ["intransitive"],
        "noun or participle which takes the aux. verb suru": ["suru verb"],
        "noun (common) (futsuumeishi)": ["noun"],
        "adjectival nouns or quasi-adjectives (keiyodoshi)": ["na-adjective"],
        "noun, used as a suffix": ["noun", "suffix"],
        "Godan verb with 'u' ending": ["godan verb"],
        "Godan verb with 'ru' ending": ["godan verb"],
        "Godan verb with 'mu' ending": ["godan verb"],
        "Godan verb with 'nu' ending": ["godan verb"],
        "Godan verb with 'gu' ending": ["godan verb"],
        "Godan verb with 'ku' ending": ["godan verb"],
        "Godan verb with 'su' ending": ["godan verb"],
        "Godan verb with 'bu' ending": ["godan verb"],
        "Godan verb with 'u' ending (special class)": ["godan verb", "special-class"],
        "Godan verb - -aru special class": ["godan verb", "special-class-aru"],
        "Godan verb with 'tsu' ending": ["godan verb"],
        "irregular nu verb": ["nu verb"],
        "Godan verb - Iku/Yuku special class": ["godan verb", "special-class"],
        "Godan verb with 'ru' ending (irregular verb)": ["godan verb", "irregular"],
        "Ichidan verb": ["ichidan verb"],
        "Ichidan verb - zuru verb (alternative form of -jiru verbs)": ["ichidan verb", "zuru verb"],
        "Kuru verb - special class": ["kuru verb", "special-class"],
        "Yodan verb with 'ru' ending (archaic)": ["yodan verb"],
        "Yodan verb with 'ku' ending (archaic)": ["yodan verb"],
        "Nidan verb (lower class) with 'ru' ending (archaic)": ["nidan verb"],
        "Nidan verb (upper class) with 'ru' ending (archaic)": ["nidan verb"],
        "adjective (keiyoushi)": ["i-adjective"],
        "adjective (keiyoushi) - yoi/ii class": ["i-adjective"],
        "adverb (fukushi)": ["adverb"],
        "adverb taking the 'to' particle": ["to-adverb"],
        "auxiliary": ["auxiliary"],
        "auxiliary adjective": ["adjective", "auxiliary"],
        "auxiliary verb": ["auxiliary"],
        "conjunction": ["conjunction"],
        "copula": ["copula"],
        "expressions (phrases, clauses, etc.)": ["expression"],
        "interjection (kandoushi)": ["interjection"],
        "irregular ru verb, plain form ends with -ri": [""],
        "noun, used as a prefix": ["prefix", "noun"],
        "nouns which may take the genitive case particle 'no'": ["noun", "no-adjective"],
        "particle": ["particle"],
        "pre-noun adjectival (rentaishi)": ["pre-noun-adjectival"],
        "prefix": ["prefix"],
        "pronoun": ["pronoun"],
        "suffix": ["suffix"],
        "suru verb - included": ["suru verb"],
        "suru verb": ["suru verb"],
        "su verb - precursor to the modern suru": ["su verb"],
        "counter": ["counter"],
        "numeric": ["numeric"],
        "noun or verb acting prenominally": ["prenominal"],
        "suru verb - special class": ["suru verb", "special-class"],
        "Ichidan verb - kureru special class": ["ichidan verb", "special-class"],
        "'taru' adjective": ["taru-adjective"]

    }
    def parts_of_speech(self) -> set[str]:
        def try_get_our_pos_name(pos: str) -> list[str]:
            return self._parts_of_speech_map[pos] if pos in self._parts_of_speech_map else ["unmapped-pos-" + pos]

        return (query(self.entry.senses)
                .select_many(lambda sense: checked_cast_generics(list[str], sense.pos))  # pyright: ignore [reportUnknownArgumentType, reportUnknownMemberType]
                .select_many(try_get_our_pos_name)
                .to_set())
</file>

<file path="language_services/jamdict_ex/dict_lookup.py">
from __future__ import annotations

from functools import cache
from typing import TYPE_CHECKING, Any

from ankiutils import app
from ex_autoslot import AutoSlots
from language_services.jamdict_ex.priority_spec import PrioritySpec
from queryablecollections.collections.q_list import QList
from queryablecollections.q_iterable import query
from sysutils.lazy import Lazy
from sysutils.timeutil import StopWatch
from sysutils.typed import non_optional, str_

if TYPE_CHECKING:
    from collections.abc import Callable

    from jamdict.jmdict import JMDEntry  # pyright: ignore[reportMissingTypeStubs]
    from jamdict.util import LookupResult  # pyright: ignore[reportMissingTypeStubs]
    from note.vocabulary.vocabnote import VocabNote

import queue
import threading
from concurrent.futures import Future

from jamdict import Jamdict  # pyright: ignore[reportMissingTypeStubs]
from language_services.jamdict_ex.dict_entry import DictEntry
from sysutils import kana_utils


class Request[T](AutoSlots):
    def __init__(self, func: Callable[[Jamdict], T], future: Future[T]) -> None:
        self.func: Callable[[Jamdict], T] = func
        self.future: Future[T] = future

class JamdictThreadingWrapper(AutoSlots):
    def __init__(self) -> None:
        self._queue: queue.Queue[Request[Any]] = queue.Queue()  # pyright: ignore[reportExplicitAny]
        self._thread: threading.Thread = threading.Thread(target=self._worker, daemon=True)
        self._running: bool = True
        self._thread.start()
        self.jamdict: Lazy[Jamdict] = Lazy(self.create_jamdict_and_log_loading_time)

    @staticmethod
    def create_jamdict_and_log_loading_time() -> Jamdict:
        jamdict = Jamdict(memory_mode=True) \
            if (app.config().load_jamdict_db_into_memory.get_value()
                and not app.is_testing) \
            else Jamdict(reuse_ctx=True)
        jamdict.lookup("俺", lookup_chars=False, lookup_ne=True)  # pyright: ignore[reportUnknownMemberType]
        return jamdict

    def _worker(self) -> None:
        while self._running:
            request = self._queue.get()
            try:
                result = request.func(self.jamdict())  # pyright: ignore[reportAny]
                request.future.set_result(result)
            except Exception as e:
                request.future.set_exception(e)

    def lookup(self, word: str, include_names: bool) -> LookupResult:
        future: Future[LookupResult] = Future()

        def do_actual_lookup(jamdict: Jamdict) -> LookupResult:
            return jamdict.lookup(word, lookup_chars=False, lookup_ne=include_names)  # pyright: ignore[reportUnknownMemberType]

        self._queue.put(Request(do_actual_lookup, future))
        return future.result()

    def run_string_query(self, sql_query: str) -> list[str]:
        def perform_query(jamdict: Jamdict) -> list[str]:
            result: list[str] = []
            for batch in non_optional(non_optional(jamdict.jmdict).ctx().conn).execute(sql_query):  # pyright: ignore[reportAny]
                for row in batch:  # pyright: ignore[reportAny]
                    result.append(str_(row))  # noqa: PERF401  # pyright: ignore[reportAny]

            return result

        future: Future[list[str]] = Future()

        self._queue.put(Request(perform_query, future))
        return future.result()

_jamdict_threading_wrapper: JamdictThreadingWrapper = JamdictThreadingWrapper()

# noinspection SqlResolve
def _find_all_words() -> set[str]:
    with StopWatch.log_execution_time("Prepopulating all word forms from jamdict."):
        kanji_forms: set[str] = set(_jamdict_threading_wrapper.run_string_query("SELECT distinct text FROM Kanji"))
        kana_forms: set[str] = set(_jamdict_threading_wrapper.run_string_query("SELECT distinct text FROM Kana"))
    return kanji_forms | kana_forms

# noinspection SqlResolve
def _find_all_names() -> set[str]:
    with StopWatch.log_execution_time("Prepopulating all name forms from jamdict."):
        kanji_forms: set[str] = set(_jamdict_threading_wrapper.run_string_query("SELECT distinct text FROM NEKanji"))
        kana_forms: set[str] = set(_jamdict_threading_wrapper.run_string_query("SELECT distinct text FROM NEKana"))
        return kanji_forms | kana_forms

_all_word_forms = Lazy(_find_all_words)
_all_name_forms = Lazy(_find_all_names)

# todo: This mixes up static querying with the results. Let's keep the two separate and readable huh?
class DictLookup(AutoSlots):
    def __init__(self, entries: QList[DictEntry], lookup_word: str, lookup_reading: QList[str]) -> None:
        self.word: str = lookup_word
        self.lookup_reading: QList[str] = lookup_reading
        self.entries: QList[DictEntry] = entries

    @staticmethod
    def _failed_for_word(word: str) -> DictLookup:
        return DictLookup(QList(), word, QList())

    def found_words_count(self) -> int: return len(self.entries)
    def found_words(self) -> bool: return len(self.entries) > 0

    def is_uk(self) -> bool: return any(ent for ent
                                        in self.entries
                                        if ent.is_kana_only())

    def valid_forms(self, force_allow_kana_only: bool = False) -> set[str]:
        return query(self.entries).select_many(lambda entry: entry.valid_forms(force_allow_kana_only)).to_set()  # set(ex_sequence.flatten([list(entry.valid_forms(force_allow_kana_only)) for entry in self.entries]))

    def parts_of_speech(self) -> set[str]:
        return query(self.entries).select_many(lambda entry: entry.parts_of_speech()).to_set()  # set(ex_sequence.flatten([list(ent.parts_of_speech()) for ent in self.entries]))

    def priority_spec(self) -> PrioritySpec:
        return PrioritySpec(self.entries.select_many(lambda entry: entry.priority_tags()).to_set())  # PrioritySpec(set(ex_iterable.flatten(entry.priority_tags() for entry in self.entries)))

    @classmethod
    def lookup_vocab_word_or_name(cls, vocab: VocabNote) -> DictLookup:
        if vocab.readings.get():
            return cls.lookup_word_or_name_with_matching_reading(vocab.question.without_noise_characters, vocab.readings.get())

        return cls.lookup_word_or_name(vocab.question.without_noise_characters)

    @classmethod
    def lookup_word_or_name_with_matching_reading(cls, word: str, readings: list[str]) -> DictLookup:
        if len(readings) == 0: raise ValueError("readings may not be empty. If you want to match without filtering on reading, use lookup_word_or_name instead")
        return cls._try_lookup_word_or_name_with_matching_reading(word, tuple(readings))

    @classmethod
    @cache
    def _try_lookup_word_or_name_with_matching_reading(cls, word: str, readings: tuple[str, ...]) -> DictLookup:  # needs to be a tuple to be hashable for caching
        if not cls.might_be_entry(word): return DictLookup._failed_for_word(word)

        def kanji_form_matches() -> QList[DictEntry]:
            return (lookup
                    .where(lambda entry: any(entry.has_matching_kana_form(reading) for reading in readings)
                                         and len(entry.kanji_forms()) > 0
                                         and entry.has_matching_kanji_form(word))
                    .to_list())
            # return [ent for ent in lookup
            #         if any(ent.has_matching_kana_form(reading) for reading in readings)
            #         and ent.kanji_forms()
            #         and ent.has_matching_kanji_form(word)]

        def any_kana_only_matches() -> QList[DictEntry]:
            return (lookup.where(lambda entry: any(entry.has_matching_kana_form(reading) for reading in readings)
                                               and entry.is_kana_only()).to_list())
            # return [ent for ent in lookup
            #         if any(ent.has_matching_kana_form(reading) for reading in readings)
            #         and ent.is_kana_only()]

        lookup: QList[DictEntry] = DictEntry.create(cls._lookup_word_raw(word), word, list(readings))
        if not lookup:
            lookup = DictEntry.create(cls._lookup_name_raw(word), word, list(readings))

        matching = any_kana_only_matches() if kana_utils.is_only_kana(word) else kanji_form_matches()

        return DictLookup(matching, word, QList(readings))

    @classmethod
    def lookup_word_or_name(cls, word: str) -> DictLookup:
        if not cls.might_be_entry(word): return DictLookup._failed_for_word(word)
        word_hit = cls.lookup_word(word)
        if word_hit.found_words():
            return word_hit
        return cls.lookup_name(word)

    @classmethod
    def lookup_word(cls, word: str) -> DictLookup:
        if not cls.might_be_word(word): return DictLookup._failed_for_word(word)
        entries = DictEntry.create(cls._lookup_word_raw(word), word, [])
        return DictLookup(entries, word, QList())

    @classmethod
    def lookup_name(cls, word: str) -> DictLookup:
        if not cls.might_be_word(word): return DictLookup._failed_for_word(word)
        entries = DictEntry.create(cls._lookup_name_raw(word), word, [])
        return DictLookup(entries, word, QList())

    @classmethod
    @cache  # _lookup_word_shallow.cache_clear(), _lookup_word_shallow.cache_info()
    def _lookup_word_raw(cls, word: str) -> list[JMDEntry]:
        if not cls.might_be_word(word): return []

        entries = list(_jamdict_threading_wrapper.lookup(word, include_names=False).entries)
        return entries if not kana_utils.is_only_kana(word) else [ent for ent in entries if cls._is_kana_only(ent)]

    @classmethod
    @cache  # _lookup_word_shallow.cache_clear(), _lookup_word_shallow.cache_info()
    def _lookup_name_raw(cls, word: str) -> list[JMDEntry]:
        if not cls.might_be_name(word): return []
        return list(_jamdict_threading_wrapper.lookup(word, include_names=True).names)

    @staticmethod
    def _is_kana_only(entry: JMDEntry) -> bool:
        return not entry.kanji_forms or any(sense for sense
                                            in entry.senses
                                            if "word usually written using kana alone" in sense.misc)  # pyright: ignore[reportUnknownMemberType]

    @classmethod
    def might_be_word(cls, word: str) -> bool:
        # this method is a pure optimization to save on dictionary calls during real runtime. During tests populating all the words is a suboptimization, so just always return true when testing
        return app.is_testing or word in _all_word_forms()

    @classmethod
    def might_be_name(cls, word: str) -> bool:
        # this method is a pure optimization to save on dictionary calls during real runtime. During tests populating all the words is a suboptimization, so just always return true when testing
        return app.is_testing or word in _all_name_forms()

    @classmethod
    def might_be_entry(cls, word: str) -> bool:
        return cls.might_be_word(word) or cls.might_be_name(word)

    @classmethod
    @cache
    def is_word(cls, word: str) -> bool:
        return cls.might_be_word(word) and cls.lookup_word(word).found_words()

    @classmethod
    @cache
    def is_dictionary_or_collection_word(cls, word: str) -> bool:
        from ankiutils import app
        return app.col().vocab.is_word(word) or cls.is_word(word)

    @classmethod
    def ensure_loaded_into_memory(cls) -> None:
        cls._lookup_name_raw("桜")
        cls._lookup_word_raw("俺")
</file>

<file path="language_services/jamdict_ex/priority_spec.py">
from __future__ import annotations

from ex_autoslot import AutoSlots

_frequency_maximum = {f"nf{num:02}" for num in range(1, 10)}
_frequency_high = {f"nf{num}" for num in range(11, 20)}
_frequency_medium = {f"nf{num}" for num in range(21, 40)}
_frequency_low = {f"nf{num}" for num in range(41, 60)}

_tags_maximum = {"ichi1"}
_tags_high = {"news1", "spec1"}
_tags_medium = {"news2", "spec2"}

class PrioritySpec(AutoSlots):
    def __init__(self, tags: set[str]) -> None:
        self.tags: set[str] = tags

        if self.tags & _tags_maximum:
            self.priority_string: str = "priority_maximum"
            self.priority: int = 1
        elif self.tags & _tags_high:
            self.priority_string = "priority_high"
            self.priority = 11
        elif self.tags & _tags_medium:
            self.priority_string = "priority_medium"
            self.priority = 21
        elif self.tags & _frequency_maximum:
            self.priority_string = "priority_maximum"
            self.priority = int(list(self.tags & _frequency_maximum)[0][-1])  # the actual number from the nf tag
        elif self.tags & _frequency_high:
            self.priority_string = "priority_high"
            self.priority = int(list(self.tags & _frequency_high)[0][-2:])  # the actual number from the nf tag
        elif self.tags & _frequency_medium:
            self.priority_string = "priority_medium"
            self.priority = int(list(self.tags & _frequency_high)[0][-2:])  # the actual number from the nf tag
        elif self.tags & _frequency_low:
            self.priority_string = "priority_low"
            self.priority = int(list(self.tags & _frequency_high)[0][-2:])  # the actual number from the nf tag
        else:
            self.priority_string = "priority_low"
            self.priority = 50
</file>

<file path="language_services/janome_ex/tokenizing/inflection_forms.py">
# noinspection PyUnusedClass, PyUnusedName
from __future__ import annotations

from typing import override

from ex_autoslot import AutoSlots

all_dict: dict[str, InflectionForm] = {}

class InflectionForm(AutoSlots):
    def __init__(self, name: str, description: str) -> None:
        self.name: str = name
        self.description: str = description

    @override
    def __repr__(self) -> str: return f"""{self.name} - {self.description}"""
    @override
    def __str__(self) -> str: return self.name

    @override
    def __eq__(self, other: object) -> bool:
        if isinstance(other, InflectionForm):
            return self.name == other.name
        return False

    @override
    def __hash__(self) -> int: return hash(self.name)

def _add_form(name: str, description: str) -> InflectionForm:
    form = InflectionForm(name, description)
    all_dict[name] = form
    return form

# noinspection PyUnusedClass,PyUnusedName
class InflectionForms(AutoSlots):
    unknowm: InflectionForm = _add_form("*", "Unknown")

    # noinspection PyUnusedClass,PyUnusedName
    class Basic(AutoSlots):
        dictionary_form: InflectionForm = _add_form("基本形", "Basic form - Dictionary/plain form")
        gemination: InflectionForm = _add_form("基本形-促音便", "Basic form with gemination - Dictionary form with consonant doubling")
        euphonic: InflectionForm = _add_form("音便基本形", "Euphonic basic form - Dictionary form with sound changes")
        classical: InflectionForm = _add_form("文語基本形", "Classical basic form - Dictionary form in classical Japanese")

    # noinspection PyUnusedClass,PyUnusedName
    class Continuative(AutoSlots):
        renyoukei_masu_stem: InflectionForm = _add_form("連用形", "Continuative - masu-stem/i-stem [読み] - verbs/adjective")
        te_connection: InflectionForm = _add_form("連用テ接続", "Continuative te-connection - Connects to te-form")
        ta_connection: InflectionForm = _add_form("連用タ接続", 'Continuative ta-connection - Connects to past tense marker "ta"')
        de_connection: InflectionForm = _add_form("連用デ接続", 'Continuative de-connection - Connects to the te-form variant "de"')
        ni_connection: InflectionForm = _add_form("連用ニ接続", 'Continuative ni-connection - Connects to the particle "ni"')
        gozai_connection: InflectionForm = _add_form("連用ゴザイ接続", 'Continuative gozai connection - Links to polite auxiliary "gozai" (polite form)')

    # noinspection PyUnusedClass,PyUnusedName
    class Misc(AutoSlots):
        garu_connection: InflectionForm = _add_form("ガル接続", 'Garu connection - Connects to suffix "garu" (showing signs of)')

    # noinspection PyUnusedClass,PyUnusedName
    class Irrealis(AutoSlots):
        general_irrealis_mizenkei: InflectionForm = _add_form("未然形", "Irrealis - a-stem [読ま] - for negatives and some auxiliaries")
        special_irrealis: InflectionForm = _add_form("未然特殊", "Irrealis - Special - Special form of the negative stem")
        u_connection: InflectionForm = _add_form("未然ウ接続", 'Irrealis u-connection - Connects to volitional auxiliary "u"')
        nu_connection: InflectionForm = _add_form("未然ヌ接続", 'Irrealis nu-connection - Connects to classical negative "nu"')
        reru_connection: InflectionForm = _add_form("未然レル接続", 'Irrealis reru-connection - Connects to passive/potential "reru"')

    # noinspection PyUnusedClass,PyUnusedName
    class Hypothetical(AutoSlots):
        general_hypothetical_kateikei: InflectionForm = _add_form("仮定形", "Hypothetical/potential - e-stem [読め] - of verbs and adjectives.")
        contraction1: InflectionForm = _add_form("仮定縮約１", "Hypothetical form of verbs and adjectives contraction version 1")
        contraction2: InflectionForm = _add_form("仮定縮約２", "Hypothetical form of verbs and adjectives contraction version 2")

    # noinspection PyUnusedClass,PyUnusedName
    class ImperativeMeireikei(AutoSlots):
        e: InflectionForm = _add_form("命令ｅ", 'Imperative/command/meireikei -  e-stem[読め] - Command form ending in "e"')
        ro: InflectionForm = _add_form("命令ｒｏ", 'Imperative/command/meireikei - ro [食べろ] - Command form ending in "ro"')
        yo: InflectionForm = _add_form("命令ｙｏ", 'Imperative/command/meireikei - yo [食べよ] - Command form ending in "yo"')
        i: InflectionForm = _add_form("命令ｉ", 'Imperative/command/meireikei - i-form - Command form ending in "i"')

    # noinspection PyUnusedClass,PyUnusedName
    class NounConnection(AutoSlots):
        general_noun_connection: InflectionForm = _add_form("体言接続", "Noun connection - Form that connects to nouns")
        special_1: InflectionForm = _add_form("体言接続特殊", "Special noun connection - Connects to nouns in special cases")
        special_2: InflectionForm = _add_form("体言接続特殊２", "Special noun connection 2 - Another variant of noun connection")
</file>

<file path="language_services/janome_ex/tokenizing/inflection_types.py">
# noinspection PyUnusedClass, PyUnusedName
from __future__ import annotations

from typing import override

from ex_autoslot import AutoSlots

all_dict: dict[str, InflectionType] = {}

class InflectionType(AutoSlots):
    def __init__(self, name: str, description: str) -> None:
        self.name: str = name
        self.description: str = description

    @override
    def __repr__(self) -> str: return f"""{self.name} - {self.description}"""
    @override
    def __str__(self) -> str: return self.name

    @override
    def __eq__(self, other: object) -> bool:
        if isinstance(other, InflectionType):
            return self.name == other.name
        return False

def _add_form(name: str, description: str) -> InflectionType:
    form = InflectionType(name, description)
    all_dict[name] = form
    return form

# noinspection PyUnusedClass,PyUnusedName
class InflectionTypes(AutoSlots):
    unknown: InflectionType = _add_form("*", "Unknown")

    # noinspection PyUnusedClass,PyUnusedName
    class Godan(AutoSlots):
        su: InflectionType = _add_form("五段・サ行", "Godan verb with 'su' ending")
        mu: InflectionType = _add_form("五段・マ行", "Godan verb with 'mu' ending")
        bu: InflectionType = _add_form("五段・バ行", "Godan verb with 'bu' ending")
        gu_ending: InflectionType = _add_form("五段・ガ行", "Godan verb with 'gu' ending")
        tsu: InflectionType = _add_form("五段・タ行", "Godan verb with 'tsu' ending")
        nu: InflectionType = _add_form("五段・ナ行", "Godan verb with 'nu' ending")
        ru: InflectionType = _add_form("五段・ラ行", "Godan verb with 'ru' ending")
        ru_special: InflectionType = _add_form("五段・ラ行特殊", "Special godan verb with 'ru' ending - Irregular 'ru' conjugation pattern")
        ru_ending_aru: InflectionType = _add_form("五段・ラ行アル", "Godan verb 'aru' (to exist) with 'ru' ending - Special case verb")

        u_gemination: InflectionType = _add_form("五段・ワ行促音便", "Godan verb with 'u' ending and 'っ' consonant assimilation - Special 'u' conjugation pattern")
        u_u_sound: InflectionType = _add_form("五段・ワ行ウ音便", "Godan verb with 'u' ending and 'u' sound change - Special 'u' conjugation pattern")

        ku_gemination_yuku: InflectionType = _add_form("五段・カ行促音便ユク", "Godan verb 'yuku' (to go) with 'ku' ending and consonant assimilation - Special case verb")
        ku_gemination: InflectionType = _add_form("五段・カ行促音便", "Godan verb with 'ku' ending and consonant assimilation - Special 'ku' conjugation pattern")
        ku_i_sound: InflectionType = _add_form("五段・カ行イ音便", "Godan verb with 'ku' ending and 'i' sound change - Special 'ku' conjugation pattern")

    # noinspection PyUnusedClass,PyUnusedName
    class Ichidan(AutoSlots):
        regular: InflectionType = _add_form("一段", "Ichidan (one-step) verb - Regular -eru/-iru verb pattern")
        #turns out this is not what we need, rather it only occurs in rare cases like ありうる
        eru: InflectionType = _add_form("一段・得ル", "Ichidan verb 'eru' (to get/obtain) - Special case ichidan verb")
        kureru: InflectionType = _add_form("一段・クレル", "Ichidan verb 'kureru' (to give) - Special case ichidan verb")

    # noinspection PyUnusedClass,PyUnusedName
    class Adjective(AutoSlots):
        i_ending: InflectionType = _add_form("形容詞・イ段", "I-adjective - Ends with 'i' and conjugates like 'takai' (high)")
        auo_ending: InflectionType = _add_form("形容詞・アウオ段", "Adjective with 'a', 'u', 'o' row - Special adjective conjugation pattern")
        ii: InflectionType = _add_form("形容詞・イイ", "Adjective 'ii' (good) - Special case adjective")

    # noinspection PyUnusedClass,PyUnusedName
    class Sahen(AutoSlots):
        suru: InflectionType = _add_form("サ変・スル", "Suru verb - Irregular verb 'suru' (to do)")
        suru_compound: InflectionType = _add_form("サ変・−スル", "Suru compound verb - Noun + suru combination")
        zuru: InflectionType = _add_form("サ変・−ズル", "Zuru verb - Classical variation of suru")

    # noinspection PyUnusedClass,PyUnusedName
    class Kahen(AutoSlots):
        kuru_kanji: InflectionType = _add_form("カ変・来ル", "Kuru verb - Irregular verb 'kuru' (to come) in kanji form")
        kuru_kana: InflectionType = _add_form("カ変・クル", "Kuru verb - Irregular verb 'kuru' (to come) in kana form")

    # noinspection PyUnusedClass,PyUnusedName
    class Bungo(AutoSlots):
        nari: InflectionType = _add_form("文語・ナリ", "Classical 'nari' - Classical Japanese copula")
        ru: InflectionType = _add_form("文語・ル", "Classical 'ru' ending - Classical verb ending")
        ki: InflectionType = _add_form("文語・キ", "Classical 'ki' ending - Classical past tense marker")
        gotoshi: InflectionType = _add_form("文語・ゴトシ", "Classical 'gotoshi' - Classical expression meaning 'like/as if'")
        keri: InflectionType = _add_form("文語・ケリ", "Classical 'keri' - Classical perfect aspect marker")
        ri: InflectionType = _add_form("文語・リ", "Classical 'ri' - Classical verb ending")
        beshi: InflectionType = _add_form("文語・ベシ", "Classical 'beshi' - Classical expression of obligation/probability")

    # noinspection PyUnusedClass,PyUnusedName
    class Special(AutoSlots):
        masu: InflectionType = _add_form("特殊・マス", "Special 'masu' form - Polite verb ending")
        ya: InflectionType = _add_form("特殊・ヤ", "Special 'ya' - Dialectal copula/question marker")
        ja: InflectionType = _add_form("特殊・ジャ", "Special 'ja' - Dialectal copula")
        ta: InflectionType = _add_form("特殊・タ", "Special 'ta' - Past tense marker")
        nai: InflectionType = _add_form("特殊・ナイ", "Special 'nai' - Negative form")
        nu: InflectionType = _add_form("特殊・ヌ", "Special 'nu' - Classical negative")
        da: InflectionType = _add_form("特殊・ダ", "Special 'da' - Copula (is/am/are)")
        tai: InflectionType = _add_form("特殊・タイ", "Special 'tai' - Desire form (-want to do)")
        desu: InflectionType = _add_form("特殊・デス", "Special 'desu' - Polite copula")

    # noinspection PyUnusedClass,PyUnusedName
    class Yodan(AutoSlots):
        ha_ending: InflectionType = _add_form("四段・ハ行", "Classical yodan verb with 'ha' ending - Classical conjugation pattern")
        ba_ending: InflectionType = _add_form("四段・バ行", "Classical yodan verb with 'ba' ending - Classical conjugation pattern")

    # noinspection PyUnusedClass,PyUnusedName
    class Ruhen(AutoSlots):
        ra_hen: InflectionType = _add_form("ラ変", "Classical ra-hen irregular verb - Classical irregular conjugation")

    # noinspection PyUnusedClass,PyUnusedName
    class Nidan(AutoSlots):
        lower_da: InflectionType = _add_form("下二・ダ行", "Lower bigrade with 'da' ending - Classical conjugation pattern")
        lower_ta: InflectionType = _add_form("下二・タ行", "Lower bigrade with 'ta' ending - Classical conjugation pattern")

    # noinspection PyUnusedClass,PyUnusedName
    class Other(AutoSlots):
        indeclinable: InflectionType = _add_form("不変化型", "Indeclinable type - Words that don't conjugate")
</file>

<file path="language_services/janome_ex/tokenizing/jn_parts_of_speech.py">
# noinspection PyUnusedClass, PyUnusedName
from __future__ import annotations

from typing import override

from ex_autoslot import AutoSlots
from sysutils import kana_utils


class JNPartsOfSpeech(AutoSlots):
    @staticmethod
    def fetch(unparsed: str) -> JNPartsOfSpeech:
        return _full_parts_of_speech_dictionary[unparsed]

    def __init__(self, level1: str, level2: str = "*", level3: str = "*", level4: str = "*") -> None:
        self.level1: PartOfSpeechDescription = _japanese_to_part_of_speech[level1]
        self.level2: PartOfSpeechDescription = _japanese_to_part_of_speech[level2]
        self.level3: PartOfSpeechDescription = _japanese_to_part_of_speech[level3]
        self.level4: PartOfSpeechDescription = _japanese_to_part_of_speech[level4]

    def is_non_word_character(self) -> bool: return self.level1.japanese in ["記号"]

    @override
    def __repr__(self) -> str:
        return "".join([
            "1:" + kana_utils.pad_to_length(self.level1.japanese, 5),
            "2:" + kana_utils.pad_to_length(self.level2.japanese.replace("*", ""), 6),
            "3:" + kana_utils.pad_to_length(self.level3.japanese.replace("*", ""), 6),
            "4:" + kana_utils.pad_to_length(self.level4.japanese.replace("*", ""), 6)])

class PartOfSpeechDescription(AutoSlots):
    def __init__(self, japanese: str, english: str, explanation: str) -> None:
        self.japanese: str = japanese
        self.english: str = english
        self.explanation: str = explanation

    @override
    def __repr__(self) -> str: return self.english

_level_1 = [
    PartOfSpeechDescription("名詞", "noun", "Names things or ideas"),
    PartOfSpeechDescription("形容詞", "i-adjective", "Describes nouns"),
    PartOfSpeechDescription("連体詞", "pre-noun adjectival / adnominal-adjective", "Modifies nouns directly"),
    PartOfSpeechDescription("接続詞的", "conjunctive", "words or expressions that function in a manner similar to conjunctions"),
    PartOfSpeechDescription("動詞", "verb", "Indicates action"),
    PartOfSpeechDescription("副詞", "adverb", "Modifies verbs/adjectives"),
    PartOfSpeechDescription("助動詞", "bound-auxiliary", "Modifies verb tense/mood"),
    PartOfSpeechDescription("助詞", "particle", "Functional word indicating relation such as marking direct object, subject etc"),
    PartOfSpeechDescription("接続詞", "conjunction", "Connects words/clauses"),
    PartOfSpeechDescription("感動詞", "interjection", "Expresses emotion"),
    PartOfSpeechDescription("接頭詞", "prefix", "Added to beginning of words"),
    PartOfSpeechDescription("フィラー", "filler", "Sound/word filling a pause"),
    PartOfSpeechDescription("記号", "symbol", "Punctuation, special symbols"),
    PartOfSpeechDescription("その他", "others", "Miscellaneous, doesn't fit other categories")
]

_level_2_3_4 = [
    PartOfSpeechDescription("*", "*", "Wildcard or general category"),
    PartOfSpeechDescription("一般", "general", "Generic, non-specific")
]

_level_2_3 = [
    PartOfSpeechDescription("サ変接続", "suru-verb", "Nouns convertible into verbs with 'する'"),
    PartOfSpeechDescription("特殊", "special", "Irregular forms"),
    PartOfSpeechDescription("副詞可能", "adverbial", "Nouns/verbs that can function as adverbs"),
    PartOfSpeechDescription("形容動詞語幹", "na-adjective stem", "Base form of na-adjectives")
]

_level_2 = _level_2_3_4 + _level_2_3 + [
    PartOfSpeechDescription("自立", "independent", "Not dependent on other words"),
    PartOfSpeechDescription("動詞接続", "verb-connective", "indicates a form or a word that is used to connect with or modify a verb"),
    PartOfSpeechDescription("代名詞", "pronoun", "Replaces a noun, e.g., he, she, it"),
    PartOfSpeechDescription("係助詞", "binding", "Connects words/clauses, e.g., は, も"),
    PartOfSpeechDescription("読点", "comma", "Punctuation to separate elements"),
    PartOfSpeechDescription("連体化", "adnominalization", "Turns word into modifier for nouns"),
    PartOfSpeechDescription("副助詞", "adverbial", "Adverbial particle, modifies verbs"),
    PartOfSpeechDescription("副助詞／並立助詞／終助詞", "adverbial/coordinating-conjunction/ending", "Various particle types"),
    PartOfSpeechDescription("形容詞接続", "adjective-connection", "Connects adjectives"),
    PartOfSpeechDescription("副詞化", "adverbialization", "Turns word into adverb"),
    PartOfSpeechDescription("間投", "interjection", "Expresses emotion or marks a pause"),
    PartOfSpeechDescription("非自立", "dependent", "Depends on another word to convey meaning"),
    PartOfSpeechDescription("括弧閉", "closing-bracket", "Closing bracket punctuation"),
    PartOfSpeechDescription("括弧開", "opening-bracket", "Opening bracket punctuation"),
    PartOfSpeechDescription("接尾", "suffix", "Appended to end of words"),
    PartOfSpeechDescription("接続助詞", "conjunctive", "Connects clauses or sentences"),
    PartOfSpeechDescription("助詞類接続", "particle-connection", "Connects particles"),
    PartOfSpeechDescription("数", "numeric", "Numerical expressions"),
    PartOfSpeechDescription("動詞非自立的", "auxiliary-verb", "Auxiliary verb form"),
    PartOfSpeechDescription("数接続", "numeric-connection", "Numeric connectors"),
    PartOfSpeechDescription("句点", "period", "Ending punctuation mark"),
    PartOfSpeechDescription("格助詞", "case-particle", "Indicates grammatical case"),
    PartOfSpeechDescription("アルファベット", "alphabet", "Alphabetical characters"),
    PartOfSpeechDescription("ナイ形容詞語幹", "negative-adjective-stem", "Negative adjective stem form"),
    PartOfSpeechDescription("空白", "space", "Whitespace or blank space"),
    PartOfSpeechDescription("名詞接続", "noun-connection", "Noun connectors"),
    PartOfSpeechDescription("終助詞", "sentence-ending", "Ends the sentence"),
    PartOfSpeechDescription("固有名詞", "proper-noun", "Names of specific entities, like Tokyo"),
    PartOfSpeechDescription("並立助詞", "coordinating-conjunction", "Connects equal grammatical items, e.g., and, or"),
    PartOfSpeechDescription("引用文字列", "quoted-character-string", "quoted-character-string")
]

_level_3 = _level_2_3_4 + _level_2_3 + [
    PartOfSpeechDescription("助数詞", "counter", "Counting words, e.g., つ, 本"),
    PartOfSpeechDescription("連語", "compound", "Compound words, two or more words combined"),
    PartOfSpeechDescription("人名", "person-name", "Names of people"),
    PartOfSpeechDescription("地域", "region", "Names of regions, cities, countries"),
    PartOfSpeechDescription("引用", "quotation", "Quotation or citation"),
    PartOfSpeechDescription("組織", "organization", "Names of organizations, companies"),
    PartOfSpeechDescription("助動詞語幹", "auxiliary-verb-stem", "Base form of auxiliary verbs"),
    PartOfSpeechDescription("縮約", "contraction", "Contracted forms, e.g., can't, don't")
]

_level_4 = _level_2_3_4 + [
    PartOfSpeechDescription("名", "first-name", "Given names"),
    PartOfSpeechDescription("姓", "surname", "Family names or surnames"),
    PartOfSpeechDescription("国", "country", "Names of countries")
]
_all_parts_of_speech = _level_1 + _level_2 + _level_3 + _level_4

_japanese_to_part_of_speech: dict[str, PartOfSpeechDescription] = {pos.japanese: pos for pos in _all_parts_of_speech}
_full_parts_of_speech_dictionary = dict[str, JNPartsOfSpeech]()

def _add_full_part_of_speech(level1: str, level2: str = "*", level3: str = "*", level4: str = "*") -> JNPartsOfSpeech:
    combined = f"{level1},{level2},{level3},{level4}"
    parts_of_speech = JNPartsOfSpeech(level1, level2, level3, level4)
    _full_parts_of_speech_dictionary[combined] = parts_of_speech
    return parts_of_speech

# noinspection PyUnusedClass, PyUnusedName
class POS(AutoSlots):
    filler: JNPartsOfSpeech = _add_full_part_of_speech("フィラー")
    bound_auxiliary: JNPartsOfSpeech = _add_full_part_of_speech("助動詞")  # た, ない, だ
    pre_noun_adjectival: JNPartsOfSpeech = _add_full_part_of_speech("連体詞")  # こんな
    interjection: JNPartsOfSpeech = _add_full_part_of_speech("感動詞")
    conjunction: JNPartsOfSpeech = _add_full_part_of_speech("接続詞")

    # noinspection PyUnusedClass, PyUnusedName
    class Other(AutoSlots):
        interjection: JNPartsOfSpeech = _add_full_part_of_speech("その他", "間投")

    class Adverb(AutoSlots):
        general: JNPartsOfSpeech = _add_full_part_of_speech("副詞", "一般")  # もう, そんなに
        particle_connection: JNPartsOfSpeech = _add_full_part_of_speech("副詞", "助詞類接続")  # こんなに

    # noinspection PyUnusedClass, PyUnusedName
    class Particle(AutoSlots):
        coordinating_conjunction: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "並立助詞")  # たり
        binding: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "係助詞")  # は, も
        adverbial: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "副助詞")  # まで
        adverbial_coordinating_ending: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "副助詞／並立助詞／終助詞")
        adverbialization: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "副詞化")  # に
        conjunctive: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "接続助詞")  # て, と, し
        special: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "特殊")
        sentence_ending: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "終助詞")  # な
        adnominalization: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "連体化")  # の

        class CaseMarking(AutoSlots):
            general: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "格助詞", "一般")  # が, に
            quotation: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "格助詞", "引用")
            compound: JNPartsOfSpeech = _add_full_part_of_speech("助詞", "格助詞", "連語")

    class Verb(AutoSlots):
        suffix: JNPartsOfSpeech = _add_full_part_of_speech("動詞", "接尾")  # れる passive
        independent: JNPartsOfSpeech = _add_full_part_of_speech("動詞", "自立")  # 疲れる, する, 走る
        dependent: JNPartsOfSpeech = _add_full_part_of_speech("動詞", "非自立")  # いる progressive/perfect, いく

    # noinspection PyUnusedClass, PyUnusedName
    class Noun(AutoSlots):
        suru_verb: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "サ変接続")  # 話
        negative_adjective_stem: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "ナイ形容詞語幹")
        general: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "一般")  # 自分
        adverbial: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "副詞可能")  # 今
        auxiliary_verb: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "動詞非自立的")
        na_adjective_stem: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "形容動詞語幹")  # 好き
        numeric: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "数")
        conjunctive: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接続詞的")
        quoted_character_string: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "引用文字列")  # ???

        # noinspection PyUnusedClass, PyUnusedName
        class Pronoun(AutoSlots):
            general: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "代名詞", "一般")  # あいつ
            contracted: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "代名詞", "縮約")

        # noinspection PyUnusedClass, PyUnusedName
        class ProperNoun(AutoSlots):
            general: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "固有名詞", "一般")
            organization: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "固有名詞", "組織")

            # noinspection PyUnusedClass, PyUnusedName
            class Person(AutoSlots):
                general: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "固有名詞", "人名", "一般")
                firstname: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "固有名詞", "人名", "名")
                surname: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "固有名詞", "人名", "姓")

            # noinspection PyUnusedClass, PyUnusedName
            class Location(AutoSlots):
                general: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "固有名詞", "地域", "一般")
                country: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "固有名詞", "地域", "国")

        # noinspection PyUnusedClass, PyUnusedName
        class Suffix(AutoSlots):
            suru_verb_connection: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "サ変接続")
            general: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "一般")
            persons_name: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "人名")
            adverbial: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "副詞可能")
            auxiliary_verb_stem: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "助動詞語幹")
            counter: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "助数詞")
            region: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "地域")
            na_adjective_stem: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "形容動詞語幹")
            special: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "接尾", "特殊")

        # noinspection PyUnusedClass, PyUnusedName
        class Special(AutoSlots):
            auxiliary_verb_stem: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "特殊", "助動詞語幹")

        # noinspection PyUnusedClass, PyUnusedName
        class Dependent(AutoSlots):
            general: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "非自立", "一般")  # こと
            adverbial: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "非自立", "副詞可能")  # なか
            auxiliary_verb_stem: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "非自立", "助動詞語幹")
            na_adjective_stem: JNPartsOfSpeech = _add_full_part_of_speech("名詞", "非自立", "形容動詞語幹")

    # noinspection PyUnusedClass, PyUnusedName
    class Adjective(AutoSlots):
        suffix: JNPartsOfSpeech = _add_full_part_of_speech("形容詞", "接尾")
        independent: JNPartsOfSpeech = _add_full_part_of_speech("形容詞", "自立")
        dependent: JNPartsOfSpeech = _add_full_part_of_speech("形容詞", "非自立")  # よかった

    # noinspection PyUnusedClass, PyUnusedName
    class Prefix(AutoSlots):
        noun: JNPartsOfSpeech = _add_full_part_of_speech("接頭詞", "名詞接続")
        adjective: JNPartsOfSpeech = _add_full_part_of_speech("接頭詞", "形容詞接続")
        number: JNPartsOfSpeech = _add_full_part_of_speech("接頭詞", "数接続")
        verb_connective: JNPartsOfSpeech = _add_full_part_of_speech("接頭詞", "動詞接続")

    # noinspection PyUnusedClass, PyUnusedName
    class Symbol(AutoSlots):
        alphabet: JNPartsOfSpeech = _add_full_part_of_speech("記号", "アルファベット")
        general: JNPartsOfSpeech = _add_full_part_of_speech("記号", "一般")
        period: JNPartsOfSpeech = _add_full_part_of_speech("記号", "句点")
        closing_bracket: JNPartsOfSpeech = _add_full_part_of_speech("記号", "括弧閉")
        opening_bracket: JNPartsOfSpeech = _add_full_part_of_speech("記号", "括弧開")
        space: JNPartsOfSpeech = _add_full_part_of_speech("記号", "空白")
        comma: JNPartsOfSpeech = _add_full_part_of_speech("記号", "読点")
</file>

<file path="language_services/janome_ex/tokenizing/jn_token.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.tokenizing import inflection_forms, inflection_types
from language_services.janome_ex.tokenizing.inflection_forms import InflectionForms
from language_services.janome_ex.tokenizing.inflection_types import InflectionTypes
from language_services.janome_ex.tokenizing.jn_parts_of_speech import POS, JNPartsOfSpeech
from sysutils import kana_utils, typed

if TYPE_CHECKING:
    from janome.tokenizer import Token  # pyright: ignore[reportMissingTypeStubs]
    from language_services.janome_ex.tokenizing.inflection_forms import InflectionForm
    from language_services.janome_ex.tokenizing.inflection_types import InflectionType

# noinspection PyUnusedFunction
class JNToken(AutoSlots):
    def __init__(self,
                 parts_of_speech: JNPartsOfSpeech,
                 base_form: str,
                 surface: str,
                 inflection_type: str | InflectionType = "*",
                 inflected_form: str | InflectionForm = "*",
                 reading: str = "",
                 phonetic: str = "",
                 node_type: str = "",
                 raw_token: Token | None = None) -> None:
        self.base_form: str = typed.str_(base_form)
        self.surface: str = typed.str_(surface)
        self.inflection_type: InflectionType = inflection_types.all_dict[inflection_type] if isinstance(inflection_type, str) else inflection_type
        self.inflected_form: InflectionForm = inflection_forms.all_dict[inflected_form] if isinstance(inflected_form, str) else inflected_form
        self.reading: str = typed.str_(reading)
        self.phonetic: str = typed.str_(phonetic)
        self.node_type: str = typed.str_(node_type)
        self.parts_of_speech: JNPartsOfSpeech = parts_of_speech
        self.raw_token: Token | None = raw_token

    @override
    def __repr__(self) -> str:
        return "".join([
            "JNToken(",
            "" + kana_utils.pad_to_length(f"'{self.base_form}'", 6),
            ", " + kana_utils.pad_to_length(f"'{self.surface}'", 6),
            ", " + kana_utils.pad_to_length(f"'{self.inflection_type}'", 6),
            ", " + kana_utils.pad_to_length(f"'{self.inflected_form}'", 10),
            # ", " + kana_utils.pad_to_length(f"'{self.reading}'", 10),
            # ", " + kana_utils.pad_to_length(f"'{self.phonetic}'", 10),
            # ", " + kana_utils.pad_to_length(f"'{self.node_type}'", 10),
            ", " + str(self.parts_of_speech)])

    @override
    def __eq__(self, other: object) -> bool:
        if isinstance(other, JNToken):
            return (self.base_form == other.base_form and
                    self.surface == other.surface and
                    self.inflection_type == other.inflection_type and
                    self.inflected_form == other.inflected_form and
                    # self.reading == other.reading and
                    # self.phonetic == other.phonetic and
                    # self.node_type == other.node_type and
                    self.parts_of_speech == other.parts_of_speech)
        return False

    def is_verb(self) -> bool:
        return self.parts_of_speech in _verb_parts_of_speech

    _pseudo_verbs_for_inflection_purposes: set[str] = {"ます"}
    def is_inflectable_word(self) -> bool:
        return self.is_verb() or self.is_adjective() or self.base_form in self._pseudo_verbs_for_inflection_purposes

    def is_verb_auxiliary(self) -> bool:
        return self.parts_of_speech in _verb_auxiliary_parts_of_speech

    def is_adjective(self) -> bool:
        return self.parts_of_speech in _adjective_parts_of_speech

    def is_adjective_auxiliary(self) -> bool:
        if self.parts_of_speech in _adjective_auxiliary_parts_of_speech:
            return True

        return self.inflection_type == InflectionTypes.Sahen.suru and self.inflected_form == InflectionForms.Continuative.renyoukei_masu_stem  # "連用形" # irregular conjugations of する like し # "サ変・スル"

    def is_ichidan_verb(self) -> bool:
        return self.inflection_type == InflectionTypes.Ichidan.regular

    def is_noun(self) -> bool:
        return self.parts_of_speech in _noun_parts_of_speech

    def is_past_tense_stem(self) -> bool:
        return self.inflected_form == InflectionForms.Continuative.ta_connection  # "連用タ接続"

    _te_connections: set[InflectionForm] = {InflectionForms.Continuative.te_connection, InflectionForms.Continuative.de_connection}
    def is_te_form_stem(self) -> bool:
        return (self.inflected_form in JNToken._te_connections or
                (self.is_past_tense_stem() and self.surface.endswith("ん")))

    def is_ichidan_masu_stem(self) -> bool:
        return self.inflected_form == InflectionForms.Continuative.renyoukei_masu_stem

    def is_special_nai_negative(self) -> bool:
        return self.inflection_type == InflectionTypes.Special.nai

    def is_past_tense_marker(self) -> bool:
        return self.inflection_type == InflectionTypes.Special.ta  # "連用タ接続"

    def is_t_form_marker(self) -> bool:
        return self.inflection_type == InflectionTypes.Special.ta  # "連用タ接続"

    def is_noun_auxiliary(self) -> bool:
        return self.parts_of_speech in _noun_auxiliary_parts_of_speech

    def is_end_of_phrase_particle(self) -> bool:
        if self.parts_of_speech in _end_of_phrase_particles:
            return True

        return self.parts_of_speech == POS.Particle.conjunctive and self.surface != "て"

_end_of_phrase_particles = {
    POS.Particle.CaseMarking.general,
    POS.Particle.CaseMarking.compound,
    POS.Particle.CaseMarking.quotation,
    POS.Particle.adverbial  # まで : this feels strange, but works so far.
}

_noun_parts_of_speech = {
    POS.Noun.general,  # 自分
    POS.Noun.Pronoun.general,  # あいつ
    POS.Noun.suru_verb,  # 話
    POS.Noun.adverbial,  # 今
    POS.Noun.na_adjective_stem  # 余慶
}

_adjective_auxiliary_parts_of_speech = {
    POS.bound_auxiliary,  # た, ない past, negation
    POS.Particle.conjunctive,  # て,と,し
    # POS.Adverb.general, # もう
}

_adjective_parts_of_speech = {
    POS.Adjective.independent,
    POS.Adjective.dependent
}

_noun_auxiliary_parts_of_speech = {
                                      POS.Noun.general,  # 自分

                                      POS.Particle.CaseMarking.general,  # が
                                      POS.Particle.adnominalization,  # の
                                      POS.Particle.binding,  # は
                                      POS.Noun.Dependent.adverbial,  # なか
                                      POS.Noun.Dependent.general,  # こと
                                      POS.Particle.adverbial,  # まで
                                      POS.Particle.adverbialization  # に
                                  } | _adjective_parts_of_speech | _adjective_auxiliary_parts_of_speech

_verb_parts_of_speech = {
    POS.Verb.independent,
    POS.Verb.dependent,
    POS.Verb.suffix
}

_verb_auxiliary_parts_of_speech = {
                                      POS.bound_auxiliary,  # た, ない past, negation
                                      POS.Particle.binding,  # は, も
                                      POS.Particle.sentence_ending,  # な
                                      POS.Verb.dependent,  # いる progressive/perfect, いく
                                      POS.Verb.suffix,  # れる passive
                                      POS.Particle.conjunctive,  # て,と,し
                                      POS.Particle.coordinating_conjunction,  # たり
                                      POS.Particle.adverbial,  # まで
                                      POS.Adjective.dependent,  # よかった
                                      POS.Adjective.independent,  # ない
                                      POS.Noun.Dependent.general,  # こと
                                      POS.Noun.general
                                  } | _verb_parts_of_speech
</file>

<file path="language_services/janome_ex/tokenizing/jn_tokenized_text.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ex_autoslot import AutoSlots
from language_services import conjugator
from language_services.jamdict_ex.dict_lookup import DictLookup

if TYPE_CHECKING:
    from janome.tokenizer import Token  # pyright: ignore[reportMissingTypeStubs]
    from language_services.janome_ex.tokenizing.jn_token import JNToken
    from note.collection.vocab_collection import VocabCollection
    from queryablecollections.collections.q_list import QList

class ProcessedToken(AutoSlots):
    def __init__(self, surface: str, base: str, is_non_word_character: bool) -> None:
        self.surface: str = surface
        self.base_form: str = base
        self.is_inflectable_word: bool = False
        self.is_non_word_character: bool = is_non_word_character
        self.potential_godan_verb: str | None = None

    def is_past_tense_stem(self) -> bool: return False
    def is_ichidan_masu_stem(self) -> bool: return False
    def is_te_form_stem(self) -> bool: return False
    def is_past_tense_marker(self) -> bool: return False
    def is_special_nai_negative(self) -> bool: return False

    @override
    def __repr__(self) -> str:
        return f"ProcessedToken('{self.surface}', '{self.base_form}', {self.is_inflectable_word})"

class JNTokenWrapper(ProcessedToken, AutoSlots):
    def __init__(self, token: JNToken, vocabs: VocabCollection) -> None:
        super().__init__(token.surface, token.base_form, token.parts_of_speech.is_non_word_character())
        self.token: JNToken = token
        self._vocabs: VocabCollection = vocabs
        self.is_inflectable_word: bool = self.token.is_inflectable_word()

    @override
    def is_past_tense_stem(self) -> bool: return self.token.is_past_tense_stem()
    @override
    def is_te_form_stem(self) -> bool: return self.token.is_te_form_stem()
    @override
    def is_ichidan_masu_stem(self) -> bool: return self.token.is_ichidan_masu_stem()
    @override
    def is_past_tense_marker(self) -> bool: return self.token.is_past_tense_marker()
    @override
    def is_special_nai_negative(self) -> bool: return self.token.is_special_nai_negative()

    # todo: restore the splitting of godan verbs, but this time around:
    #  1. output a first token that has the whole current surface, but with the normal Godan as it's base, along with the metadata that this is a potential godan stem token.
    #  2. output a second token with an EMPTY surface and a base that is える. Via the base the potential compound will be matched later.
    #  3. add a fallback/alternative surface member to ProcessedToken. For potential godans that will be え.
    #  4. When a CandidateWord encounters an empty form at the start of it's token chain, it will replace it with the alternative surface,
    #     thus effectively creating an alternative repretation of that index in the analysis, since the next token starts at the same index.
    #  5. All that would have been matched before will be matched normally from the next token, while the potential token/location will allow any compounds starting with e
    #   and marked as requiring a potential godan stem, to correctly match all the potential conjugations.
    #  6. A configuration option can determine whether the matches from the original surface or the alternative surface should be the ones used in the display,
    #    in the indexing both will of course always be output so that all the sentences containing える will be correctly identified.
    #  7. Note, when calculating shadowing the empty tokens must not be counted, or words that are actually shadowed will be displayed.
    def pre_process(self) -> list[ProcessedToken]:
        self.potential_godan_verb: str | None = self._try_find_vocab_based_potential_verb_compound() or self._try_find_dictionary_based_potential_godan_verb()
        return [self]

    def _try_find_vocab_based_potential_verb_compound(self) -> str | None:
        for vocab in self._vocabs.with_question(self.base_form):
            compound_parts = vocab.compound_parts.all()
            if len(compound_parts) == 2 and compound_parts[1] == "える":
                return compound_parts[0]
        return None

    def _try_find_dictionary_based_potential_godan_verb(self) -> str | None:
        if (len(self.token.base_form) >= 3
                and self.token.base_form[-2:] in conjugator.godan_potential_verb_ending_to_dictionary_form_endings
                and self.token.is_ichidan_verb()
                and not DictLookup.is_word(self.token.base_form)):  # the potential verbs are generally not in the dictionary this is how we know them
            root_verb = conjugator.construct_root_verb_for_possibly_potential_godan_verb_dictionary_form(self.token.base_form)
            if DictLookup.is_word(root_verb):
                lookup = DictLookup.lookup_word(root_verb)
                if lookup.found_words():
                    is_godan = any(e for e in lookup.entries if "godan verb" in e.parts_of_speech())
                    if not is_godan:
                        return None
                return root_verb
        return None

class JNTokenizedText(AutoSlots):
    def __init__(self, text: str, raw_tokens: QList[Token], tokens: QList[JNToken]) -> None:
        self.raw_tokens: list[Token] = raw_tokens
        self.text: str = text
        self.tokens: QList[JNToken] = tokens

    def pre_process(self) -> QList[ProcessedToken]:
        vocab = app.col().vocab

        return self.tokens.select_many(lambda token: JNTokenWrapper(token, vocab).pre_process()).to_list()
        # query(JNTokenWrapper(token, vocab) for token in self.tokens)
        #return ex_sequence.flatten([token.pre_process() for token in step1])
</file>

<file path="language_services/janome_ex/tokenizing/jn_tokenizer.py">
from __future__ import annotations

from typing import final

from ex_autoslot import AutoSlots
from janome.tokenizer import Token, Tokenizer  # pyright: ignore[reportMissingTypeStubs]
from language_services.janome_ex.tokenizing.jn_parts_of_speech import JNPartsOfSpeech
from language_services.janome_ex.tokenizing.jn_token import JNToken
from language_services.janome_ex.tokenizing.jn_tokenized_text import JNTokenizedText
from queryablecollections.collections.q_list import QList
from sysutils import ex_str, typed


@final
class JNTokenizer(AutoSlots):
    def __init__(self) -> None:
        self._tokenizer = Tokenizer()

    def tokenize(self, text: str) -> JNTokenizedText:
        # apparently janome does not fully understand that invisible spaces are word separators, so we replace them with ordinary spaces
        sanitized_text = text.replace(ex_str.invisible_space, " ")
        tokens = [typed.checked_cast(Token, token) for token in self._tokenizer.tokenize(sanitized_text)]

        return JNTokenizedText(text,
                               QList(tokens),
                               QList(JNToken(JNPartsOfSpeech.fetch(typed.str_(token.part_of_speech)),  # pyright: ignore[reportAny]
                                             typed.str_(token.base_form),  # pyright: ignore[reportAny]
                                             typed.str_(token.surface),  # pyright: ignore[reportAny]
                                             typed.str_(token.infl_type),  # pyright: ignore[reportAny]
                                             typed.str_(token.infl_form),  # pyright: ignore[reportAny]
                                             typed.str_(token.reading),  # pyright: ignore[reportAny]
                                             typed.str_(token.phonetic),  # pyright: ignore[reportAny]
                                             typed.str_(token.node_type),  # pyright: ignore[reportAny]
                                             token) for token in tokens))
</file>

<file path="language_services/janome_ex/word_extraction/analysis_constants.py">
from __future__ import annotations

from sysutils import ex_str

noise_characters = {".", ",", ":", ";", "/", "|", "。", "、", "?", "!", "～", "｡", ex_str.invisible_space}
</file>

<file path="language_services/janome_ex/word_extraction/candidate_word_variant.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final, override

from ankiutils import app
from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.dictionary_match import DictionaryMatch
from language_services.janome_ex.word_extraction.matches.missing_match import MissingMatch
from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch
from language_services.janome_ex.word_extraction.word_exclusion import WordExclusion
from queryablecollections.collections.q_list import QList
from sysutils import ex_assert
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from language_services.jamdict_ex.dict_lookup import DictLookup
    from language_services.janome_ex.word_extraction.candidate_word import CandidateWord
    from language_services.janome_ex.word_extraction.matches.match import Match
    from note.sentences.sentence_configuration import SentenceConfiguration

@final
class CandidateWordVariant(WeakRefable, AutoSlots):
    def __init__(self, word: WeakRef[CandidateWord], form: str) -> None:

        self.weak_ref = WeakRef(self)
        from language_services.jamdict_ex.dict_lookup import DictLookup

        self._word: WeakRef[CandidateWord] = word
        self.form: str = form

        self._dict_lookup: Lazy[DictLookup] = Lazy(lambda: DictLookup.lookup_word(form))
        self.vocab_matches: QList[VocabMatch] = QList(VocabMatch(self.weak_ref, vocab) for vocab in app.col().vocab.with_form(form))

        # will be completed in complete_analysis
        self.completed_analysis = False
        self.matches: QList[Match] = QList()

    @property
    def is_surface(self) -> bool: return self.form == self.word.surface_form
    @property
    def vocabs_control_match_status(self) -> bool:
        return (any(self.valid_vocab_matches)
                or any(self.form_owning_vocab_matches)
                or (any(self.vocab_matches) and not self._dict_lookup().found_words() and self.word.is_custom_compound))

    def run_validity_analysis(self) -> None:
        ex_assert.that(not self.completed_analysis)

        if self.vocabs_control_match_status:
            self.matches = QList(self.vocab_matches)
        else:
            if self._dict_lookup().found_words():
                self.matches = QList([DictionaryMatch(self.weak_ref, self._dict_lookup().entries[0])])
            else:
                self.matches = QList([MissingMatch(self.weak_ref)])

        self.completed_analysis = True

    @property
    def start_index(self) -> int: return self.word.start_location.character_start_index
    @property
    def configuration(self) -> SentenceConfiguration: return self.word.analysis.configuration
    @property
    def word(self) -> CandidateWord: return self._word()
    @property
    def is_known_word(self) -> bool: return len(self.vocab_matches) > 0 or self._dict_lookup().found_words()
    @property
    def form_owning_vocab_matches(self) -> list[VocabMatch]: return [vm for vm in self.vocab_matches if vm.vocab.forms.is_owned_form(self.form)]
    @property
    def valid_vocab_matches(self) -> list[VocabMatch]: return [vm for vm in self.vocab_matches if vm.is_valid]
    @property
    def is_valid(self) -> bool: return any(match for match in self._once_analyzed.matches if match.is_valid)
    @property
    def display_matches(self) -> list[Match]: return [match for match in self._once_analyzed.matches if match.is_displayed]

    @property
    def _once_analyzed(self) -> CandidateWordVariant:
        ex_assert.that(self.completed_analysis)
        return self

    def to_exclusion(self) -> WordExclusion:
        return WordExclusion.at_index(self.form, self.start_index)

    @override
    def __repr__(self) -> str:
        return f"""{self.form}, is_valid_candidate:{self.is_valid}"""
</file>

<file path="language_services/janome_ex/word_extraction/candidate_word.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.analysis_constants import noise_characters
from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
from language_services.janome_ex.word_extraction.matches.match import Match
from queryablecollections.q_iterable import QIterable
from sysutils.typed import non_optional
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis
    from language_services.janome_ex.word_extraction.text_location import TextAnalysisLocation
    from queryablecollections.collections.q_list import QList

from sysutils.ex_str import newline


@final
class CandidateWord(WeakRefable, AutoSlots):
    def __init__(self, locations: list[WeakRef[TextAnalysisLocation]]) -> None:
        self.weakref = WeakRef(self)
        self.locations: list[WeakRef[TextAnalysisLocation]] = locations

        self.surface_form = "".join([t().token.surface for t in self.locations])
        self.base_form = "".join([location().token.surface for location in self.locations[:-1]]) + self.locations[-1]().token.base_form
        self.surface_variant: CandidateWordVariant = CandidateWordVariant(self.weakref, self.surface_form)
        self.base_variant: CandidateWordVariant | None = CandidateWordVariant(self.weakref, self.base_form) if self.base_form != self.surface_form else None

        self.indexing_variants: list[CandidateWordVariant] = []
        self.valid_variants: list[CandidateWordVariant] = []
        self.display_variants: list[CandidateWordVariant] = []

    @property
    def should_include_surface_in_valid_words(self) -> bool: return (self.surface_variant.is_valid
                                                                     and (not self.is_inflected_word or not self.has_valid_base_variant))
    @property
    def has_valid_base_variant(self) -> bool: return self.base_variant is not None and self.base_variant.is_valid
    @property
    def should_include_surface_in_all_words(self) -> bool: return (self.should_include_surface_in_valid_words
                                                                   or (not self.has_valid_base_variant and self.has_seemingly_valid_single_token))
    @property
    def has_valid_base_with_display_matches(self) -> bool: return self.has_valid_base_variant and any(non_optional(self.base_variant).display_matches)
    @property
    def should_include_surface_in_display_variants(self) -> bool: return self.should_include_surface_in_all_words and any(self.surface_variant.display_matches)
    @property
    def should_index_base(self) -> bool: return self.base_variant is not None and (self.base_variant.is_known_word or self.has_valid_base_variant)
    @property
    def should_index_surface(self) -> bool: return self.surface_variant.is_known_word or self.should_include_surface_in_all_words


    @property
    def all_matches(self) -> QList[Match]:
        return self.surface_variant.matches.concat(self.base_variant.matches if self.base_variant else QIterable[Match].empty()).to_list()

    def run_validity_analysis(self) -> None:
        self.surface_variant.run_validity_analysis()
        if self.base_variant is not None: self.base_variant.run_validity_analysis()

        self.valid_variants = []
        if self.has_valid_base_variant:
            self.valid_variants.append(non_optional(self.base_variant))
        if self.should_include_surface_in_valid_words:
            self.valid_variants.append(self.surface_variant)

        if self.should_index_surface:
            self.indexing_variants.append(self.surface_variant)

        if self.should_index_base:
            self.indexing_variants.append(non_optional(self.base_variant))

    def run_display_analysis_pass_true_if_there_were_changes(self) -> bool:
        old_display_word_variants = self.display_variants
        self.display_variants = []

        if self.should_include_surface_in_display_variants:
            self.display_variants.append(self.surface_variant)
        elif self.has_valid_base_with_display_matches:
            self.display_variants.append(non_optional(self.base_variant))

        def displaywords_were_changed() -> bool:
            if len(old_display_word_variants) != len(self.display_variants):
                return True

            return any(old_display_word_variants[index] != self.display_variants[index] for index in range(len(old_display_word_variants)))

        return displaywords_were_changed()

    def has_valid_words(self) -> bool: return len(self.valid_variants) > 0

    @property
    def has_seemingly_valid_single_token(self) -> bool: return not self.is_custom_compound and not self.starts_with_non_word_character
    @property
    def analysis(self) -> TextAnalysis: return self.locations[0]().analysis()
    @property
    def is_custom_compound(self) -> bool: return self.location_count > 1
    @property
    def start_location(self) -> TextAnalysisLocation: return self.locations[0]()
    @property
    def end_location(self) -> TextAnalysisLocation: return self.locations[-1]()
    @property
    def location_count(self) -> int: return len(self.locations)
    @property
    def starts_with_non_word_token(self) -> bool: return self.start_location.token.is_non_word_character

    @property
    def is_word(self) -> bool: return self.surface_variant.is_known_word or (self.base_variant is not None and self.base_variant.is_known_word)
    @property
    def is_inflectable_word(self) -> bool: return self.end_location.token.is_inflectable_word
    @property
    def next_token_is_inflecting_word(self) -> bool: return self.end_location.is_next_location_inflecting_word()
    @property
    def is_inflected_word(self) -> bool: return self.is_inflectable_word and self.next_token_is_inflecting_word
    @property
    def starts_with_non_word_character(self) -> bool: return self.starts_with_non_word_token or self.surface_form in noise_characters

    @property
    def is_shadowed(self) -> bool: return self.is_shadowed_by is not None
    @property
    def shadowed_by_text(self) -> str: return non_optional(self.is_shadowed_by).form if self.is_shadowed else ""
    @property
    def is_shadowed_by(self) -> CandidateWordVariant | None:
        if any(self.start_location.is_shadowed_by):
            return self.start_location.is_shadowed_by[0]().display_variants[0]
        if (any(self.start_location.display_variants)
                and self.start_location.display_variants[0].word.location_count > self.location_count):
            return self.start_location.display_variants[0]
        return None

    @override
    def __repr__(self) -> str: return f"""
surface: {self.surface_variant.__repr__()} | base:{self.base_variant.__repr__()},
hdc:{self.has_valid_words()},
iw:{self.is_word}
icc:{self.is_custom_compound})""".replace(newline, "")
</file>

<file path="language_services/janome_ex/word_extraction/matches/dictionary_match.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.match import Match
from sysutils import typed

if TYPE_CHECKING:
    from language_services.jamdict_ex.dict_entry import DictEntry
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from sysutils.weak_ref import WeakRef

class DictionaryMatch(Match, AutoSlots):
    def __init__(self, word_variant: WeakRef[CandidateWordVariant], dictionary_entry: DictEntry) -> None:
        super().__init__(word_variant,
                         validity_requirements=[],
                         display_requirements=[])
        self.dictionary_entry: DictEntry = dictionary_entry

    @property
    @override
    def answer(self) -> str: return self.dictionary_entry.generate_answer()
    @property
    @override
    def readings(self) -> list[str]: return [typed.str_(f.text) for f in self.dictionary_entry.entry.kana_forms]  # pyright: ignore[reportUnknownArgumentType, reportUnknownMemberType]
</file>

<file path="language_services/janome_ex/word_extraction/matches/match.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.requirements.forbids_state import Forbids
from language_services.janome_ex.word_extraction.matches.state_tests.is_configured_hidden import IsConfiguredHidden
from language_services.janome_ex.word_extraction.matches.state_tests.is_configured_incorrect import IsConfiguredIncorrect
from language_services.janome_ex.word_extraction.matches.state_tests.is_shadowed import IsShadowed
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.candidate_word import CandidateWord
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from language_services.janome_ex.word_extraction.matches.requirements.requirement import MatchRequirement

class Match(WeakRefable, AutoSlots):
    def __init__(self, word_variant: WeakRef[CandidateWordVariant],
                 validity_requirements: list[MatchRequirement],
                 display_requirements: list[MatchRequirement]) -> None:
        weakref = WeakRef(self)
        self.weakref: WeakRef[Match] = weakref
        self._variant: WeakRef[CandidateWordVariant] = word_variant
        self._validity_requirements: list[MatchRequirement] = ([
                                                                       Forbids(IsConfiguredIncorrect(self.weakref))
                                                               ]
                                                               + validity_requirements)
        self._display_requirements: list[MatchRequirement] = ([
                                                                      Forbids(IsShadowed(self.weakref)),
                                                                      Forbids(IsConfiguredHidden(self.weakref))
                                                              ]
                                                              + display_requirements)

        self.can_cache_validity: bool = all(requirement.state_test.is_cachable for requirement in self._validity_requirements)
        self._cached_is_valid_internal: Lazy[bool] | None = Lazy(lambda: weakref().__is_valid_internal_implementation()) if self.can_cache_validity else None

    @property
    def answer(self) -> str: raise NotImplementedError()
    @property
    def readings(self) -> list[str]: raise NotImplementedError()

    @property
    def tokenized_form(self) -> str: return self.variant.form
    @property
    def match_form(self) -> str: return self.tokenized_form
    @property
    def parsed_form(self) -> str: return self.tokenized_form

    @property
    def word(self) -> CandidateWord: return self.variant.word
    @property
    def variant(self) -> CandidateWordVariant: return self._variant()

    @property
    def is_valid(self) -> bool: return (self._is_valid_internal
                                        or (self.is_highlighted
                                            and not any(valid_sibling for valid_sibling in self._sibling_matches if valid_sibling._is_valid_internal)))
    @property
    def _is_valid_internal(self) -> bool:
        return self._cached_is_valid_internal() \
            if self._cached_is_valid_internal is not None \
            else self.__is_valid_internal_implementation()

    def __is_valid_internal_implementation(self) -> bool: return all(requirement.is_fulfilled for requirement in self._validity_requirements)
    @property
    def is_highlighted(self) -> bool: return self.match_form in self.variant.configuration.highlighted_words
    @property
    def is_displayed(self) -> bool: return self.is_valid_for_display or self.is_emergency_displayed
    @property
    def start_index(self) -> int: return self.variant.start_index
    @property
    def is_valid_for_display(self) -> bool: return self.is_valid and all(requirement.is_fulfilled for requirement in self._display_requirements)

    @property
    def is_emergency_displayed(self) -> bool:
        return (self._surface_is_seemingly_valid_single_token
                and self.variant.is_surface
                and not self.is_shadowed
                and not self._base_is_valid_word
                and not self._has_valid_for_display_sibling)

    @property
    def _has_valid_for_display_sibling(self) -> bool: return any(other_match for other_match in self._sibling_matches if other_match.is_valid_for_display)
    @property
    def _sibling_matches(self) -> list[Match]: return [match for match in self.variant.matches if match != self]
    @property
    def _base_is_valid_word(self) -> bool: return self.word.base_variant is not None and self.word.base_variant.is_valid

    @property
    def _surface_is_seemingly_valid_single_token(self) -> bool: return self.word.has_seemingly_valid_single_token

    @property
    def is_shadowed(self) -> bool: return self.word.is_shadowed
    @property
    def failure_reasons(self) -> list[str]: return [requirement.failure_reason for requirement in self._validity_requirements if not requirement.is_fulfilled]
    @property
    def hiding_reasons(self) -> list[str]: return [requirement.failure_reason for requirement in self._display_requirements if not requirement.is_fulfilled]

    @override
    def __repr__(self) -> str: return f"""{self.parsed_form}, {self.match_form[:10]}: failure_reasons: {" ".join(self.failure_reasons) or "None"} ## hiding_reasons: {" ".join(self.hiding_reasons) or "None"}"""

    @override
    def __str__(self) -> str: return self.__repr__()
</file>

<file path="language_services/janome_ex/word_extraction/matches/missing_match.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.match import Match

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from sysutils.weak_ref import WeakRef

class MissingMatch(Match, AutoSlots):
    def __init__(self, word_variant: WeakRef[CandidateWordVariant]) -> None:
        super().__init__(word_variant,
                         validity_requirements=[],
                         display_requirements=[])

    @property
    @override
    def answer(self) -> str: return "---"
    @property
    @override
    def match_form(self) -> str: return "[MISSING]"  # Change this so the tests can distinguish that this is a missing match
    @property
    @override
    def is_valid(self) -> bool: return False
    @property
    @override
    def readings(self) -> list[str]: return []
    @property
    @override
    def failure_reasons(self) -> list[str]:
        return super().failure_reasons + ["no_dictionary_or_vocabulary_match_found"]
</file>

<file path="language_services/janome_ex/word_extraction/matches/requirements/forbids_state.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.requirements.requirement import MatchRequirement

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

class Forbids(MatchRequirement, AutoSlots):
    def __init__(self, state_test: MatchStateTest, is_requirement_active: bool = True) -> None:
        super().__init__(state_test)
        self.is_requirement_active: bool = is_requirement_active

    @property
    @override
    def is_fulfilled(self) -> bool:
        if not self.is_requirement_active:
            return True

        if not self.state_test.match_is_in_state:  # noqa: SIM103
            return True

        return False

    @property
    @override
    def failure_reason(self) -> str: return f"""forbids::{self.state_test.state_description}""" if not self.is_fulfilled else ""
</file>

<file path="language_services/janome_ex/word_extraction/matches/requirements/requirement.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest


class MatchRequirement(AutoSlots):
    def __init__(self, state_test: MatchStateTest) -> None:
        self.state_test: MatchStateTest = state_test

    @property
    def is_fulfilled(self) -> bool: raise NotImplementedError()

    @property
    def failure_reason(self) -> str: raise NotImplementedError()

    @override
    def __repr__(self) -> str: return self.failure_reason
</file>

<file path="language_services/janome_ex/word_extraction/matches/requirements/requires_forbids_requirement.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.requirements.requirement import MatchRequirement

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest
    from note.notefields.require_forbid_flag_field import RequireForbidFlagField


class RequiresOrForbids(MatchRequirement, AutoSlots):
    def __init__(self, state_test: MatchStateTest, requires_forbids: RequireForbidFlagField) -> None:
        super().__init__(state_test)
        self.is_required: bool = requires_forbids.is_required
        self.is_forbidden: bool = requires_forbids.is_forbidden

    @property
    @override
    def is_fulfilled(self) -> bool:
        if self.is_required and not self.state_test.match_is_in_state:
            return False

        if self.is_forbidden and self.state_test.match_is_in_state:  # noqa: SIM103 these returns are useful for breakpoints when debugging
            return False

        return True

    @property
    @override
    def failure_reason(self) -> str:
        if self.is_fulfilled:
            return ""

        if self.is_required:
            return f"required::{self.state_test.description}"

        if self.is_forbidden:
            return f"forbids::{self.state_test.description}"

        raise AssertionError("This should never happen")
</file>

<file path="language_services/janome_ex/word_extraction/matches/requirements/requires_state.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.requirements.requirement import MatchRequirement

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

class Requires(MatchRequirement, AutoSlots):
    def __init__(self, state_test: MatchStateTest, is_requirement_active: bool = True) -> None:
        super().__init__(state_test)
        self.rule_active: bool = is_requirement_active

    @property
    @override
    def is_fulfilled(self) -> bool: return (not self.rule_active
                                            or self.state_test.match_is_in_state)

    @property
    @override
    def failure_reason(self) -> str: return f"requires::{self.state_test.state_description}" if not self.is_fulfilled else ""
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/another_match_owns_the_form.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.vocab_match_state_test import VocabMatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch
    from sysutils.weak_ref import WeakRef

    pass

class AnotherMatchOwnsTheForm(VocabMatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[VocabMatch]) -> None:
        super().__init__(match, "another_match_owns_the_form", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if self.vocab.forms.is_owned_form(self.tokenized_form):
            return False

        if any(other_match for other_match in self.variant.vocab_matches  # noqa: SIM103
               if other_match != self.match
                  and other_match.vocab.forms.is_owned_form(self.tokenized_form)
                  and other_match.is_valid):
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/head/has_a_stem.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services import conjugator
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class HasAStem(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "a_stem", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if len(self.prefix) > 0 and self.prefix[-1] in conjugator.a_stem_characters:  # noqa: SIM103
            return True

        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/head/has_e_stem.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services import conjugator
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest
from sysutils import kana_utils

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class HasEStem(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "e_stem", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if not self.prefix:
            return False

        if self.prefix[-1] in conjugator.e_stem_characters:
            return True

        if kana_utils.character_is_kanji(self.prefix[-1]):  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/head/has_past_tense_stem.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class HasPastTenseStem(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "past_tense_stem", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if self.previous_location is None:
            return False

        if self.previous_location.token.is_past_tense_stem():
            return True

        if self.word.start_location.token.is_past_tense_marker():  # noqa: SIM103
            return True

        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/head/has_te_form_stem.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class HasTeFormStem(MatchStateTest, AutoSlots):
    _te_forms: set[str] = {"て", "って", "で"}
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "te_form_stem", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        #todo get this stuff moved into the tokenizing stage...
        if self.previous_location is None:
            return False

        if self.previous_location.token.is_special_nai_negative():  # todo: review: this code means that we do not consider ない to be a te form stem, but it seems that janome does.....
            return False

        if self.previous_location.token.is_te_form_stem():
            return True

        if not any(te_form_start for te_form_start in HasTeFormStem._te_forms if self.parsed_form.startswith(te_form_start)):
            return False

        if self.previous_location.token.is_past_tense_stem():
            return True

        if self.previous_location.token.is_ichidan_masu_stem():  # noqa: SIM103
            return True

        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/head/is_sentence_start.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class IsSentenceStart(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "sentence_start", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if len(self.prefix) == 0 or self.prefix[-1].isspace():  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/head/prefix_is_in.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class PrefixIsIn(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match], prefixes: set[str]) -> None:
        super().__init__(match, f"""prefix_in:{",".join(prefixes)}""", cache_is_in_state=True)
        self.prefixes: set[str] = prefixes

    @override
    def _internal_match_is_in_state(self) -> bool:
        if any(prefix for prefix in self.prefixes if self.prefix.endswith(prefix)):  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/is_configured_hidden.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class IsConfiguredHidden(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "configured_hidden", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        #todo: think a bit about this. Now we use the variant start index, which may differ from the match start index. Which should be used?
        if self.variant.configuration.hidden_matches.excludes_at_index(self.tokenized_form, self.variant.start_index):  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/is_configured_incorrect.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class IsConfiguredIncorrect(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "configured_incorrect", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        return self.configuration.incorrect_matches.excludes_at_index(self.tokenized_form,
                                                                      self.match.start_index)
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/is_exact_match.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.vocab_match_state_test import VocabMatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch
    from sysutils.weak_ref import WeakRef

    pass

class IsExactMatch(VocabMatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[VocabMatch]) -> None:
        super().__init__(match, "exact_match", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if not self.variant.is_surface:
            return False

        if self.variant.form in self.vocab.forms.all_set():  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/is_poison_word.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.vocab_match_state_test import VocabMatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch
    from sysutils.weak_ref import WeakRef
    pass

class IsPoisonWord(VocabMatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[VocabMatch]) -> None:
        super().__init__(match, "poison_word", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if self.rules.bool_flags.is_poison_word.is_set():  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/is_shadowed.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class IsShadowed(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "shadowed", cache_is_in_state=False)

    @override
    def _internal_match_is_in_state(self) -> bool: return self.match.is_shadowed

    @property
    @override
    def state_description(self) -> str:
        return f"shadowed_by:{self.match.word.shadowed_by_text}" \
            if self.match_is_in_state \
            else "forbids::shadowed"
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/is_single_token.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class IsSingleToken(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "single_token", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if not self.word.is_custom_compound:  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/match_state_test.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final, override

from ex_autoslot import AutoSlots
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.candidate_word import CandidateWord
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from language_services.janome_ex.word_extraction.matches.match import Match
    from language_services.janome_ex.word_extraction.text_location import TextAnalysisLocation
    from note.sentences.sentence_configuration import SentenceConfiguration

class MatchStateTest(WeakRefable, AutoSlots):
    def __init__(self, match: WeakRef[Match], name: str, cache_is_in_state: bool) -> None:
        self._match: WeakRef[Match] = match
        self.description: str = name
        self.is_cachable: bool = cache_is_in_state
        weakrefthis = WeakRef(self)
        self.weakref: WeakRef[MatchStateTest] = weakrefthis
        self._future_match_is_in_state: Lazy[bool] | None = Lazy(lambda: weakrefthis()._internal_match_is_in_state()) if cache_is_in_state else None

    @property
    @final
    def match_is_in_state(self) -> bool: return self._future_match_is_in_state() if self._future_match_is_in_state else self._internal_match_is_in_state()

    @property
    def match(self) -> Match: return self._match()
    @property
    def state_description(self) -> str: return self.description
    @property
    def tokenized_form(self) -> str: return self.match.tokenized_form
    @property
    def parsed_form(self) -> str: return self.match.parsed_form
    @property
    def variant(self) -> CandidateWordVariant: return self.match.variant
    @property
    def word(self) -> CandidateWord: return self.variant.word
    @property
    def end_location(self) -> TextAnalysisLocation: return self.word.end_location
    @property
    def configuration(self) -> SentenceConfiguration: return self.variant.configuration
    @property
    def previous_location(self) -> TextAnalysisLocation | None: return self.word.start_location.previous() if self.word.start_location.previous else None
    @property
    def prefix(self) -> str: return self.previous_location.token.surface if self.previous_location else ""
    @property
    def next_location(self) -> TextAnalysisLocation | None: return self.word.end_location.next() if self.word.end_location.next else None
    @property
    def suffix(self) -> str: return self.next_location.token.surface if self.next_location else ""

    def _internal_match_is_in_state(self) -> bool: raise NotImplementedError()

    @override
    def __repr__(self) -> str: return self.state_description
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/surface_is_in.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class SurfaceIsIn(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match], surfaces: set[str]) -> None:
        super().__init__(match, f"""surface_in:{",".join(surfaces)}""", cache_is_in_state=True)
        self.surfaces: set[str] = surfaces

    @override
    def _internal_match_is_in_state(self) -> bool:
        if self.word.surface_variant.form in self.surfaces:  # noqa: SIM103
            return True
        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/tail/has_overlapping_following_compound.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

    pass

class HasDisplayedOverlappingFollowingCompound(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "has_displayed_following_overlapping_compound", cache_is_in_state=False)

    @override
    def _internal_match_is_in_state(self) -> bool:
        # todo: this is a problematic reference to display_words_starting_here. Thot collection is initialized using this class,
        # so this class will return different results depending on whether it is used after or before display_words_starting_here is first initialized. Ouch
        if not any(self.end_location.display_words):
            return False

        if self.end_location.display_words[0].is_custom_compound:  # noqa: SIM103
            return True

        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/tail/is_sentence_end.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class IsSentenceEnd(MatchStateTest, AutoSlots):
    _quote_characters:set[str] = {"と", "って"}
    def __init__(self, match: WeakRef[Match]) -> None:
        super().__init__(match, "sentence_end", cache_is_in_state=True)

    @override
    def _internal_match_is_in_state(self) -> bool:
        if len(self.suffix) == 0:
            return True

        if self.suffix[0].isspace():
            return True

        if self.suffix in self._quote_characters:  # noqa: SIM103
            return True

        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/tail/suffix_is_in.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from sysutils.weak_ref import WeakRef

class SuffixIsIn(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[Match], suffixes: set[str]) -> None:
        super().__init__(match, f"""suffix_in:{",".join(suffixes)}""", cache_is_in_state=True)
        self.suffixes: set[str] = suffixes

    @override
    def _internal_match_is_in_state(self) -> bool:
        if any(suffix for suffix in self.suffixes if self.suffix.startswith(suffix)):  # noqa: SIM103
            return True

        return False
</file>

<file path="language_services/janome_ex/word_extraction/matches/state_tests/vocab_match_state_test.py">
from __future__ import annotations

from typing import TYPE_CHECKING, cast, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.state_tests.match_state_test import MatchStateTest

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch
    from note.vocabulary.vocabnote import VocabNote
    from note.vocabulary.vocabnote_matching_rules import VocabNoteMatchingConfiguration
    from sysutils.weak_ref import WeakRef


class VocabMatchStateTest(MatchStateTest, AutoSlots):
    def __init__(self, match: WeakRef[VocabMatch], name: str, cache_is_in_state: bool) -> None:
        super().__init__(match, name, cache_is_in_state)
        self.name: str = name

    @property
    @override
    def match(self) -> VocabMatch:
        from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch  # pyright: ignore[reportUnusedImport]
        return cast(VocabMatch, super().match)

    @property
    def rules(self) -> VocabNoteMatchingConfiguration: return self.match.vocab.matching_configuration

    @property
    def vocab(self) -> VocabNote: return self.match.vocab
</file>

<file path="language_services/janome_ex/word_extraction/matches/vocab_match.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.match import Match
from language_services.janome_ex.word_extraction.matches.requirements.forbids_state import Forbids
from language_services.janome_ex.word_extraction.matches.requirements.requires_forbids_requirement import RequiresOrForbids
from language_services.janome_ex.word_extraction.matches.requirements.requires_state import Requires
from language_services.janome_ex.word_extraction.matches.state_tests.another_match_owns_the_form import AnotherMatchOwnsTheForm
from language_services.janome_ex.word_extraction.matches.state_tests.head.has_a_stem import HasAStem
from language_services.janome_ex.word_extraction.matches.state_tests.head.has_e_stem import HasEStem
from language_services.janome_ex.word_extraction.matches.state_tests.head.has_past_tense_stem import HasPastTenseStem
from language_services.janome_ex.word_extraction.matches.state_tests.head.has_te_form_stem import HasTeFormStem
from language_services.janome_ex.word_extraction.matches.state_tests.head.is_sentence_start import IsSentenceStart
from language_services.janome_ex.word_extraction.matches.state_tests.head.prefix_is_in import PrefixIsIn
from language_services.janome_ex.word_extraction.matches.state_tests.is_exact_match import IsExactMatch
from language_services.janome_ex.word_extraction.matches.state_tests.is_poison_word import IsPoisonWord
from language_services.janome_ex.word_extraction.matches.state_tests.is_single_token import IsSingleToken
from language_services.janome_ex.word_extraction.matches.state_tests.surface_is_in import SurfaceIsIn
from language_services.janome_ex.word_extraction.matches.state_tests.tail.has_overlapping_following_compound import HasDisplayedOverlappingFollowingCompound
from language_services.janome_ex.word_extraction.matches.state_tests.tail.is_sentence_end import IsSentenceEnd
from language_services.janome_ex.word_extraction.matches.state_tests.tail.suffix_is_in import SuffixIsIn
from sysutils.weak_ref import WeakRef

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from note.vocabulary.vocabnote import VocabNote
    from note.vocabulary.vocabnote_matching_rules import VocabNoteMatchingConfiguration

@final
class VocabMatch(Match, AutoSlots):
    def __init__(self, word_variant: WeakRef[CandidateWordVariant], vocab: VocabNote) -> None:
        weakref: WeakRef[VocabMatch] = WeakRef(self)
        self.requires_forbids = vocab.matching_configuration.requires_forbids
        self.rules = vocab.matching_configuration.configurable_rules
        super().__init__(word_variant,
                         validity_requirements=[
                             Forbids(AnotherMatchOwnsTheForm(weakref)),
                             # head requirements
                             Forbids(PrefixIsIn(weakref, self.rules.prefix_is_not.get()),
                                     is_requirement_active=self.rules.prefix_is_not.any()),
                             Requires(PrefixIsIn(weakref, self.rules.required_prefix.get()),
                                      is_requirement_active=self.rules.required_prefix.any()),
                             RequiresOrForbids(IsSentenceStart(weakref), self.requires_forbids.sentence_start),
                             RequiresOrForbids(HasTeFormStem(weakref), self.requires_forbids.te_form_stem),
                             RequiresOrForbids(HasAStem(weakref), self.requires_forbids.a_stem),
                             RequiresOrForbids(HasPastTenseStem(weakref), self.requires_forbids.past_tense_stem),
                             RequiresOrForbids(HasEStem(weakref), self.requires_forbids.e_stem),

                             # tail requirements
                             RequiresOrForbids(IsSentenceEnd(weakref), self.requires_forbids.sentence_end),
                             Forbids(SuffixIsIn(weakref, self.rules.suffix_is_not.get()),
                                     is_requirement_active=self.rules.suffix_is_not.any()),

                             # misc requirements
                             Forbids(IsPoisonWord(weakref)),
                             RequiresOrForbids(IsExactMatch(weakref), self.requires_forbids.exact_match),
                             RequiresOrForbids(IsSingleToken(weakref), self.requires_forbids.single_token),
                             Forbids(SurfaceIsIn(weakref, self.rules.surface_is_not.get()),
                                     is_requirement_active=self.rules.surface_is_not.any()),
                             Forbids(SurfaceIsIn(weakref, self.rules.yield_to_surface.get()),
                                     is_requirement_active=self.rules.yield_to_surface.any()),
                         ],
                         display_requirements=[
                             Forbids(HasDisplayedOverlappingFollowingCompound(weakref),
                                     is_requirement_active=self.requires_forbids.yield_last_token.is_required)
                         ])
        self.vocab: VocabNote = vocab
        self.word_variant: WeakRef[CandidateWordVariant] = word_variant

    @property
    def matching_configuration(self) -> VocabNoteMatchingConfiguration: return self.vocab.matching_configuration
    @property
    @override
    def match_form(self) -> str: return self.vocab.get_question()
    @property
    @override
    def answer(self) -> str: return self.vocab.get_answer()
    @property
    @override
    def readings(self) -> list[str]: return self.vocab.readings.get()

    @property
    @override
    def parsed_form(self) -> str:
        return self.vocab.question.raw \
            if self.matching_configuration.bool_flags.question_overrides_form.is_set() \
            else super().parsed_form

    @property
    @override
    def start_index(self) -> int:
        if self.matching_configuration.bool_flags.question_overrides_form.is_set():
            if self.requires_forbids.a_stem.is_required or self.requires_forbids.e_stem.is_required:
                return super().start_index - 1
            if self.rules.required_prefix.any():
                matched_prefixes = [prefix for prefix in self.rules.required_prefix.get()
                                    if self.parsed_form.startswith(prefix)]
                if matched_prefixes:
                    matched_prefix_length = max(len(prefix) for prefix in matched_prefixes)
                    return super().start_index - matched_prefix_length

        return super().start_index

    @override
    def __repr__(self) -> str: return f"""{self.vocab.get_question()}, {self.vocab.get_answer()[:10]}: {self.match_form[:10]}: failure_reasons: {" ".join(self.failure_reasons) or "None"} ## hiding_reasons: {" ".join(self.hiding_reasons) or "None"}"""
</file>

<file path="language_services/janome_ex/word_extraction/text_analysis.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final, override

from ex_autoslot import AutoSlots
from queryablecollections.collections.q_list import QList
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from language_services.janome_ex.tokenizing.jn_tokenized_text import ProcessedToken
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from language_services.janome_ex.word_extraction.matches.match import Match

from language_services.janome_ex.tokenizing.jn_tokenizer import JNTokenizer
from language_services.janome_ex.word_extraction.text_location import TextAnalysisLocation
from note.sentences.sentence_configuration import SentenceConfiguration

_tokenizer = JNTokenizer()

@final
class TextAnalysis(WeakRefable, AutoSlots):
    version = "text_analysis_0.1"

    def __init__(self, sentence: str, sentence_configuration: SentenceConfiguration) -> None:
        self.weakref = WeakRef(self)
        self.text = sentence
        self.configuration = sentence_configuration
        self.tokenized_text = _tokenizer.tokenize(sentence)
        self.pre_processed_tokens: QList[ProcessedToken] = self.tokenized_text.pre_process()

        self.locations: QList[TextAnalysisLocation] = QList()

        character_index = 0
        for token_index, token in enumerate(self.pre_processed_tokens):
            self.locations.append(TextAnalysisLocation(self.weakref, token, character_index, token_index))
            character_index += len(token.surface)

        self.start_location = self.locations[0]
        self._connect_next_and_previous_to_locations()
        self._analysis_step_1_analyze_non_compound()
        self._analysis_step_2_analyze_compounds()
        self._analysis_step_3_run_initial_display_analysis()
        self._analysis_step_5_calculate_preference_between_overlapping_display_candidates()

        self.all_matches: QList[Match] = (self.locations
                                          .select_many(lambda location: location.candidate_words)
                                          .select_many(lambda candidate: candidate.all_matches)
                                          .to_list())

        self.indexing_word_variants: QList[CandidateWordVariant] = self.locations.select_many(lambda location: location.indexing_variants).to_list()
        self.valid_word_variants: QList[CandidateWordVariant] = self.locations.select_many(lambda location: location.valid_variants).to_list()
        self.display_word_variants: QList[CandidateWordVariant] = self.locations.select_many(lambda location: location.display_variants).to_list()

        self.indexing_matches: QList[Match] = self.indexing_word_variants.select_many(lambda variant: variant.matches).to_list()
        self.valid_word_variant_matches: QList[Match] = self.valid_word_variants.select_many(lambda variant: variant.matches).to_list()
        # todo: bug valid_matches here and valid_word_variant_valid_matches should be identical. Once they are, the valid_word_variant_valid_matches collection should be removed
        self.valid_matches: QList[Match] = self.indexing_matches.where(lambda match: match.is_valid).to_list()
        self.valid_word_variant_valid_matches: QList[Match] = self.valid_word_variant_matches.where(lambda match: match.is_valid).to_list()
        self.display_matches: QList[Match] = self.indexing_matches.where(lambda match: match.is_displayed).to_list()

    @classmethod
    def from_text(cls, text: str) -> TextAnalysis:
        return cls(text, SentenceConfiguration.empty())

    def all_words_strings(self) -> list[str]:
        return [w.form for w in self.valid_word_variants]
    @override
    def __repr__(self) -> str:
        return self.text

    def _connect_next_and_previous_to_locations(self) -> None:
        for token_index, location in enumerate(self.locations):
            if len(self.locations) > token_index + 1:
                location.next = self.locations[token_index + 1].weakref

            if token_index > 0:
                location.previous = self.locations[token_index - 1].weakref

    def _analysis_step_1_analyze_non_compound(self) -> None:
        for location in self.locations:
            location.analysis_step_1_analyze_non_compound_validity()

    def _analysis_step_2_analyze_compounds(self) -> None:
        for location in self.locations:
            location.analysis_step_2_analyze_compound_validity()

    def _analysis_step_3_run_initial_display_analysis(self) -> None:
        for location in self.locations:
            location.analysis_step_3_run_initial_display_analysis()

    def _analysis_step_5_calculate_preference_between_overlapping_display_candidates(self) -> None:
        changes_made = True
        while changes_made:
            changes_made = False
            for location in self.locations:
                if location.analysis_step_5_resolve_chains_of_compounds_yielding_to_the_next_compound_pass_true_if_there_were_changes():
                    changes_made = True
</file>

<file path="language_services/janome_ex/word_extraction/text_location.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final, override

from ankiutils import app
from ex_autoslot import AutoSlots
from queryablecollections.collections.q_list import QList
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from language_services.janome_ex.tokenizing.jn_tokenized_text import ProcessedToken
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis

from language_services.janome_ex.word_extraction.candidate_word import CandidateWord
from sysutils.ex_str import newline

_max_lookahead = 12 # In my collection the longest so far is 9, so 12 seems a pretty good choice.

@final
class TextAnalysisLocation(WeakRefable, AutoSlots):
    def __init__(self, analysis: WeakRef[TextAnalysis], token: ProcessedToken, character_start_index: int, token_index: int) -> None:
        self.weakref = WeakRef(self)
        self.next: WeakRef[TextAnalysisLocation] | None = None
        self.previous: WeakRef[TextAnalysisLocation] | None = None
        self.token: ProcessedToken = token
        self.is_shadowed_by: list[WeakRef[TextAnalysisLocation]] = []
        self.shadows: list[WeakRef[TextAnalysisLocation]] = []
        self.analysis: WeakRef[TextAnalysis] = analysis
        self.token_index: int = token_index
        self.character_start_index: int = character_start_index
        self.character_end_index: int = character_start_index + len(self.token.surface) - 1

        self.known_words: list[CandidateWord] = []
        self.valid_words: QList[CandidateWord] = QList()
        self.display_variants: list[CandidateWordVariant] = []
        self.valid_variants: QList[CandidateWordVariant] = QList()
        self.indexing_variants: QList[CandidateWordVariant] = QList()
        self.candidate_words: QList[CandidateWord] = QList()
        self.display_words: list[CandidateWord] = []

    @override
    def __repr__(self) -> str:
        return f"""
TextLocation('{self.character_start_index}-{self.character_end_index}, {self.token.surface} | {self.token.base_form})
{newline.join([cand.__repr__() for cand in self.known_words])}
"""

    def forward_list(self, length: int = 99999) -> list[TextAnalysisLocation]:
        return self.analysis().locations[self.token_index: self.token_index + length + 1]

    def analysis_step_1_analyze_non_compound_validity(self) -> None:
        lookahead_max = min(_max_lookahead, len(self.forward_list(_max_lookahead)))
        self.candidate_words = QList(CandidateWord([location.weakref for location in self.forward_list(index)]) for index in range(lookahead_max - 1, -1, -1))
        self.candidate_words[-1].run_validity_analysis()  # the non-compound part needs to be completed first

    def analysis_step_2_analyze_compound_validity(self) -> None:
        for range_ in self.candidate_words[:-1]:  # we already have the last one completed
            range_.run_validity_analysis()

        self.known_words = self.candidate_words.where(lambda candidate: candidate.is_word).to_list()
        self.valid_words = self.candidate_words.where(lambda candidate: candidate.has_valid_words()).to_list()
        self.indexing_variants = self.candidate_words.select_many(lambda candidate: candidate.indexing_variants).to_list()
        self.valid_variants = self.valid_words.select_many(lambda valid: valid.valid_variants).to_list()

    def _run_display_analysis_pass_true_if_there_were_changes(self) -> bool:
        changes_made = False
        for range_ in self.candidate_words:
            if range_.run_display_analysis_pass_true_if_there_were_changes():
                changes_made = True

        if changes_made:
            self.display_words = [candidate for candidate in self.candidate_words if candidate.display_variants]
        return changes_made

    def analysis_step_3_run_initial_display_analysis(self) -> None:
        self._run_display_analysis_pass_true_if_there_were_changes()

    def analysis_step_5_resolve_chains_of_compounds_yielding_to_the_next_compound_pass_true_if_there_were_changes(self) -> bool:
        # todo this does not feel great. Currently we need the first version of display_words_starting_here to be created
        # in order for the DisplayRequirements class to inspect it and mark itself as not being displayed so that it can be removed here.
        # this is some truly strange invisible order dependency that is making me quite uncomfortable
        # it also relies on the check for is_yield_last_token_to_overlapping_compound_requirement_fulfilled to return different values at different times
        # because that method has a circular dependency to display_words_starting_here which we set up here.

        the_next_compound_yields_to_the_one_after_that_so_this_one_no_longer_yields = self._run_display_analysis_pass_true_if_there_were_changes()
        if self.display_words and not any(self.is_shadowed_by):
            self.display_variants = self.display_words[0].display_variants

            covering_forward_count = self.display_words[0].location_count - 1
            for shadowed in self.forward_list(covering_forward_count)[1:]:
                shadowed.is_shadowed_by.append(self.weakref)
                self.shadows.append(shadowed.weakref)
                for shadowed_shadowed in shadowed.shadows:
                    shadowed_shadowed().is_shadowed_by.remove(shadowed.weakref)
                shadowed.shadows.clear()

        return the_next_compound_yields_to_the_one_after_that_so_this_one_no_longer_yields

    def is_next_location_inflecting_word(self) -> bool:
        return self.next is not None and self.next().is_inflecting_word()

    # todo: having this check here only means that marking a compound as an inflecting word has no effect, and figuring out why things are not working can be quite a pain
    def is_inflecting_word(self) -> bool:
        vocab = app.col().vocab.with_any_form_in([self.token.base_form, self.token.surface])
        return any(voc for voc in vocab if voc.matching_configuration.bool_flags.is_inflecting_word.is_active)
</file>

<file path="language_services/janome_ex/word_extraction/word_exclusion.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from sysutils.json.json_reader import JsonReader



@final
class WordExclusion(AutoSlots):
    secret = "aoesunth9cgrcgf"
    _no_index = -1
    def __init__(self, word: str, index: int, _secret: str) -> None:
        if _secret != "aoesunth9cgrcgf": raise ValueError("please use the factory methods instead of this private constructor")
        self.word = word
        self.index = index

    def excludes_form_at_index(self, form: str, index: int) -> bool:
        return form == self.word and (self.index == WordExclusion._no_index or self.index == index)

    @classmethod
    def global_(cls, exclusion: str) -> WordExclusion: return WordExclusion(exclusion.strip(), WordExclusion._no_index, WordExclusion.secret)
    @classmethod
    def at_index(cls, exclusion: str, index: int) -> WordExclusion: return WordExclusion(exclusion.strip(), index, WordExclusion.secret)

    @override
    def __eq__(self, other: object) -> bool:
        if isinstance(other, WordExclusion):
            return self.word == other.word and self.index == other.index
        return False

    @override
    def __hash__(self) -> int:
        return hash((self.word, self.index))

    @override
    def __repr__(self) -> str:
        return f"WordExclusion('{self.word}', {self.index})"

    def excludes_all_words_excluded_by(self, other: WordExclusion) -> bool:
        return self.word == other.word and (self.index == WordExclusion._no_index or self.index == other.index)

    def to_dict(self) -> dict[str, object]:
        return {"word": self.word, "index": self.index}

    @classmethod
    def from_reader(cls, reader: JsonReader) -> WordExclusion:
        return cls(word=reader.string("word"), index=reader.integer("index"), _secret=WordExclusion.secret)
</file>

<file path="language_services/katakana_chart.py">
from __future__ import annotations

from ex_autoslot import AutoSlots


# noinspection PyUnusedName,PyUnusedClass
class KatakanaChart(AutoSlots):
    k_index: int = 1
    s_index: int = 2
    t_index: int = 3
    n_index: int = 4
    h_index: int = 5
    m_index: int = 6
    y_index: int = 7
    r_index: int = 8
    w_index: int = 9

    a_row_1: list[str] = ["ア", "カ", "サ", "タ", "ナ", "ハ", "マ", "ヤ", "ラ", "ワ"]
    a_row_2: list[str] = ["　", "ガ", "ザ", "ダ", "　", "バ", "　", "　", "　", "　"]
    a_row_3: list[str] = ["　", "　", "　", "　", "　", "パ", "　", "　", "　", "　"]

    i_row_1: list[str] = ["イ", "キ", "シ", "チ", "ニ", "ヒ", "ミ", "　", "リ", "　"]
    i_row_2: list[str] = ["　", "ギ", "ジ", "ヂ", "　", "ビ", "　", "　", "　", "　"]
    i_row_3: list[str] = ["　", "　", "　", "　", "　", "ピ", "　", "　", "　", "　"]

    u_row_1: list[str] = ["ウ", "ク", "ス", "ツ", "ヌ", "フ", "ム", "ユ", "ル", "　"]
    u_row_2: list[str] = ["　", "グ", "ズ", "ヅ", "　", "ブ", "　", "　", "　", "　"]
    u_row_3: list[str] = ["　", "　", "　", "　", "　", "プ", "　", "　", "　", "　"]

    e_row_1: list[str] = ["エ", "ケ", "セ", "テ", "ネ", "ヘ", "メ", "　", "レ", "　"]
    e_row_2: list[str] = ["　", "ゲ", "ゼ", "デ", "　", "ベ", "　", "　", "　", "　"]
    e_row_3: list[str] = ["　", "　", "　", "　", "　", "ペ", "　", "　", "　", "　"]

    o_row_1: list[str] = ["オ", "コ", "ソ", "ト", "ノ", "ホ", "モ", "ヨ", "ロ", "ヲ"]
    o_row_2: list[str] = ["　", "ゴ", "ゾ", "ド", "　", "ボ", "　", "　", "　", "　"]
    o_row_3: list[str] = ["　", "　", "　", "　", "　", "ポ", "　", "　", "　", "　"]
</file>

<file path="line_profiling_hacks.py">
from __future__ import annotations

import os
from typing import TYPE_CHECKING, ParamSpec, TypeVar

if TYPE_CHECKING:
    from collections.abc import Callable

is_running_line_profiling = os.environ.get("LINE_PROFILE") == "1"

# noinspection PyUnusedName
ParameterSpec = ParamSpec("ParameterSpec")  # Preserves parameter types and names
# noinspection PyUnusedName
ReturnValueType = TypeVar("ReturnValueType")  # Preserves return type

if is_running_line_profiling:
    from line_profiler_pycharm import profile  # pyright: ignore [reportMissingTypeStubs, reportUnknownVariableType, reportAssignmentType]
else:
    def profile[**ParameterSpec, ReturnValueType](func: Callable[ParameterSpec, ReturnValueType]) -> Callable[ParameterSpec, ReturnValueType]:
        return func

def profile_lines[**ParameterSpec, ReturnValueType](func: Callable[ParameterSpec, ReturnValueType]) -> Callable[ParameterSpec, ReturnValueType]:
    return profile(func)
</file>

<file path="manually_copied_in_libraries/autoslot.py">
""" Classes and metaclasses for easier ``__slots__`` handling.  """
# we manually copied this into the project because pyright did not understand the types when imported as a library and since we inherit these everywhere analysis broke down completely
from __future__ import annotations

import dis
from inspect import getmro
from itertools import tee

__version__ = "2024.12.1"
__all__ = ["Slots", "SlotsMeta", "SlotsPlusDict", "SlotsPlusDictMeta"]


def assignments_to_self(method) -> set:
    """Given a method, collect all the attribute names for assignments
    to "self"."""
    # Get the name of the var used to refer to the instance. Usually,
    # this will be "self". It's the first parameter to the
    # __init__(self, ...)  method call. If there are no parameters,
    # just pretend it was "self".
    instance_var = next(iter(method.__code__.co_varnames), "self")
    # We need to scan all the bytecode instructions to see all the times
    # an attribute of "self" got assigned-to. First get the list of
    # instructions.
    instructions = dis.Bytecode(method)
    # Assignments to attributes of "self" are identified by a first
    # LOAD_FAST (with a value of "self") immediately followed by a
    # STORE_ATTR (with a value of the attribute name). So we will need
    # to look at a sequence of pairs through the bytecode. The easiest
    # way to do this is with two iterators.
    i0, i1 = tee(instructions)
    # March the second one ahead by one step.
    next(i1, None)
    names = set()
    # a and b are a pair of bytecode instructions; b follows a.
    for a, b in zip(i0, i1, strict=False):
        accessing_self = (
            (
                a.opname in ("LOAD_FAST", "LOAD_DEREF")
                and a.argval == instance_var
            ) or (
                a.opname == "LOAD_FAST_LOAD_FAST"
                and a.argval[1] == instance_var
            )
        )
        storing_attribute = (b.opname == "STORE_ATTR")
        if accessing_self and storing_attribute:
            names.add(b.argval)
    return names


class SlotsMeta(type):
    def __new__(mcs, name, bases, ns):
        # Caller may have already provided slots, in which case just
        # retain them and keep going. Note that we make a set() to make
        # it easier to avoid dupes.
        slots = set(ns.get("__slots__", ()))
        if "__init__" in ns:
            slots |= assignments_to_self(ns["__init__"])
        ns["__slots__"] = slots
        return super().__new__(mcs, name, bases, ns)


class Slots(metaclass=SlotsMeta):
    pass


def super_has_dict(cls):
    return hasattr(cls, "__slots__") and "__dict__" in cls.__slots__


class SlotsPlusDictMeta(SlotsMeta):
    def __new__(mcs, name, bases, ns):
        slots = set(ns.get("__slots__", ()))
        # It seems like "__dict__" is only allowed to appear once in
        # the entire MRO slots hierarchy, so check them all to see
        # whether to add __dict__ or not.
        if not any(super_has_dict(s) for b in bases for s in getmro(b)):
            slots.add("__dict__")
        ns["__slots__"] = slots
        return super().__new__(mcs, name, bases, ns)


class SlotsPlusDict(metaclass=SlotsPlusDictMeta):
    pass
</file>

<file path="mylog.py">
from __future__ import annotations

import os
from typing import TYPE_CHECKING

from sysutils.lazy import Lazy

if TYPE_CHECKING:
    import logging
    from logging.handlers import RotatingFileHandler
    from pathlib import Path

def is_testing() -> bool:
    import sys
    return "pytest" in sys.modules

def log_file_path(addon: str) -> Path:
    from pathlib import Path

    from aqt import mw
    logs_dir = Path(mw.addonManager.addonsFolder(addon)) / "user_files" / "logs"
    logs_dir.mkdir(parents=True, exist_ok=True)
    return logs_dir / f"{addon}.log"

def get_logger(module: str) -> logging.Logger:
    import logging
    import sys
    from logging.handlers import RotatingFileHandler

    from anki.hooks import wrap  # pyright: ignore[reportUnknownVariableType]
    from aqt import mw
    from aqt.addons import AddonManager
    addon = ""
    if is_testing():
        logger = logging.getLogger("addon")
    else:
        addon = mw.addonManager.addonFromModule(module)
        logger = logging.getLogger(addon)
    logger.setLevel(logging.DEBUG)
    logger.propagate = False

    stdout_handler = logging.StreamHandler(stream=sys.stdout)
    stdout_handler.setLevel(logging.DEBUG if "ANKIDEV" in os.environ else logging.INFO)
    stdout_formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    stdout_handler.setFormatter(stdout_formatter)
    logger.addHandler(stdout_handler)

    file_handler: RotatingFileHandler | None = None

    # Prevent errors when deleting/updating the add-on on Windows
    # noinspection PyUnusedLocal
    def close_log_file(manager: AddonManager, m: str, *args: object, **kwargs: object) -> None:  # pyright: ignore[reportUnusedParameter]
        if m == addon and file_handler:
            file_handler.close()

    if not is_testing():
        log_path = log_file_path(addon)
        # noinspection PyArgumentEqualDefault
        file_handler = RotatingFileHandler(str(log_path), "a", encoding="utf-8", maxBytes=3 * 1024 * 1024, backupCount=5, )
        file_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)

        AddonManager.deleteAddon = wrap(  # type: ignore
            AddonManager.deleteAddon, close_log_file, "before"
        )
        AddonManager.backupUserFiles = wrap( # type: ignore
            AddonManager.backupUserFiles, close_log_file, "before"
        )

    return logger

_addon_name = os.path.basename(os.path.dirname(__file__))
_logger: Lazy[logging.Logger] = Lazy(lambda: get_logger(_addon_name))

def info(msg: str) -> None: _logger().info(msg)
def warning(msg: str) -> None: _logger().warning(msg)
</file>

<file path="note/cardutils.py">
# noinspection PyUnresolvedReferences
from __future__ import annotations

from typing import TYPE_CHECKING

from anki.consts import QUEUE_TYPE_NEW
from ex_autoslot import AutoSlots
from note.note_constants import NoteTypes
from sysutils import typed

if TYPE_CHECKING:
    from anki.cards import Card


class CardUtils(AutoSlots):
    @staticmethod
    def is_new(card: Card) -> bool:
        return card.queue == QUEUE_TYPE_NEW

    @classmethod
    def get_note_type_priority(cls, card: Card) -> int:
        note_type_name = typed.str_(card.note_type()["name"])  # pyright: ignore[reportAny]
        if note_type_name == NoteTypes.Kanji: return 1
        if note_type_name == NoteTypes.Vocab: return 2

        return 4  # It's nice to use it for other note types too so default them to 4.

    @classmethod
    def prioritize(cls, card: Card) -> None:
        if CardUtils.is_new(card):
            card.due = cls.get_note_type_priority(card)
            card.flush()
</file>

<file path="note/collection/backend_facade.py">
from __future__ import annotations

from typing import TYPE_CHECKING, final

from anki_extentions.note_ex import NoteBulkLoader
from ex_autoslot import AutoSlots
from line_profiling_hacks import profile_lines
from note.jpnote import JPNote

if TYPE_CHECKING:
    from collections.abc import Callable, Sequence

    from anki.collection import Collection
    from anki.notes import Note, NoteId
    from qt_utils.task_runner_progress_dialog import ITaskRunner

@final
class BackEndFacade[TNote: JPNote](AutoSlots):
    def __init__(self, anki_collection: Collection, constructor: Callable[[Note], TNote], note_type: str) -> None:
        self.anki_collection = anki_collection
        self.jp_note_constructor = constructor
        self.note_type = note_type

    @profile_lines
    def all(self, task_runner: ITaskRunner) -> list[TNote]:
        backend_notes = NoteBulkLoader.load_all_notes_of_type(self.anki_collection, self.note_type, task_runner)
        return task_runner.process_with_progress(backend_notes, self.jp_note_constructor, f"Constructing {self.note_type} notes")

    def search(self, query: str) -> list[TNote]:
        return self.by_id(self.anki_collection.find_notes(query))

    def by_id(self, note_ids: Sequence[NoteId]) -> list[TNote]:
        return [self.jp_note_constructor(self.anki_collection.get_note(note_id)) for note_id in note_ids]
</file>

<file path="note/collection/cache_runner.py">
from __future__ import annotations

import threading
from typing import TYPE_CHECKING, cast

from anki import hooks
from anki.models import ModelManager, NotetypeDict
from anki_extentions.notetype_ex.note_type_ex import NoteTypeEx
from ankiutils import app
from ex_autoslot import AutoSlots
from note.note_constants import NoteTypes
from sysutils import app_thread_pool, ex_assert, ex_thread
from sysutils.typed import checked_cast

if TYPE_CHECKING:
    from collections.abc import Callable, Sequence

    from anki.collection import Collection
    from anki.decks import DeckId
    from anki.notes import Note, NoteId

class CacheRunner(AutoSlots):
    def __init__(self, anki_collection: Collection) -> None:
        self._merge_pending_subscribers: list[Callable[[], None]] = []
        self._will_add_subscribers: list[Callable[[Note], None]] = []
        self._will_flush_subscribers: list[Callable[[Note], None]] = []
        self._will_remove_subscribers: list[Callable[[Sequence[NoteId]], None]] = []
        self._destructors: list[Callable[[], None]] = []
        self._anki_collection: Collection = anki_collection
        self._running: bool = False
        self._lock: threading.RLock = threading.RLock()

        model_manager: ModelManager = anki_collection.models
        all_note_types: list[NoteTypeEx] = [NoteTypeEx.from_dict(model) for model in model_manager.all()]
        self._note_types: list[NoteTypeEx] = [note_type for note_type in all_note_types if note_type.name in NoteTypes.ALL]
        ex_assert.equal(len(self._note_types), len(NoteTypes.ALL))

        hooks.notes_will_be_deleted.append(self._on_will_be_removed)  # pyright: ignore[reportUnknownMemberType]
        hooks.note_will_be_added.append(self._on_will_be_added)  # pyright: ignore[reportUnknownMemberType]
        hooks.note_will_flush.append(self._on_will_flush)

    def start(self) -> None:
        with self._lock:
            ex_assert.that(not self._running)
            self._running = True
            app_thread_pool.pool.submit(self._run_periodic_flushes)

    def _run_periodic_flushes(self) -> None:
        while self._running:
            self.flush_updates()
            ex_thread.sleep_thread_not_doing_the_current_work(0.1)
#
    def destruct(self) -> None:
        with self._lock:
            self._running = False
            self._internal_flush_updates()

            hooks.notes_will_be_deleted.remove(self._on_will_be_removed)  # pyright: ignore[reportUnknownMemberType]
            hooks.note_will_be_added.remove(self._on_will_be_added)  # pyright: ignore[reportUnknownMemberType]
            hooks.note_will_flush.remove(self._on_will_flush)

            for destructor in self._destructors: destructor()

            from note import noteutils
            noteutils.clear_studying_cache()

    def _internal_flush_updates(self) -> None:
        for callback in self._merge_pending_subscribers: callback()

    def flush_updates(self) -> None:
        with self._lock:
            self._check_for_updated_note_types_and_reset_app_if_found()
            self._internal_flush_updates()

    def _on_will_be_added(self, _collection: Collection, backend_note: Note, _deck_id: DeckId) -> None:
        if not self._running: return
        with self._lock:
            for callback in self._will_add_subscribers: callback(backend_note)

    def _on_will_flush(self, backend_note: Note) -> None:
        if not self._running: return
        with self._lock:
            for subscriber in self._will_flush_subscribers: subscriber(backend_note)

    def _on_will_be_removed(self, _: Collection, note_ids: Sequence[NoteId]) -> None:
        if not self._running: return
        with self._lock:
            for callback in self._will_remove_subscribers: callback(note_ids)

    def connect_merge_pending_adds(self, _merge_pending_added_notes: Callable[[], None]) -> None:
        with self._lock:
            self._merge_pending_subscribers.append(_merge_pending_added_notes)

    def connect_will_add(self, _merge_pending_added_notes: Callable[[Note], None]) -> None:
        with self._lock:
            self._will_add_subscribers.append(_merge_pending_added_notes)

    def connect_will_flush(self, _merge_pending_added_notes: Callable[[Note], None]) -> None:
        with self._lock:
            self._will_flush_subscribers.append(_merge_pending_added_notes)

    def connect_will_remove(self, _merge_pending_added_notes: Callable[[Sequence[NoteId]], None]) -> None:
        with self._lock:
            self._will_remove_subscribers.append(_merge_pending_added_notes)

    def _check_for_updated_note_types_and_reset_app_if_found(self) -> None:
        for cached_note_type in self._note_types:
            ex_assert.not_none(self._anki_collection.db)
            current = NoteTypeEx.from_dict(cast(NotetypeDict, checked_cast(ModelManager, self._anki_collection.models).get(cached_note_type.id)))
            try:
                current.assert_schema_matches(cached_note_type)
            except AssertionError:
                app_thread_pool.pool.submit(lambda: app.reset())  # We are running on the thread that will be killed by the reset...
</file>

<file path="note/collection/jp_collection.py">
from __future__ import annotations

import threading
from typing import TYPE_CHECKING

import mylog
from ankiutils import app
from ex_autoslot import AutoSlots
from line_profiling_hacks import profile_lines
from note import noteutils
from note.collection.cache_runner import CacheRunner
from note.collection.kanji_collection import KanjiCollection
from note.collection.sentence_collection import SentenceCollection
from note.collection.vocab_collection import VocabCollection
from note.jpnote import JPNote
from note.note_constants import Mine
from qt_utils.task_runner_progress_dialog import TaskRunner
from sysutils import app_thread_pool
from sysutils.timeutil import StopWatch
from sysutils.typed import non_optional
from sysutils.weak_ref import WeakRefable

if TYPE_CHECKING:
    from anki.collection import Collection
    from anki.notes import NoteId

class JPCollection(WeakRefable, AutoSlots):
    _is_inital_load: bool = True  # running the GC on initial load slows startup a lot but does not decrease memory usage in any significant way.

    def __init__(self, anki_collection: Collection, delay_seconds: float | None = None) -> None:
        from sysutils.object_instance_tracker import ObjectInstanceTracker
        self._instance_tracker: ObjectInstanceTracker = ObjectInstanceTracker.tracker_for(self)
        self.anki_collection: Collection = anki_collection
        self._is_initialized: bool = False
        self._initialization_started: bool = False
        self._cache_runner: CacheRunner | None = None

        self._vocab: VocabCollection | None = None
        self._kanji: KanjiCollection | None = None
        self._sentences: SentenceCollection | None = None
        self._pending_init_timer: threading.Timer | None = None
        if delay_seconds is not None:
            self._pending_init_timer = threading.Timer(delay_seconds or 0, self._initialize_wrapper)
            self._pending_init_timer.start()
        else:
            self._initialize()

    def reshchedule_init_for(self, delay_seconds: float) -> None:
        if self._initialization_started: return
        if self._pending_init_timer is None: raise AssertionError("Pending init timer is None")
        self._pending_init_timer.cancel()
        self._pending_init_timer = threading.Timer(delay_seconds, self._initialize_wrapper)

    def _initialized_self(self) -> JPCollection:
        if not self._is_initialized:
            self._initialize()
        return self

    def _initialize_wrapper(self) -> None:
        if app.config().load_studio_in_foreground.get_value():
            app_thread_pool.run_on_ui_thread_synchronously(self._initialize)
        else:
            self._initialize()

    @profile_lines
    def _initialize(self) -> None:
        if self._initialization_started:
            return
        self._initialization_started = True
        mylog.info("JPCollection.__init__")
        if self._pending_init_timer is not None:
            self._pending_init_timer.cancel()
        app.get_ui_utils().tool_tip(f"{Mine.app_name} loading", 60000)
        stopwatch = StopWatch()
        with StopWatch.log_warning_if_slower_than(5, "Full collection setup"):
            task_runner = TaskRunner.create(f"Loading {Mine.app_name}", "reading notes from anki", not app.is_testing and app.config().load_studio_in_foreground.get_value())
            if not app.is_testing and not JPCollection._is_inital_load:
                task_runner.set_label_text("Running garbage collection")
                self._instance_tracker.run_gc_if_multiple_instances_and_assert_single_instance_after_gc()
                app.get_ui_utils().tool_tip(f"{Mine.app_name} loading", 60000)

            with StopWatch.log_warning_if_slower_than(5, "Core collection setup - no gc"):
                self._cache_runner = CacheRunner(self.anki_collection)

                self._vocab = VocabCollection(self.anki_collection, self._cache_runner, task_runner)
                self._sentences = SentenceCollection(self.anki_collection, self._cache_runner, task_runner)
                self._kanji = KanjiCollection(self.anki_collection, self._cache_runner, task_runner)

            self._cache_runner.start()

            if app.config().load_jamdict_db_into_memory.get_value():
                from language_services.jamdict_ex.dict_lookup import DictLookup
                task_runner.run_on_background_thread_with_spinning_progress_dialog("Loading Jamdict db into memory", DictLookup.ensure_loaded_into_memory)

            if app.config().pre_cache_card_studying_status.get_value():
                noteutils.initialize_studying_cache(self.anki_collection, task_runner)

            if not app.is_testing and not JPCollection._is_inital_load:
                self._instance_tracker.run_gc_if_multiple_instances_and_assert_single_instance_after_gc()

            self._is_initialized = True
            JPCollection._is_inital_load = False

            task_runner.close()
            app.get_ui_utils().tool_tip(f"{Mine.app_name} done loading in {str(stopwatch.elapsed_seconds())[0:4]} seconds.", milliseconds=6000)

    @property
    def cache_runner(self) -> CacheRunner: return non_optional(self._initialized_self()._cache_runner)
    @property
    def is_initialized(self) -> bool: return self._is_initialized
    @property
    def vocab(self) -> VocabCollection: return non_optional(self._initialized_self()._vocab)
    @property
    def kanji(self) -> KanjiCollection: return non_optional(self._initialized_self()._kanji)
    @property
    def sentences(self) -> SentenceCollection: return non_optional(self._initialized_self()._sentences)

    @classmethod
    def note_from_note_id(cls, note_id: NoteId) -> JPNote:
        col = app.col()
        return (col.kanji.with_id_or_none(note_id)
                or col.vocab.with_id_or_none(note_id)
                or col.sentences.with_id_or_none(note_id)
                or JPNote(app.anki_collection().get_note(note_id)))

    def destruct_sync(self) -> None:
        if self._pending_init_timer is not None: self._pending_init_timer.cancel()
        if self._is_initialized:
            self.cache_runner.destruct()
            self._is_initialized = False

    def flush_cache_updates(self) -> None: self.cache_runner.flush_updates()
</file>

<file path="note/collection/kanji_collection.py">
from __future__ import annotations

from collections import defaultdict
from typing import TYPE_CHECKING, final, override

from ex_autoslot import AutoSlots
from queryablecollections.collections.q_list import QList
from queryablecollections.q_iterable import query

if TYPE_CHECKING:
    from anki.collection import Collection
    from anki.notes import Note, NoteId
    from note.collection.cache_runner import CacheRunner
    from qt_utils.task_runner_progress_dialog import ITaskRunner

from note.collection.backend_facade import BackEndFacade
from note.collection.note_cache import CachedNote, NoteCache
from note.kanjinote import KanjiNote
from note.note_constants import NoteTypes
from sysutils import kana_utils


@final
class _KanjiSnapshot(CachedNote, AutoSlots):
    def __init__(self, note: KanjiNote) -> None:
        super().__init__(note)
        self.radicals: set[str] = set(note.get_radicals())
        self.readings: set[str] = set(note.get_readings_clean())

class _KanjiCache(NoteCache[KanjiNote, _KanjiSnapshot], AutoSlots):
    def __init__(self, all_kanji: list[KanjiNote], cache_runner: CacheRunner, task_runner: ITaskRunner) -> None:
        self._by_radical: dict[str, set[KanjiNote]] = defaultdict(set)
        self.by_reading: dict[str, set[KanjiNote]] = defaultdict(set)
        super().__init__(all_kanji, KanjiNote, cache_runner, task_runner)

    @override
    def _create_snapshot(self, note: KanjiNote) -> _KanjiSnapshot: return _KanjiSnapshot(note)

    @override
    def _inheritor_remove_from_cache(self, note: KanjiNote, snapshot:_KanjiSnapshot) -> None:
        for form in snapshot.radicals: self._by_radical[form].remove(note)
        for reading in snapshot.readings: self.by_reading[reading].remove(note)

    @override
    def _inheritor_add_to_cache(self, note: KanjiNote, snapshot: _KanjiSnapshot) -> None:
        for form in snapshot.radicals: self._by_radical[form].add(note)
        for reading in snapshot.readings: self.by_reading[reading].add(note)

    def with_radical(self, radical: str) -> QList[KanjiNote]: return QList(self._by_radical[radical])

class KanjiCollection(AutoSlots):
    def __init__(self, collection: Collection, cache_manager: CacheRunner, task_runner: ITaskRunner) -> None:
        def kanji_constructor_call_while_populating_kanji_collection(note: Note) -> KanjiNote: return KanjiNote(note)
        self.collection: BackEndFacade[KanjiNote] = BackEndFacade[KanjiNote](collection, kanji_constructor_call_while_populating_kanji_collection, NoteTypes.Kanji)
        all_kanji = self.collection.all(task_runner)
        self._cache: _KanjiCache = _KanjiCache(all_kanji, cache_manager, task_runner)

    def all(self) -> QList[KanjiNote]: return self._cache.all()

    def with_id_or_none(self, note_id:NoteId) -> KanjiNote | None:
        return self._cache.with_id_or_none(note_id)

    def with_any_kanji_in(self, kanji_list: list[str]) -> QList[KanjiNote]:
        return query(kanji_list).select_many(self._cache.with_question).to_list()  # ex_sequence.flatten([self._cache.with_question(kanji) for kanji in kanji_list])

    def with_kanji(self, kanji: str) -> KanjiNote | None:
        return self._cache.with_question(kanji).single_or_none()

    def with_radical(self, radical:str) -> QList[KanjiNote]: return self._cache.with_radical(radical)
    def with_reading(self, reading:str) -> set[KanjiNote]:
        return self._cache.by_reading[kana_utils.anything_to_hiragana(reading)]

    def add(self, note: KanjiNote) -> None:
        self.collection.anki_collection.addNote(note.backend_note)
        self._cache.add_note_to_cache(note)
</file>

<file path="note/collection/note_cache.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from line_profiling_hacks import profile_lines
from note.jpnote import JPNote
from queryablecollections.collections.q_list import QList
from sysutils.collections.default_dict_case_insensitive import DefaultDictCaseInsensitive
from sysutils.typed import checked_cast

if TYPE_CHECKING:
    from collections.abc import Sequence

    from anki.notes import Note, NoteId
    from note.collection.cache_runner import CacheRunner
    from qt_utils.task_runner_progress_dialog import ITaskRunner

class CachedNote(AutoSlots):
    def __init__(self, note: JPNote) -> None:
        self.id: NoteId = note.get_id()
        self.answer: str = note.get_answer()
        self.question: str = note.get_question()

class NoteCache[TNote: JPNote, TSnapshot: CachedNote](AutoSlots):
    @profile_lines
    def __init__(self, all_notes: list[TNote], cached_note_type: type[TNote], cache_runner: CacheRunner, task_runner: ITaskRunner) -> None:
        self._note_type: type[TNote] = cached_note_type
        self._by_question: DefaultDictCaseInsensitive[set[TNote]] = DefaultDictCaseInsensitive(set)
        self._by_id: dict[NoteId, TNote] = {}
        self._snapshot_by_id: dict[NoteId, TSnapshot] = {}
        self._by_answer: DefaultDictCaseInsensitive[set[TNote]] = DefaultDictCaseInsensitive(set)

        self._deleted: set[NoteId] = set()

        self._flushing: bool = False
        self._pending_add: list[Note] = []

        if len(all_notes) > 0:
            task_runner.process_with_progress(all_notes, self.add_note_to_cache, f"Pushing {all_notes[0].__class__.__name__} notes into cache")

        cache_runner.connect_merge_pending_adds(self._merge_pending_added_notes)
        cache_runner.connect_will_remove(self._on_will_be_removed)
        cache_runner.connect_will_add(self._on_will_be_added)
        cache_runner.connect_will_flush(self._on_will_flush)

    def all(self) -> QList[TNote]:
        return QList(self._by_id.values())

    def with_id_or_none(self, note_id: NoteId) -> TNote | None:
        return self._by_id.get(note_id, None)

    def with_question(self, question: str) -> QList[TNote]:
        return QList(self._by_question[question])

    def _create_snapshot(self, note: TNote) -> TSnapshot: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter]
    def _inheritor_remove_from_cache(self, note: TNote, snapshot: TSnapshot) -> None: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter]
    def _inheritor_add_to_cache(self, note: TNote, snapshot: TSnapshot) -> None: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter]

    def _merge_pending_added_notes(self) -> None:
        completely_added_list = [pending for pending in self._pending_add if pending.id]
        for backend_note in completely_added_list:
            self._pending_add.remove(backend_note)
            note = checked_cast(self._note_type, JPNote.note_from_note(backend_note))
            self.add_note_to_cache(note)

    def _on_will_flush(self, backend_note: Note) -> None:
        if backend_note.id and backend_note.id in self._by_id:
            cached_note = self._by_id[backend_note.id]

            if cached_note.is_flushing:  # our code called flush, we should just make sure the cached data is up to date
                self._refresh_in_cache(cached_note)
            else:  # a note has been edited outside of our control, we need to switch to that up-to-date note and refresh generated data
                note = self._create_note(backend_note)
                with note.recursive_flush_guard.pause_flushing():
                    note.update_generated_data()
                    self._refresh_in_cache(note)
        elif backend_note.id in self._deleted:  # undeleted note
            self._deleted.remove(backend_note.id)
            note = self._create_note(backend_note)
            with note.recursive_flush_guard.pause_flushing():
                note.update_generated_data()
                self.add_note_to_cache(note)

    def _on_will_be_added(self, backend_note: Note) -> None:
        note = JPNote.note_from_note(backend_note)
        if isinstance(note, self._note_type):
            self._pending_add.append(backend_note)

    def _on_will_be_removed(self, note_ids: Sequence[NoteId]) -> None:
        my_notes_ids = [note_id for note_id in note_ids if note_id in self._by_id]
        cached_notes = [self._by_id[note_id] for note_id in my_notes_ids]
        self._deleted.update(my_notes_ids)
        for cached in cached_notes:
            self._remove_from_cache(cached)

    def _create_note(self, backend_note: Note) -> TNote:
        return checked_cast(self._note_type, JPNote.note_from_note(backend_note))

    def _refresh_in_cache(self, note: TNote) -> None:
        self._remove_from_cache(note)
        self.add_note_to_cache(note)

    def _remove_from_cache(self, note: TNote) -> None:
        assert note.get_id()
        cached = self._snapshot_by_id.pop(note.get_id())
        self._by_id.pop(note.get_id())
        self._by_question[cached.question].remove(note)
        self._by_answer[cached.answer].remove(note)
        self._inheritor_remove_from_cache(note, cached)

    def add_note_to_cache(self, note: TNote) -> None:
        assert note.get_id()
        self._by_id[note.get_id()] = note
        snapshot = self._create_snapshot(note)
        self._snapshot_by_id[note.get_id()] = snapshot
        self._by_question[note.get_question()].add(note)
        self._by_answer[note.get_answer()].add(note)
        self._inheritor_add_to_cache(note, snapshot)
</file>

<file path="note/collection/sentence_collection.py">
from __future__ import annotations

from collections import defaultdict
from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from line_profiling_hacks import profile_lines
from note.collection.backend_facade import BackEndFacade
from note.collection.note_cache import CachedNote, NoteCache
from note.note_constants import NoteTypes
from note.sentences.sentencenote import SentenceNote

if TYPE_CHECKING:
    from anki.collection import Collection
    from anki.notes import Note, NoteId
    from note.collection.cache_runner import CacheRunner
    from note.vocabulary.vocabnote import VocabNote
    from qt_utils.task_runner_progress_dialog import ITaskRunner
    from queryablecollections.collections.q_list import QList

class _SentenceSnapshot(CachedNote, AutoSlots):
    @profile_lines
    def __init__(self, note: SentenceNote) -> None:
        super().__init__(note)
        self.words: set[str] = note.get_words()
        self.detected_vocab: set[int] = note.parsing_result.get().matched_vocab_ids
        self.user_highlighted_vocab: set[str] = set(note.configuration.highlighted_words())

class _SentenceCache(NoteCache[SentenceNote, _SentenceSnapshot], AutoSlots):
    @profile_lines
    def __init__(self, all_kanji: list[SentenceNote], cache_runner: CacheRunner, task_runner: ITaskRunner) -> None:
        self._by_vocab_form: dict[str, set[SentenceNote]] = defaultdict(set)
        self._by_user_highlighted_vocab: dict[str, set[SentenceNote]] = defaultdict(set)
        self._by_vocab_id: dict[int, set[SentenceNote]] = defaultdict(set)
        super().__init__(all_kanji, SentenceNote, cache_runner, task_runner)

    @override
    def _create_snapshot(self, note: SentenceNote) -> _SentenceSnapshot: return _SentenceSnapshot(note)

    def with_vocab(self, vocab: VocabNote) -> list[SentenceNote]: return list(self._by_vocab_id[vocab.get_id()])
    def with_vocab_form(self, form: str) -> list[SentenceNote]: return list(self._by_vocab_form[form])
    def with_user_highlighted_vocab(self, form: str) -> list[SentenceNote]: return list(self._by_user_highlighted_vocab[form])

    @override
    def _inheritor_remove_from_cache(self, note: SentenceNote, snapshot: _SentenceSnapshot) -> None:
        for vocab_form in snapshot.words: self._by_vocab_form[vocab_form].remove(note)
        for vocab_form in snapshot.user_highlighted_vocab: self._by_user_highlighted_vocab[vocab_form].remove(note)
        for vocab_id in snapshot.detected_vocab: self._by_vocab_id[vocab_id].remove(note)

    @override
    def _inheritor_add_to_cache(self, note: SentenceNote, snapshot: _SentenceSnapshot) -> None:
        for vocab_form in snapshot.words: self._by_vocab_form[vocab_form].add(note)
        for vocab_form in snapshot.user_highlighted_vocab: self._by_user_highlighted_vocab[vocab_form].add(note)
        for vocab_id in snapshot.detected_vocab: self._by_vocab_id[vocab_id].add(note)

class SentenceCollection(AutoSlots):
    @profile_lines
    def __init__(self, collection: Collection, cache_manager: CacheRunner, task_runner: ITaskRunner) -> None:
        def sentence_constructor_call_while_populating_sentence_collection(note: Note) -> SentenceNote: return SentenceNote(note)
        self.collection: BackEndFacade[SentenceNote] = BackEndFacade[SentenceNote](collection, sentence_constructor_call_while_populating_sentence_collection, NoteTypes.Sentence)
        all_sentences = list(self.collection.all(task_runner))
        self._cache: _SentenceCache = _SentenceCache(all_sentences, cache_manager, task_runner)

    def all(self) -> list[SentenceNote]: return self._cache.all()

    def with_id_or_none(self, note_id: NoteId) -> SentenceNote | None:
        return self._cache.with_id_or_none(note_id)

    def with_question(self, question: str) -> QList[SentenceNote]:
        return self._cache.with_question(question)

    def with_vocab(self, vocab_note: VocabNote) -> list[SentenceNote]:
        matches = self._cache.with_vocab(vocab_note)
        question = vocab_note.get_question()
        # todo: isn't this check redundant, won't the match have been removed during indexing?
        return [match for match in matches if question not in match.configuration.incorrect_matches.words()]

    def with_vocab_owned_form(self, vocab_note: VocabNote) -> QList[SentenceNote]:
        question = vocab_note.get_question()
        return (vocab_note.forms.not_owned_by_other_vocab()
                .select_many(self._cache.with_vocab_form)
                .distinct()
                .where(lambda match: question not in match.configuration.incorrect_matches.words())  # todo: isn't this check redundant, won't the match have been removed during indexing?
                .to_list())
        # owned_forms = vocab_note.forms.not_owned_by_other_vocab()
        #
        # matches = ex_sequence.remove_duplicates(ex_sequence.flatten([self._cache.with_vocab_form(form) for form in owned_forms]))
        # question = vocab_note.get_question()
        # # todo: isn't this check redundant, won't the match have been removed during indexing?
        # return [match for match in matches if question not in match.configuration.incorrect_matches.words()]

    def with_form(self, form: str) -> list[SentenceNote]: return self._cache.with_vocab_form(form)

    def with_highlighted_vocab(self, vocab_note: VocabNote) -> QList[SentenceNote]:
        return vocab_note.forms.all_set().select_many(self._cache.with_user_highlighted_vocab).to_list()  #ex_sequence.remove_duplicates(ex_sequence.flatten([self._cache.with_user_highlighted_vocab(form) for form in vocab_note.forms.all_set()]))

    def search(self, query: str) -> list[SentenceNote]: return list(self.collection.search(query))

    def add(self, note: SentenceNote) -> None:
        self.collection.anki_collection.addNote(note.backend_note)
        self._cache.add_note_to_cache(note)
</file>

<file path="note/collection/vocab_collection.py">
from __future__ import annotations

from collections import defaultdict
from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from line_profiling_hacks import profile_lines
from note.collection.backend_facade import BackEndFacade
from note.collection.note_cache import CachedNote, NoteCache
from note.note_constants import NoteTypes
from note.vocabulary.vocabnote import VocabNote
from queryablecollections.collections.q_list import QList
from queryablecollections.q_iterable import query

if TYPE_CHECKING:
    from collections.abc import Iterable

    from anki.collection import Collection
    from anki.notes import Note, NoteId
    from note.collection.cache_runner import CacheRunner
    from note.kanjinote import KanjiNote
    from qt_utils.task_runner_progress_dialog import ITaskRunner

class _VocabSnapshot(CachedNote, AutoSlots):
    def __init__(self, note: VocabNote) -> None:
        super().__init__(note)
        self.forms: set[str] = set(note.forms.all_set())
        self.compound_parts: set[str] = set(note.compound_parts.all())
        self.main_form_kanji: set[str] = set(note.kanji.extract_main_form_kanji())
        self.all_kanji: set[str] = note.kanji.extract_all_kanji()
        self.readings: set[str] = set(note.readings.get())
        self.derived_from: str = note.related_notes.derived_from.get()
        self.stems: list[str] = note.conjugator.get_stems_for_primary_form()

class _VocabCache(NoteCache[VocabNote, _VocabSnapshot], AutoSlots):
    @profile_lines
    def __init__(self, all_vocab: list[VocabNote], cache_runner: CacheRunner, task_runner: ITaskRunner) -> None:
        self._by_form: dict[str, set[VocabNote]] = defaultdict(set)
        self._by_kanji_in_main_form: dict[str, set[VocabNote]] = defaultdict(set)
        self._by_kanji_in_any_form: dict[str, set[VocabNote]] = defaultdict(set)
        self._by_compound_part: dict[str, set[VocabNote]] = defaultdict(set)
        self._by_derived_from: dict[str, set[VocabNote]] = defaultdict(set)
        self._by_reading: dict[str, set[VocabNote]] = defaultdict(set)
        self._by_stem: dict[str, set[VocabNote]] = defaultdict(set)
        super().__init__(all_vocab, VocabNote, cache_runner, task_runner)

    def with_form(self, form: str) -> QList[VocabNote]: return QList(self._by_form[form]) if form in self._by_form else QList[VocabNote]()

    def with_compound_part(self, form: str) -> list[VocabNote]:
        compound_parts: set[VocabNote] = set()

        def fetch_parts(part_form: str) -> None:
            for vocab in self._by_compound_part[part_form]:
                if vocab not in compound_parts:
                    compound_parts.add(vocab)
                    fetch_parts(vocab.get_question())

        fetch_parts(form)

        def get_vocab_question(vocab: VocabNote) -> str: return vocab.get_question()
        return sorted(compound_parts, key=get_vocab_question)

    def derived_from(self, form: str) -> list[VocabNote]: return list(self._by_derived_from[form])
    def with_kanji_in_main_form(self, kanji: str) -> list[VocabNote]: return list(self._by_kanji_in_main_form[kanji])
    def with_kanji_in_any_form(self, kanji: str) -> list[VocabNote]: return list(self._by_kanji_in_any_form[kanji])
    def with_reading(self, reading: str) -> list[VocabNote]: return list(self._by_reading[reading])
    def with_stem(self, stem: str) -> list[VocabNote]: return list(self._by_stem[stem])

    @override
    def _create_snapshot(self, note: VocabNote) -> _VocabSnapshot: return _VocabSnapshot(note)

    @override
    def _inheritor_remove_from_cache(self, note: VocabNote, snapshot: _VocabSnapshot) -> None:
        for form in snapshot.forms: self._by_form[form].remove(note)
        for part in snapshot.compound_parts: self._by_compound_part[part].remove(note)
        self._by_derived_from[snapshot.derived_from].remove(note)
        for kanji in snapshot.main_form_kanji: self._by_kanji_in_main_form[kanji].remove(note)
        for kanji in snapshot.all_kanji: self._by_kanji_in_any_form[kanji].remove(note)
        for kanji in snapshot.readings: self._by_reading[kanji].remove(note)
        for stem in snapshot.stems: self._by_stem[stem].remove(note)

    @override
    def _inheritor_add_to_cache(self, note: VocabNote, snapshot: _VocabSnapshot) -> None:
        for form in snapshot.forms: self._by_form[form].add(note)
        for compound_part in snapshot.compound_parts: self._by_compound_part[compound_part].add(note)
        # todo: We add these regardless of whether they have a value in derived from? Won't there be a ton of instances for the empty string?
        self._by_derived_from[snapshot.derived_from].add(note)
        for kanji in snapshot.main_form_kanji: self._by_kanji_in_main_form[kanji].add(note)
        for kanji in snapshot.all_kanji: self._by_kanji_in_any_form[kanji].add(note)
        for reading in snapshot.readings: self._by_reading[reading].add(note)
        for stem in snapshot.stems: self._by_stem[stem].add(note)

class VocabCollection(AutoSlots):
    @profile_lines
    def __init__(self, collection: Collection, cache_manager: CacheRunner, task_runner: ITaskRunner) -> None:
        def vocab_constructor_call_while_populating_vocab_collection(note: Note) -> VocabNote: return VocabNote(note)
        self.collection: BackEndFacade[VocabNote] = BackEndFacade[VocabNote](collection, vocab_constructor_call_while_populating_vocab_collection, NoteTypes.Vocab)
        all_vocab = self.collection.all(task_runner)
        self._cache: _VocabCache = _VocabCache(all_vocab, cache_manager, task_runner)

    def is_word(self, form: str) -> bool: return any(self._cache.with_form(form))
    def all(self) -> list[VocabNote]: return self._cache.all()
    def with_id_or_none(self, note_id: NoteId) -> VocabNote | None: return self._cache.with_id_or_none(note_id)
    def with_form(self, form: str) -> QList[VocabNote]: return self._cache.with_form(form)
    def with_compound_part(self, compound_part: str) -> list[VocabNote]: return self._cache.with_compound_part(compound_part)
    def derived_from(self, derived_from: str) -> list[VocabNote]: return self._cache.derived_from(derived_from)
    def with_kanji_in_main_form(self, kanji: KanjiNote) -> list[VocabNote]: return self._cache.with_kanji_in_main_form(kanji.get_question())
    def with_kanji_in_any_form(self, kanji: KanjiNote) -> list[VocabNote]: return self._cache.with_kanji_in_any_form(kanji.get_question())
    def with_question(self, question: str) -> QList[VocabNote]: return self._cache.with_question(question)
    def with_reading(self, question: str) -> list[VocabNote]: return self._cache.with_reading(question)
    def with_stem(self, question: str) -> list[VocabNote]: return self._cache.with_stem(question)

    def with_form_prefer_exact_match(self, form: str) -> list[VocabNote]:
        matches: list[VocabNote] = self.with_form(form)
        exact_match = [voc for voc in matches if voc.question.without_noise_characters == form]
        sequence = exact_match if exact_match else matches
        return query(sequence).distinct().to_list()

    def with_any_form_in_prefer_exact_match(self, forms: list[str]) -> QList[VocabNote]:
        return query(forms).select_many(self.with_form_prefer_exact_match).distinct().to_list()  #ex_sequence.remove_duplicates_while_retaining_order(ex_sequence.flatten([self.with_form_prefer_exact_match(form) for form in forms]))

    def with_any_form_in(self, forms: list[str]) -> list[VocabNote]:
        return query(forms).select_many(self.with_form).distinct().to_list()  #ex_sequence.remove_duplicates_while_retaining_order(ex_sequence.flatten([self.with_form(form) for form in forms]))

    def with_any_question_in(self, questions: Iterable[str]) -> QList[VocabNote]:
        return query(questions).select(self.with_question).select_many(lambda x: x).to_list()

    def add(self, note: VocabNote) -> None:
        self.collection.anki_collection.addNote(note.backend_note)
        self._cache.add_note_to_cache(note)
</file>

<file path="note/difficulty_calculator.py">
from __future__ import annotations

from ex_autoslot import AutoSlots
from queryablecollections.q_iterable import query
from sysutils import kana_utils


class DifficultyCalculator(AutoSlots):
    def __init__(self, starting_seconds: float, hiragana_seconds: float, katakata_seconds: float, kanji_seconds: float) -> None:
        self.starting_seconds: float = starting_seconds
        self.hiragana_seconds: float = hiragana_seconds
        self.katakata_seconds: float = katakata_seconds
        self.kanji_seconds: float = kanji_seconds

    @staticmethod
    def is_other_character(char: str) -> bool:
        return not kana_utils.character_is_kana(char) and not kana_utils.character_is_kanji(char)

    def allowed_seconds(self, string: str) -> float:
        qstring = query(string)
        hiragana_seconds = qstring.qcount(kana_utils.character_is_hiragana) * self.hiragana_seconds  # ex_sequence.count(string, kana_utils.character_is_hiragana) * self.hiragana_seconds
        katakana_seconds = qstring.qcount(kana_utils.character_is_katakana) * self.katakata_seconds  # ex_sequence.count(string, kana_utils.character_is_katakana) * self.katakata_seconds
        kanji_seconds = qstring.qcount(kana_utils.character_is_kanji) * self.kanji_seconds  # ex_sequence.count(string, kana_utils.character_is_kanji) * self.kanji_seconds

        other_character_seconds = qstring.qcount(self.is_other_character) * self.hiragana_seconds #ex_sequence.count(string, self.is_other_character) * self.hiragana_seconds

        return self.starting_seconds + hiragana_seconds + katakana_seconds + kanji_seconds + other_character_seconds
</file>

<file path="note/jpnote.py">
from __future__ import annotations

from collections import defaultdict
from typing import TYPE_CHECKING, cast, override

from anki.models import NotetypeDict
from anki_extentions.card_ex import CardEx
from anki_extentions.notetype_ex.note_type_ex import NoteTypeEx
from ankiutils import app
from ex_autoslot import AutoSlots
from note import noteutils
from note.note_constants import CardTypes, MyNoteFields, NoteTypes, Tags
from note.note_flush_guard import NoteRecursiveFlushGuard
from sysutils import ex_assert, ex_str
from sysutils.typed import non_optional, str_
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from collections.abc import Callable

    from anki.cards import Card
    from anki.notes import Note, NoteId
    from note.collection.jp_collection import JPCollection

class JPNote(WeakRefable, AutoSlots):
    def __init__(self, note: Note) -> None:
        self.weakref: WeakRef[JPNote] = WeakRef(self)
        self.recursive_flush_guard: NoteRecursiveFlushGuard = NoteRecursiveFlushGuard(self.weakref)
        self.backend_note: Note = note
        self.__hash_value = 0
        self._tag_updated_callbacks: dict[str, list[Callable[[], None]]] = defaultdict(list)

    @property
    def is_flushing(self) -> bool: return self.recursive_flush_guard.is_flushing

    @override
    def __eq__(self, other: object) -> bool:
        ex_assert.not_none(self.get_id(), "You cannot compare or hash a note that has not been saved yet since it has no id")
        return isinstance(other, JPNote) and other.get_id() == self.get_id()

    @override
    def __hash__(self) -> int:
        if not self.__hash_value:
            self.__hash_value = int(self.get_id())
            ex_assert.that(self.__hash_value != 0, "You cannot compare or hash a note that has not been saved yet since it has no id")
        return self.__hash_value

    @override
    def __repr__(self) -> str:
        return f"""{self.get_question()}: {self.get_answer()}"""

    @property
    def collection(self) -> JPCollection:
        return app.col()

    def get_question(self) -> str:
        value = self.get_field(MyNoteFields.question)
        return value if value else "[EMPTY]"

    def get_answer(self) -> str:
        return self.get_field(MyNoteFields.answer)

    def is_studying(self, card: str = "") -> bool:
        return noteutils.has_card_being_studied_cached(self.backend_note, card)

    @classmethod
    def note_from_card(cls, card: Card) -> JPNote:
        note = card.note()
        return cls.note_from_note(note)

    @classmethod
    def note_from_note(cls, note: Note) -> JPNote:
        from note.kanjinote import KanjiNote
        from note.sentences.sentencenote import SentenceNote
        from note.vocabulary.vocabnote import VocabNote

        if cls.get_note_type(note) == NoteTypes.Kanji: return KanjiNote(note)
        if cls.get_note_type(note) == NoteTypes.Vocab: return VocabNote(note)
        if cls.get_note_type(note) == NoteTypes.Sentence: return SentenceNote(note)
        return JPNote(note)

    @staticmethod
    def get_note_type(note: Note) -> str:
        return str_(cast(NotetypeDict, note.note_type())["name"])  # pyright: ignore[reportAny]

    def get_type(self) -> NoteTypeEx:
        return NoteTypeEx.from_dict(non_optional(self.backend_note.note_type()))

    def get_direct_dependencies(self) -> set[JPNote]:
        return set()

    def _get_dependencies_recursive(self, found: set[JPNote]) -> set[JPNote]:
        if self in found:
            return found
        found.add(self)
        for dependency in self.get_direct_dependencies():
            dependency._get_dependencies_recursive(found)
        return found

    def get_dependencies_recursive(self) -> set[JPNote]:
        return self._get_dependencies_recursive(set())

    def get_id(self) -> NoteId:
        return self.backend_note.id

    def cards(self) -> list[CardEx]:
        return [CardEx(card) for card in self.backend_note.cards()]

    def has_suspended_cards(self) -> bool:
        return any(_card for _card in self.cards() if _card.is_suspended())

    def has_active_cards(self) -> bool:
        return any(_card for _card in self.cards() if not _card.is_suspended())

    def has_suspended_cards_or_depencies_suspended_cards(self) -> bool:
        return any(note for note in self.get_dependencies_recursive() if note.has_suspended_cards())

    def unsuspend_all_cards(self) -> None:
        for card in self.cards(): card.un_suspend()

    def unsuspend_all_cards_and_dependencies(self) -> None:
        for note in self.get_dependencies_recursive():
            note.unsuspend_all_cards()

    def suspend_all_cards(self) -> None:
        for card in self.cards(): card.suspend()

    def update_generated_data(self) -> None:
        noteutils.remove_from_studying_cache(self.get_id())

    def get_field(self, field_name: str) -> str:
        return self.backend_note[field_name]

    def _is_persisted(self) -> bool:
        return int(self.backend_note.id) != 0

    def _flush(self) -> None:
        if self._is_persisted():
            self.recursive_flush_guard.flush()

    def set_field(self, field_name: str, value: str) -> None:
        field_value = self.backend_note[field_name]
        if field_value != value:
            self.backend_note[field_name] = value
            self._flush()

    def get_tags(self) -> list[str]:
        return self.backend_note.tags

    def has_tag(self, tag: str) -> bool:
        return self.backend_note.has_tag(tag)

    def priority_tag_value(self) -> int:
        for tag in self.backend_note.tags:
            if tag.startswith(Tags.priority_folder):
                return int(ex_str.first_number(tag))
        return 0

    def get_meta_tags(self) -> set[str]:
        tags: set[str] = set()
        for tag in self.backend_note.tags:
            if tag.startswith(Tags.priority_folder):
                if "high" in tag: tags.add("high_priority")
                if "low" in tag: tags.add("low_priority")

        if self.is_studying(CardTypes.reading) or self.is_studying(CardTypes.listening): tags.add("is_studying")
        if self.is_studying(CardTypes.reading): tags.add("is_studying_reading")
        if self.is_studying(CardTypes.listening): tags.add("is_studying_listening")
        if self.has_tag(Tags.TTSAudio): tags.add("tts_audio")

        return tags

    def get_source_tag(self) -> str:
        source_tags = [t for t in self.get_tags() if t.startswith(Tags.source_folder)]
        if source_tags:
            source_tags = sorted(source_tags, key=lambda tag: len(tag))
            return source_tags[0][len(Tags.source_folder):]
        return ""

    def remove_tag(self, tag: str) -> None:
        if self.has_tag(tag):
            self.backend_note.remove_tag(tag)
            for callback in self._tag_updated_callbacks[tag]: callback()
            self._flush()


    def on_tag_updated(self, tag: str, callback: Callable[[], None]) -> None:
        self._tag_updated_callbacks[tag].append(callback)


    def set_tag(self, tag: str) -> None:
        if not self.has_tag(tag):
            self.backend_note.tags.append(tag)
            for callback in self._tag_updated_callbacks[tag]: callback()
            self._flush()

    def toggle_tag(self, tag: str, on: bool) -> None:
        if on:
            self.set_tag(tag)
        else:
            self.remove_tag(tag)
</file>

<file path="note/kanjinote_mnemonic_maker.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from sysutils import kana_utils
from sysutils.ex_str import newline

if TYPE_CHECKING:
    from note.kanjinote import KanjiNote

def create_default_mnemonic(kanji_note:KanjiNote) -> str:
    readings_mappings = app.config().readings_mappings_dict

    def create_readings_tag(kana_reading: str) -> str:
        reading = kana_utils.romanize(kana_reading)
        reading_length = len(reading)

        if reading in readings_mappings:
            read = readings_mappings[reading]
            return read if read.count("<read>") <= 1 else f"""<compound-reading>{read}</compound-reading>"""

        def try_combine_framentary_matches_into_one_reading() -> str:
            segments_with_mapped_readings_by_start_index: list[list[str]] = []
            for current_postion in range(reading_length):
                all_substrings_permutations_starting_at_current_position = [reading[current_postion:sub_string_length] for sub_string_length in range(current_postion + 1, reading_length + 1)]
                segments_with_mapped_readings_by_start_index.append([candidate for candidate in all_substrings_permutations_starting_at_current_position if candidate in readings_mappings])

            def remove_dead_end_paths() -> None:
                def reached_end_of_reading() -> bool: return path_index + len(match) == reading_length
                def is_dead_end_path() -> bool: return not segments_with_mapped_readings_by_start_index[path_index + len(match)]
                matches_removed = True
                while matches_removed:
                    matches_removed = False
                    for path_index in range(reading_length):
                        for match in segments_with_mapped_readings_by_start_index[path_index]:
                            if not reached_end_of_reading() and is_dead_end_path():
                                matches_removed = True
                                segments_with_mapped_readings_by_start_index[path_index].remove(match)

            def find_shortest_path_prefer_long_starting_segments() -> list[str]:
                shortest_paths_to_position: dict[int, list[str]] = {0: []}  # Start with an empty path at position 0

                def current_index_is_reachable() -> bool: return current_position in shortest_paths_to_position

                def current_segment_is_shortest_path_to_position_after_segment() -> bool:
                    return (position_after_segment not in shortest_paths_to_position or
                            len(shortest_paths_to_position[current_position]) < len(shortest_paths_to_position[position_after_segment]))

                def sort_candidates_longest_first_so_that_the_longest_starting_candidate_will_be_preferred() -> None:
                    for start_position in range(reading_length):
                        segments_with_mapped_readings_by_start_index[start_position].sort(key=lambda candidate: -len(candidate))

                sort_candidates_longest_first_so_that_the_longest_starting_candidate_will_be_preferred()

                current_position:int = 0
                for current_position in range(reading_length):
                    if not current_index_is_reachable(): continue

                    for current_segment in segments_with_mapped_readings_by_start_index[current_position]:
                        position_after_segment:int = current_position + len(current_segment)
                        if current_segment_is_shortest_path_to_position_after_segment():
                            shortest_paths_to_position[position_after_segment] = shortest_paths_to_position[current_position] + [current_segment]

                return shortest_paths_to_position.get(reading_length, [])

            remove_dead_end_paths()
            shortest_path = find_shortest_path_prefer_long_starting_segments()

            if not shortest_path: return ""
            combined_reading = "-".join([readings_mappings[fragment] for fragment in shortest_path])
            return f"""<compound-reading>{combined_reading}</compound-reading>"""


        combined = try_combine_framentary_matches_into_one_reading()
        if combined: return combined

        return f"<read>{reading.capitalize()}</read>"

    radical_names = [rad.get_primary_radical_meaning() for rad in kanji_note.get_radicals_notes()]
    mnemonic = f"""
{" ".join([f"<rad>{name}</rad>" for name in radical_names])}
 <kan>{kanji_note.get_primary_meaning()}</kan>
 {" ".join([create_readings_tag(reading) for reading in kanji_note.get_primary_readings()])}
 ...
""".replace(newline, "")
    return mnemonic.strip()
</file>

<file path="note/kanjinote.py">
from __future__ import annotations

import re
from typing import TYPE_CHECKING, cast, override

from anki.notes import Note
from ankiutils import app
from ex_autoslot import AutoSlots
from note import kanjinote_mnemonic_maker
from note.vocabulary import vocabnote_sorting
from queryablecollections.collections.q_list import QList
from queryablecollections.q_iterable import query
from sysutils.weak_ref import WeakRef

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote

from note.jpnote import JPNote
from note.note_constants import CardTypes, NoteFields, NoteTypes
from sysutils import ex_str, kana_utils, typed


class KanjiNote(JPNote, AutoSlots):
    def __init__(self, note: Note) -> None:
        super().__init__(note)
        self.weakref_kanji: WeakRef[KanjiNote] = cast(WeakRef[KanjiNote], self.weakref)

    @override
    def get_direct_dependencies(self) -> set[JPNote]:
        return set(self.get_radicals_notes())

    def tag_vocab_readings(self, vocab: VocabNote) -> list[str]:
        def primary_reading(read: str) -> str:
            return f'<span class="kanjiReadingPrimary">{read}</span>'

        def secondary_reading(read: str) -> str:
            return f'<span class="kanjiReadingSecondary">{read}</span>'

        primary_readings = self.get_primary_readings()
        secondary_readings = [reading for reading in self.get_readings_clean() if reading not in primary_readings and reading]

        result: list[str] = []

        vocab_form = vocab.get_question()
        for vocab_reading in vocab.readings.get():
            found = False
            for kanji_reading in primary_readings:
                if self.reading_in_vocab_reading(kanji_reading, vocab_reading, vocab_form):
                    result.append(vocab_reading.replace(kanji_reading, primary_reading(kanji_reading)))
                    found = True
                    break

            if not found:
                for kanji_reading in secondary_readings:
                    if self.reading_in_vocab_reading(kanji_reading, vocab_reading, vocab_form):
                        result.append(vocab_reading.replace(kanji_reading, secondary_reading(kanji_reading)))
                        found = True
                        break

                if not found:
                    result.append(vocab_reading)

        return result

    @override
    def get_question(self) -> str:
        return self.get_field(NoteFields.Kanji.question)

    def set_question(self, value: str) -> None:
        self.set_field(NoteFields.Kanji.question, value)

    @override
    def get_answer(self) -> str:
        return self.get_user_answer() or self.get_field(NoteFields.Kanji.source_answer)

    def get_answer_text(self) -> str:
        return ex_str.strip_html_markup(self.get_answer())

    def get_user_answer(self) -> str:
        return self.get_field(NoteFields.Kanji.user_answer)

    def set_user_answer(self, value: str) -> None:
        return self.set_field(NoteFields.Kanji.user_answer, value)

    @override
    def update_generated_data(self) -> None:
        super().update_generated_data()

        self.set_reading_on(kana_utils.katakana_to_hiragana(self.get_reading_on_html()))  # Katakana sneaks in via yomitan etc

        def update_primary_audios() -> None:
            vocab_we_should_play = self.get_primary_vocab().select_many(app.col().vocab.with_question)  # ex_sequence.flatten([app.col().vocab.with_question(vocab) for vocab in self.get_primary_vocab()])
            self.set_primary_vocab_audio("".join([vo.audio.get_primary_audio() for vo in vocab_we_should_play]) if vocab_we_should_play else "")

        self.set_field(NoteFields.Kanji.active_answer, self.get_answer())
        update_primary_audios()

    def get_vocab_notes_sorted(self) -> list[VocabNote]:
        return vocabnote_sorting.sort_vocab_list_by_studying_status(self.get_vocab_notes(), self.get_primary_vocabs_or_defaults(), preferred_kanji=self.get_question())

    def get_vocab_notes(self) -> list[VocabNote]:
        return app.col().vocab.with_kanji_in_any_form(self)

    def get_user_mnemonic(self) -> str:
        return self.get_field(NoteFields.Kanji.user_mnemonic)

    def set_user_mnemonic(self, value: str) -> None:
        self.set_field(NoteFields.Kanji.user_mnemonic, value)

    def get_readings_on(self) -> list[str]:
        return ex_str.extract_comma_separated_values(ex_str.strip_html_markup(self.get_reading_on_html()))

    def get_reading_on_list_html(self) -> list[str]:
        return ex_str.extract_comma_separated_values(self.get_reading_on_html())

    def get_readings_kun(self) -> list[str]:
        return ex_str.extract_comma_separated_values(ex_str.strip_html_markup(self.get_reading_kun_html()))

    def get_reading_kun_list_html(self) -> list[str]:
        return ex_str.extract_comma_separated_values(self.get_reading_kun_html())

    def get_reading_nan_list_html(self) -> list[str]:
        return ex_str.extract_comma_separated_values(self.get_reading_nan_html())

    def get_readings_clean(self) -> list[str]:
        return [ex_str.strip_html_markup(reading) for reading in self.get_reading_on_list_html() + self.get_reading_kun_list_html() + self.get_reading_nan_list_html()]

    def get_reading_on_html(self) -> str:
        return self.get_field(NoteFields.Kanji.Reading_On)

    def set_reading_on(self, value: str) -> None:
        self.set_field(NoteFields.Kanji.Reading_On, value)

    primary_reading_pattern: re.Pattern[str] = re.compile(r"<primary>(.*?)</primary>")

    def get_primary_readings(self) -> list[str]:
        return self.get_primary_readings_on() + self.get_primary_readings_kun() + self.get_primary_readings_nan()

    def get_primary_readings_on(self) -> list[str]:
        return [ex_str.strip_html_markup(reading) for reading in KanjiNote.primary_reading_pattern.findall(self.get_reading_on_html())]  # pyright: ignore[reportAny]

    def get_primary_readings_kun(self) -> list[str]:
        return [ex_str.strip_html_markup(reading) for reading in KanjiNote.primary_reading_pattern.findall(self.get_reading_kun_html())]  # pyright: ignore[reportAny]

    def get_primary_readings_nan(self) -> list[str]:
        return [ex_str.strip_html_markup(reading) for reading in KanjiNote.primary_reading_pattern.findall(self.get_reading_nan_html())]  # pyright: ignore[reportAny]

    def get_reading_kun_html(self) -> str:
        return self.get_field(NoteFields.Kanji.Reading_Kun)

    def set_reading_kun(self, value: str) -> None:
        self.set_field(NoteFields.Kanji.Reading_Kun, value)

    def get_reading_nan_html(self) -> str:
        return self.get_field(NoteFields.Kanji.Reading_Nan)

    def add_primary_on_reading(self, reading: str) -> None:
        self.set_reading_on(ex_str.replace_word(reading, f"<primary>{reading}</primary>", self.get_reading_on_html()))

    def remove_primary_on_reading(self, reading: str) -> None:
        self.set_reading_on(self.get_reading_on_html().replace(f"<primary>{reading}</primary>", reading))

    def add_primary_kun_reading(self, reading: str) -> None:
        self.set_reading_kun(ex_str.replace_word(reading, f"<primary>{reading}</primary>", self.get_reading_kun_html()))

    def remove_primary_kun_reading(self, reading: str) -> None:
        self.set_reading_kun(self.get_reading_kun_html().replace(f"<primary>{reading}</primary>", reading))

    def get_radicals(self) -> list[str]:
        return [rad for rad in ex_str.extract_comma_separated_values(self.get_field(NoteFields.Kanji.Radicals)) if rad != self.get_question()]

    def _set_radicals(self, value: str) -> None:
        self.set_field(NoteFields.Kanji.Radicals, value)

    def get_radicals_notes(self) -> list[KanjiNote]:
        return [kanji_radical for kanji_radical in (app.col().kanji.with_kanji(radical) for radical in self.get_radicals()) if kanji_radical]

    def get_active_mnemonic(self) -> str:
        return self.get_user_mnemonic() if self.get_user_mnemonic() \
            else f"# {kanjinote_mnemonic_maker.create_default_mnemonic(self)}" if app.config().prefer_default_mnemonics_to_source_mnemonics.get_value() \
            else self.get_source_meaning_mnemonic()

    def get_user_similar_meaning(self) -> set[str]:
        return set(ex_str.extract_comma_separated_values(self.get_field(NoteFields.Kanji.user_similar_meaning)))

    def add_user_similar_meaning(self, new_synonym_question: str, _is_recursive_call: bool = False) -> None:
        near_synonyms_questions = self.get_user_similar_meaning()
        near_synonyms_questions.add(new_synonym_question)

        self.set_field(NoteFields.Kanji.user_similar_meaning, ", ".join(near_synonyms_questions))

        if not _is_recursive_call:
            new_synonym = app.col().kanji.with_kanji(new_synonym_question)
            if new_synonym:
                new_synonym.add_user_similar_meaning(self.get_question(), _is_recursive_call=True)

    def get_source_meaning_mnemonic(self) -> str:
        return self.get_field(NoteFields.Kanji.Source_Meaning_Mnemonic)

    def get_related_confused_with(self) -> set[str]:
        return set(ex_str.extract_comma_separated_values(self.get_field(NoteFields.Kanji.related_confused_with)))

    def add_related_confused_with(self, new_confused_with: str) -> None:
        confused_with = self.get_related_confused_with()
        confused_with.add(new_confused_with)
        self.set_field(NoteFields.Kanji.related_confused_with, ", ".join(confused_with))

    def get_primary_vocabs_or_defaults(self) -> list[str]:
        return self.get_primary_vocab() if self.get_primary_vocab() else self.generate_default_primary_vocab()

    def get_primary_vocab(self) -> QList[str]:
        return QList(ex_str.extract_comma_separated_values(self.get_field(NoteFields.Kanji.PrimaryVocab)))

    def set_primary_vocab(self, value: list[str]) -> None:
        self.set_field(NoteFields.Kanji.PrimaryVocab, ", ".join(value))

    _any_word_pattern: re.Pattern[str] = re.compile(r"\b[-\w]+\b", re.UNICODE)

    def get_primary_meaning(self) -> str:
        radical_meaning_match = self._any_word_pattern.search(self.get_answer_text().replace("{", "").replace("}", ""))
        # noinspection PyArgumentEqualDefault
        return radical_meaning_match.group(0) if radical_meaning_match else ""

    _parenthesized_word_pattern: re.Pattern[str] = re.compile(r"\([-\w]+\)", re.UNICODE)

    def get_primary_radical_meaning(self) -> str:
        def get_dedicated_radical_primary_meaning() -> str:
            radical_meaning_match = self._parenthesized_word_pattern.search(self.get_answer_text())
            # noinspection PyArgumentEqualDefault
            return radical_meaning_match.group(0).replace("(", "").replace(")", "") if radical_meaning_match else ""

        result = get_dedicated_radical_primary_meaning()
        return result or self.get_primary_meaning()

    def reading_in_vocab_reading(self, kanji_reading: str, vocab_reading: str, vocab_form: str) -> bool:
        vocab_form = ex_str.strip_html_and_bracket_markup_and_noise_characters(vocab_form)
        covering_readings = [covering_reading for covering_reading in self.get_readings_clean() if kanji_reading != covering_reading and kanji_reading in covering_reading]

        if any(covering_reading for covering_reading in covering_readings if self.reading_in_vocab_reading(covering_reading, vocab_reading, vocab_form)):
            return False

        if vocab_form.startswith(self.get_question()):
            return vocab_reading.startswith(kanji_reading)
        if vocab_form.endswith(self.get_question()):
            return vocab_reading.endswith(kanji_reading)
        return kanji_reading in vocab_reading[1:-1]

    def generate_default_primary_vocab(self) -> list[str]:
        result: list[str] = []

        def sort_key(_vocab: VocabNote) -> tuple[int, int]:
            return -len(_vocab.sentences.studying()), len(_vocab.get_question())

        studying_reading_vocab_in_descending_studying_sentences_order = sorted((voc for voc in self.get_vocab_notes() if voc.is_studying(CardTypes.reading)), key=sort_key)

        sequence = self.get_primary_readings()
        primary_readings = query(sequence).distinct().to_list()
        for primary_reading in primary_readings:
            for vocab in studying_reading_vocab_in_descending_studying_sentences_order:
                if any(vocab.readings.get()) and self.reading_in_vocab_reading(primary_reading, vocab.readings.get()[0], vocab.get_question()):
                    result.append(vocab.get_question())
                    break

        return result

    def position_primary_vocab(self, vocab: str, new_index: int = -1) -> None:
        vocab = vocab.strip()
        primary_vocab_list = self.get_primary_vocab()
        if vocab in primary_vocab_list:
            primary_vocab_list.remove(vocab)

        if new_index == -1:
            primary_vocab_list.append(vocab)
        else:
            primary_vocab_list.insert(new_index, vocab)

        self.set_primary_vocab(primary_vocab_list)

    def remove_primary_vocab(self, vocab: str) -> None:
        self.set_primary_vocab([v for v in self.get_primary_vocab() if v != vocab])

    def set_primary_vocab_audio(self, value: str) -> None:
        self.set_field(NoteFields.Kanji.Audio__, value)

    def bootstrap_mnemonic_from_radicals(self) -> None:
        self.set_user_mnemonic(kanjinote_mnemonic_maker.create_default_mnemonic(self))

    def populate_radicals_from_mnemonic_tags(self) -> None:
        def detect_radicals_from_mnemonic() -> list[str]:
            radical_names = QList(typed.checked_cast_generics(list[str], re.findall(r"<rad>(.*?)</rad>", self.get_user_mnemonic())))

            def kanji_answer_contains_radical_name_as_a_separate_word(radical_name: str, kanji: KanjiNote) -> bool:
                return re.search(r"\b" + re.escape(radical_name) + r"\b", kanji.get_answer()) is not None

            def kanji_answer_contains_any_radical_name_as_a_separate_word(kanji: KanjiNote) -> bool:
                return radical_names.any(lambda name: kanji_answer_contains_radical_name_as_a_separate_word(name, kanji))

            return (app.col().kanji.all()
                    .where(kanji_answer_contains_any_radical_name_as_a_separate_word)
                    .select(lambda kanji: kanji.get_question())
                    .to_list())

        radicals = self.get_radicals()
        for radical in detect_radicals_from_mnemonic():
            if radical not in radicals:
                radicals += radical

        self._set_radicals(", ".join(radicals))

    @classmethod
    def create(cls, question: str, answer: str, on_readings: str, kun_reading: str) -> KanjiNote:
        backend_note = Note(app.anki_collection(), app.anki_collection().models.by_name(NoteTypes.Kanji))
        note = KanjiNote(backend_note)
        note.set_question(question)
        note.set_user_answer(answer)
        note.set_reading_on(on_readings)
        note.set_reading_kun(kun_reading)
        note.update_generated_data()
        app.col().kanji.add(note)
        return note

    def get_romaji_readings(self) -> str:
        return kana_utils.romanize(", ".join(self.get_readings_clean()))
</file>

<file path="note/note_constants.py">
# noinspection PyUnusedName
from __future__ import annotations

from ex_autoslot import AutoSlots


class Builtin(AutoSlots):
    # noinspection PyUnusedName
    Tag: str = "tag"
    Note: str = "note"
    Deck: str = "deck"
    Card: str = "card"

class MyNoteFields(AutoSlots):
    question: str = "Q"
    answer: str = "A"

class ImmersionKitSentenceNoteFields(AutoSlots):
    audio: str = "Audio Sentence"
    id: str = "ID"
    screenshot: str = "Screenshot"
    reading: str = "Reading"
    answer: str = "English"
    question: str = "Expression"

class SentenceNoteFields(AutoSlots):
    reading: str = "Reading"
    id: str = "ID"
    active_question: str = MyNoteFields.question
    source_question: str = "source_question"
    source_comments: str = "Comments"
    user_comments: str = "__comments"
    user_question: str = "__question"
    user_answer_analysis: str = "__answer_analysis"
    active_answer: str = MyNoteFields.answer
    source_answer: str = "source_answer"
    user_answer: str = "__answer"
    parsing_result: str = "__parsing_result"
    user_excluded_vocab: str = "__excluded_vocab"
    user_extra_vocab: str = "__extra_vocab"
    audio: str = "Audio Sentence"
    screenshot: str = "Screenshot"
    configuration: str = "__configuration"

class CardTypes(AutoSlots):
    reading: str = "Reading"
    listening: str = "Listening"

class NoteTypes(AutoSlots):
    immersion_kit: str = "Immersion Kit Sentence"
    Kanji: str = "_Kanji"
    Vocab: str = "_Vocab"
    Sentence: str = "_japanese_sentence"

    ALL: set[str] = {Kanji, Vocab, Sentence}

class NoteFields(AutoSlots):
    note_id: str = "nid"

    class VocabNoteType(AutoSlots):
        class Card(AutoSlots):
            Reading: str = CardTypes.reading
            Listening: str = CardTypes.listening

    class SentencesNoteType(AutoSlots):
        class Card(AutoSlots):
            Reading: str = CardTypes.reading
            Listening: str = CardTypes.listening

    class Kanji(AutoSlots):
        question: str = MyNoteFields.question
        active_answer: str = MyNoteFields.answer
        source_answer: str = "source_answer"
        user_answer: str = "__answer"
        Reading_On: str = "Reading_On"
        Reading_Kun: str = "Reading_Kun"
        Reading_Nan: str = "__reading_Nan"
        Radicals: str = "Radicals"
        Source_Meaning_Mnemonic: str = "Meaning_Mnemonic"
        Meaning_Info: str = "Meaning_Info"
        Reading_Mnemonic: str = "Reading_Mnemonic"
        Reading_Info: str = "Reading_Info"
        PrimaryVocab: str = "__primary_Vocab"
        Audio__: str = "__audio"

        user_mnemonic: str = "__mnemonic"
        user_similar_meaning: str = "__similar_meaning"
        related_confused_with: str = "__confused_with"

    class Vocab(AutoSlots):
        matching_rules: str = "__matching_rules"
        related_vocab: str = "__related_vocab"
        sentence_count: str = "sentence_count"
        question: str = MyNoteFields.question
        active_answer: str = MyNoteFields.answer
        source_answer: str = "source_answer"
        user_answer: str = "__answer"
        user_explanation: str = "__explanation"
        user_explanation_long: str = "__explanation_long"
        user_compounds: str = "__compounds"
        user_mnemonic: str = "__mnemonic"
        Reading: str = "Reading"
        parts_of_speech: str = "TOS"
        source_mnemonic: str = "source_mnemonic"
        Audio_b: str = "Audio_b"
        Audio_g: str = "Audio_g"
        Audio_TTS: str = "Audio_TTS"

        Forms: str = "F"
        source_reading_mnemonic: str = "source_reading_mnemonic"
        Homophones: str = "Homophones"
        ParsedTypeOfSpeech: str = "ParsedTypeOfSpeech"

f_root = "-::"
f_sentence: str = f"{f_root}sentence::"
f_kanji: str = f"{f_root}kanji::"
f_sentence_uses: str = f"{f_sentence}uses::"
f_vocab: str = f"{f_root}vocab::"
f_vocab_matching: str = f"{f_vocab}matching::"
f_vocab_matching_requires: str = f"{f_vocab_matching}requires::"
f_vocab_matching_forbids: str = f"{f_vocab_matching}forbids::"
f_vocab_matching_todo: str = f"{f_vocab_matching}todo::"
f_vocab_matching_uses: str = f"{f_vocab_matching}uses::"

class Tags(AutoSlots):
    class Sentence(AutoSlots):
        class Uses(AutoSlots):
            incorrect_matches: str = f"{f_sentence_uses}incorrect-matches"
            hidden_matches: str = f"{f_sentence_uses}hidden-matches"

    class Kanji(AutoSlots):
        is_radical: str = f"{f_kanji}is-radical"
        is_radical_purely: str = f"{f_kanji}is-radical-purely"
        is_radical_silent: str = f"{f_kanji}is-radical-silent"
        in_vocab_main_form: str = f"{f_kanji}in-vocab-main-form"
        in_any_vocab_form: str = f"{f_kanji}in-any-vocab-form"

        with_single_kanji_vocab: str = f"{f_kanji}single-kanji-vocab"
        with_single_kanji_vocab_with_different_reading: str = f"{f_kanji}single-kanji-vocab-with-different-reading"
        with_studying_single_kanji_vocab_with_different_reading: str = f"{f_kanji}studying-single-kanji-vocab-with-different-reading"
        with_no_primary_on_readings: str = f"{f_kanji}no-primary-on-readings"
        with_no_primary_readings: str = f"{f_kanji}no-primary-readings"
        with_studying_vocab: str = f"{f_kanji}studying-vocab"
        with_vocab_with_primary_on_reading: str = f"{f_kanji}has-vocab-with-primary-on-reading"
        with_studying_vocab_with_primary_on_reading: str = f"{f_kanji}studying-vocab-with-primary-on-reading"

        has_studying_vocab_with_no_matching_primary_reading: str = f"{f_kanji}has-studying-vocab-with-no-matching-primary-reading"
        has_studying_vocab_for_each_primary_reading: str = f"{f_kanji}has-studying-vocab-for-each-primary-reading"
        has_primary_reading_with_no_studying_vocab: str = f"{f_kanji}has-primary-reading-with-no-studying-vocab"
        has_non_primary_on_reading_vocab: str = f"{f_kanji}has-non-primary-on-reading-vocab"
        has_non_primary_on_reading_vocab_with_only_known_kanji: str = f"{f_kanji}has-non-primary-on-reading-vocab-with-only-known-kanji"

    class Vocab(AutoSlots):
        root: str = f_vocab
        has_no_studying_sentences: str = f"{f_vocab}has-no-studying-sentences"
        question_overrides_form: str = f"{f_vocab}question-overrides-form"

        class Matching(AutoSlots):
            yield_last_token_to_overlapping_compound: str = f"{f_vocab_matching}yield-last-token-to-upcoming-compound"
            is_poison_word: str = f"{f_vocab_matching}is-poison-word"
            is_inflecting_word: str = f"{f_vocab_matching}is-inflecting-word"

            class Requires(AutoSlots):
                a_stem: str = f"{f_vocab_matching_requires}a-stem"
                e_stem: str = f"{f_vocab_matching_requires}e-stem"
                past_tense_stem: str = f"{f_vocab_matching_requires}past-tense-stem"
                te_form_stem: str = f"{f_vocab_matching_requires}te-form-stem"
                sentence_end: str = f"{f_vocab_matching_requires}sentence-end"
                sentence_start: str = f"{f_vocab_matching_requires}sentence-start"
                exact_match: str = f"{f_vocab_matching_requires}exact-match"
                single_token: str = f"{f_vocab_matching_requires}single-token"
                compound: str = f"{f_vocab_matching_requires}compound"

            class Forbids(AutoSlots):
                a_stem: str = f"{f_vocab_matching_forbids}a-stem"
                e_stem: str = f"{f_vocab_matching_forbids}e-stem"
                past_tense_stem: str = f"{f_vocab_matching_forbids}past-tense-stem"
                te_form_stem: str = f"{f_vocab_matching_forbids}te-form-stem"
                sentence_end: str = f"{f_vocab_matching_forbids}sentence-end"
                sentence_start: str = f"{f_vocab_matching_forbids}sentence-start"
                exact_match: str = f"{f_vocab_matching_forbids}exact-match"
                auto_yielding: str = f"{f_vocab_matching_forbids}auto_yielding"

            class Todo(AutoSlots):
                with_preceding_vowel: str = f"{f_vocab_matching_todo}match-with-preceding-vowel"

            class Uses(AutoSlots):
                prefix_is_not: str = f"{f_vocab_matching_uses}prefix-is-not"
                suffix_is_not: str = f"{f_vocab_matching_uses}suffix-is-not"
                required_prefix: str = f"{f_vocab_matching_uses}required-prefix"
                surface_is_not: str = f"{f_vocab_matching_uses}surface-is-not"

    priority_folder: str = f"{f_root}priority::"

    source_folder: str = "source::"

    immersion_kit: str = f"{source_folder}immersion_kit"

    DisableKanaOnly: str = "_disable_uk"
    UsuallyKanaOnly: str = "_uk"
    TTSAudio: str = "_tts_audio"

class Mine(AutoSlots):
    app_name: str = "JA-Studio"
    app_still_loading_message: str = f"{app_name} still loading....."
    VocabPrefixSuffixMarker: str = "〜"
</file>

<file path="note/note_flush_guard.py">
from __future__ import annotations

from contextlib import contextmanager
from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from sysutils import ex_assert

if TYPE_CHECKING:
    from collections.abc import Iterator

    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

class NoteRecursiveFlushGuard(AutoSlots):
    def __init__(self, note: WeakRef[JPNote]) -> None:
        self._note: WeakRef[JPNote] = note
        self._depth:int = 0

    @contextmanager
    def pause_flushing(self) -> Iterator[None]:
        ex_assert.equal(self._depth, 0, "We don't support nested flushing since the complexities have not been figured out yet")
        self._depth += 1
        try: yield
        finally:
            self._depth -= 1

    def _should_flush(self) -> bool: return self._depth == 0

    @property
    def is_flushing(self) -> bool: return self._depth > 0

    def flush(self) -> None:
        if self._should_flush():
            with self.pause_flushing():
                self._note().backend_note.col.update_note(self._note().backend_note)
</file>

<file path="note/notefields/audio_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.notefields.mutable_string_field import MutableStringField

if TYPE_CHECKING:
    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

class AudioField(AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field_name: str) -> None:
        self._field: MutableStringField = MutableStringField(note, field_name)

    def has_audio(self) -> bool:
        field = self._field
        return field.value.strip().startswith("[sound:")

    def first_audiofile_path(self) -> str:
        return self.audio_files_paths()[0] if self.has_audio() else ""

    def raw_walue(self) -> str:
        field = self._field
        return field.value

    def audio_files_paths(self) -> list[str]:
        if not self.has_audio(): return []

        field = self._field
        stripped_paths = field.value.strip().replace("[sound:", "").split("]")
        return [path.strip() for path in stripped_paths]

    @override
    def __repr__(self) -> str: return self._field.__repr__()

class WritableAudioField(AudioField, AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field_name: str) -> None:
        super().__init__(note, field_name)

    def set_raw_value(self, value: str) -> None: self._field.set(value)
</file>

<file path="note/notefields/auto_save_wrappers/field_wrapper.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.notefields.auto_save_wrappers.value_wrapper import ValueWrapper
    from note.notefields.json_object_field import MutableSerializedObjectField


class FieldWrapper[TValue, TWrapper](AutoSlots):
    def __init__(self, field: MutableSerializedObjectField[TWrapper], value: ValueWrapper[TValue]) -> None:
        self._field: MutableSerializedObjectField[TWrapper] = field
        self._value: ValueWrapper[TValue] = value

    def set(self, value: TValue) -> None:
        self._value.set(value)
        self._field.save()

    def get(self) -> TValue:
        return self._value.get()

    @override
    def __repr__(self) -> str: return self._value.__repr__()
</file>

<file path="note/notefields/auto_save_wrappers/set_wrapper.py">
from __future__ import annotations

from typing import TYPE_CHECKING, Any, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from collections.abc import Callable

    from note.notefields.json_object_field import MutableSerializedObjectField

class FieldSetWrapper[TValue](AutoSlots):
    _secret: str = "aoeulrcaboeusthb"
    def __init__(self, save_callback: Callable[[], None], value: Callable[[], set[TValue]], secret: str) -> None:
        if FieldSetWrapper._secret != secret: raise ValueError("use the factory methods, not this private constructor")
        self._save: Callable[[], None] = save_callback
        self._value: Callable[[], set[TValue]] = value #never replace _value or the save method will stop working...

    def get(self) -> set[TValue]: return self._value()
    def __call__(self) -> set[TValue]: return self.get()

    def add(self, value: TValue) -> None:
        self._value().add(value)
        self._save()

    def remove(self, key: TValue) -> None:
        self._value().remove(key)
        self._save()

    def discard(self, key: TValue) -> None:
        self._value().discard(key)
        self._save()

    def clear(self) -> None: self._value().clear()

    def overwrite_with(self, other: FieldSetWrapper[TValue]) -> None:
        self._value().clear()
        self._value().update(other.get())
        self._save()

    def none(self) -> bool: return not self.any()
    def any(self) -> bool: return any(self._value())

    @classmethod
    def for_json_object_field(cls, field: MutableSerializedObjectField[Any], value: set[TValue]) -> FieldSetWrapper[TValue]:  # pyright: ignore[reportExplicitAny]
        return FieldSetWrapper(lambda: field.save(), lambda: value, FieldSetWrapper._secret)

    @override
    def __repr__(self) -> str: return self._value().__repr__() if self._value() else "{}"
</file>

<file path="note/notefields/auto_save_wrappers/value_wrapper.py">
from __future__ import annotations

from typing import override

from ex_autoslot import AutoSlots


class ValueWrapper[TValue](AutoSlots):
    def __init__(self, value: TValue) -> None:
        self._value: TValue = value

    def set(self, value: TValue) -> None:
        self._value = value

    def get(self) -> TValue:
        return self._value

    @override
    def __repr__(self) -> str: return self._value.__repr__()
</file>

<file path="note/notefields/comma_separated_strings_list_field_de_duplicated.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.notefields.comma_separated_strings_list_field import MutableCommaSeparatedStringsListField
from queryablecollections.q_iterable import query

if TYPE_CHECKING:
    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

class MutableCommaSeparatedStringsListFieldDeDuplicated(MutableCommaSeparatedStringsListField, AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field_name: str) -> None:
        super().__init__(note, field_name)

    @override
    def set(self, value: list[str]) -> None:
        super().set(query(value).distinct().to_list())
</file>

<file path="note/notefields/comma_separated_strings_list_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.notefields.mutable_string_field import MutableStringField
from sysutils import ex_str

if TYPE_CHECKING:
    from collections.abc import Callable

    from note.jpnote import JPNote
    from sysutils.lazy import Lazy
    from sysutils.weak_ref import WeakRef

class MutableCommaSeparatedStringsListField(AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field_name: str) -> None:
        field = MutableStringField(note, field_name)
        self._field: MutableStringField = field
        self._value: Lazy[list[str]] = self._field.lazy_reader(lambda: ex_str.extract_comma_separated_values(field.value))

    def get(self) -> list[str]:
        return self._value()

    def remove(self, remove: str) -> None:
        self.set([item for item in self.get() if item != remove])

    def set(self, value: list[str]) -> None:
        self._field.set(", ".join(value))

    def raw_string_value(self) -> str:
        field = self._field
        return field.value

    def set_raw_string_value(self, value: str) -> None:
        self.set(ex_str.extract_comma_separated_values(value))

    def add(self, add: str) -> None:
        self.set(self.get() + [add])

    def lazy_reader[TValue](self, reader: Callable[[], TValue]) -> Lazy[TValue]: return self._field.lazy_reader(reader)

    @override
    def __repr__(self) -> str: return ", ".join(self.get())
</file>

<file path="note/notefields/comma_separated_strings_set_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.notefields.comma_separated_strings_list_field import MutableCommaSeparatedStringsListField
from sysutils.lazy import Lazy

if TYPE_CHECKING:
    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

class MutableCommaSeparatedStringsSetField(AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field_name: str) -> None:
        self._field: MutableCommaSeparatedStringsListField = MutableCommaSeparatedStringsListField(note, field_name)
        field_with_no_reference_loop = self._field
        self._value: Lazy[set[str]] = Lazy(lambda: set(field_with_no_reference_loop.get()))

    def get(self) -> set[str]:
        return self._value()

    def set(self, value: set[str]) -> None:
        self._value = Lazy[set[str]].from_value(value)
        self._field.set(list(value))

    def remove(self, value: str) -> None:
        self.set(self.get() - {value})

    def raw_string_value(self) -> str:
        return self._field.raw_string_value()

    def set_raw_string_value(self, value: str) -> None:
        self._field.set_raw_string_value(value)

    @override
    def __repr__(self) -> str: return self.raw_string_value()
</file>

<file path="note/notefields/fallback_string_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from sysutils.weak_ref import WeakRefable

if TYPE_CHECKING:
    from note.notefields.mutable_string_field import MutableStringField

class FallbackStringField(WeakRefable, AutoSlots):
    def __init__(self, primary_field: MutableStringField, fallback_field: MutableStringField) -> None:
        self._field: MutableStringField = primary_field
        self._fallback_field: MutableStringField = fallback_field

    def get(self) -> str:
        return self._field.value or self._fallback_field.value
</file>

<file path="note/notefields/integer_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from note.notefields.mutable_string_field import MutableStringField

if TYPE_CHECKING:
    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

class IntegerField(AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field_name: str) -> None:
        self._note: WeakRef[JPNote] = note
        self._field: MutableStringField = MutableStringField(note, field_name)

    # noinspection PyUnusedFunction
    def get(self) -> int:
        field = self._field
        return int(field.value) if self._field.has_value() else 0

    def set(self, value: int) -> None:
        self._field.set(str(value))
</file>

<file path="note/notefields/json_object_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.notefields.mutable_string_field import MutableStringField
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from note.jpnote import JPNote

class ObjectSerializer[T](AutoSlots):
    def serialize(self, instance: T) -> str: raise NotImplementedError()  # pyright: ignore
    def deserialize(self, serialized: str) -> T: raise NotImplementedError()  # pyright: ignore

class MutableSerializedObjectField[T](WeakRefable, AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field: str, serializer: ObjectSerializer[T]) -> None:
        self._note: WeakRef[JPNote] = note
        self._field: MutableStringField = MutableStringField(note, field)
        self._serializer: ObjectSerializer[T] = serializer
        weakrefthis = WeakRef(self)
        string_field = weakrefthis()._field
        self._value: Lazy[T] = Lazy(lambda: serializer.deserialize(string_field.value))

    def get(self) -> T: return self._value()
    def set(self, value: T) -> None:
        self._value = Lazy[T].from_value(value)
        self._field.set(self._serializer.serialize(value))

    def save(self) -> None:
        self._field.set(self._serializer.serialize(self._value()))

    @override
    def __repr__(self) -> str: return self.get().__repr__()
</file>

<file path="note/notefields/mutable_string_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from sysutils.lazy import Lazy

if TYPE_CHECKING:
    from collections.abc import Callable

    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

# this field is used extremely much, so its design is crucial for both performance and memory usage, keep in mind when changing anything
# if a field is read-only, make sure to to use ReadOnlyStringField instead, which uses less memory
class MutableStringField(AutoSlots):
    def __init__(self, note: WeakRef[JPNote], field_name: str) -> None:
        self._note: WeakRef[JPNote] = note
        self._field_name: str = field_name
        self._reset_callbacks: list[Callable[[], None]] | None = None
        self._value: str = self._string_field_get_initial_value_for_caching()

    # this method is interesting for profiling so we want a unuique name we can find in the trace
    def _string_field_get_initial_value_for_caching(self) -> str: return self._note().get_field(self._field_name)

    @property
    def value(self) -> str: return self._value

    def set(self, value: str) -> None:
        new_value = value.strip()
        if new_value != self._value:
            self._note().set_field(self._field_name, value.strip())
            self._value = new_value
            if self._reset_callbacks is not None:
                for callback in self._reset_callbacks: callback()

    def has_value(self) -> bool: return self.value != ""
    def empty(self) -> None: self.set("")

    def lazy_reader[TValue](self, reader: Callable[[], TValue]) -> Lazy[TValue]:
        lazy = Lazy(reader)
        self.on_change(lazy.reset)
        return lazy

    def on_change(self, callback: Callable[[], None]) -> None:
        if self._reset_callbacks is None:
            self._reset_callbacks = []
        self._reset_callbacks.append(callback)

    @override
    def __repr__(self) -> str: return self.value
</file>

<file path="note/notefields/require_forbid_flag_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

class RequireForbidFlagField(AutoSlots):
    def __init__(self, note: WeakRef[JPNote], required_tag: str, forbidden_tag: str) -> None:
        self._note: WeakRef[JPNote] = note
        self._required_tag: str = required_tag
        self._forbidden_tag: str = forbidden_tag
        self._is_required: bool = note().has_tag(required_tag)
        self._is_forbidden: bool = note().has_tag(forbidden_tag)
        note().on_tag_updated(required_tag, self._on_tag_updated)
        note().on_tag_updated(forbidden_tag, self._on_tag_updated)

    def _on_tag_updated(self) -> None:
        self._is_required = self._note().has_tag(self._required_tag)
        self._is_forbidden = self._note().has_tag(self._forbidden_tag)

    @property
    def is_configured_required(self) -> bool: return self._is_required
    @property
    def is_configured_forbidden(self) -> bool: return self._is_forbidden

    @property
    def is_required(self) -> bool: return self.is_configured_required
    @property
    def is_forbidden(self) -> bool: return self.is_configured_forbidden

    def set_forbidden(self, value: bool) -> None:
        self.set_required(not value)

    def set_required(self, value: bool) -> None:
        if value:
            self._note().set_tag(self._required_tag)
            self._note().remove_tag(self._forbidden_tag)
        else:
            self._note().set_tag(self._forbidden_tag)
            self._note().remove_tag(self._required_tag)

    @override
    def __repr__(self) -> str: return f"""{self._required_tag.replace("requires::", "")} required: {self.is_required}, forbidden: {self.is_forbidden}"""
</file>

<file path="note/notefields/sentence_question_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from sysutils import ex_str
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from note.notefields.mutable_string_field import MutableStringField

class SentenceQuestionField(WeakRefable, AutoSlots):
    word_break_tag: str = "<wbr>"
    def __init__(self, primary_field: MutableStringField, fallback_field: MutableStringField) -> None:
        self._field: MutableStringField = primary_field
        self._fallback_field: MutableStringField = fallback_field
        weakself = WeakRef(self)
        self._value: Lazy[str] = Lazy(lambda: ex_str.strip_html_markup(weakself()._sentence_question_field_raw_value().replace(self.word_break_tag, ex_str.invisible_space)))
        self._field.on_change(self._value.reset)
        self._fallback_field.on_change(self._value.reset)

    def _sentence_question_field_raw_value(self) -> str: return self._field.value or self._fallback_field.value

    def get(self) -> str: return self._value()

    def split_token_with_word_break_tag(self, section: str) -> None:
        if len(section) < 2: return
        raw_value = self._sentence_question_field_raw_value()
        new_section = f"{section[0]}{self.word_break_tag}{section[1:]}"
        new_value = raw_value.replace(section, new_section)
        self._field.set(new_value)
</file>

<file path="note/notefields/strip_html_on_read_fallback_string_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from note.notefields.fallback_string_field import FallbackStringField
from sysutils import ex_str

if TYPE_CHECKING:
    from note.notefields.mutable_string_field import MutableStringField

class StripHtmlOnReadFallbackStringField(AutoSlots):
    def __init__(self, primary_field: MutableStringField, fallback_field: MutableStringField) -> None:
        self._field: FallbackStringField = FallbackStringField(primary_field, fallback_field)

    def get(self) -> str: return ex_str.strip_html_markup(self._field.get().replace("<wbr>", ex_str.invisible_space))
</file>

<file path="note/notefields/tag_flag_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.jpnote import JPNote
    from sysutils.weak_ref import WeakRef

class TagFlagField(AutoSlots):
    def __init__(self, note: WeakRef[JPNote], tag: str) -> None:
        self._note: WeakRef[JPNote] = note
        self.tag: str = tag
        self._value: bool = self._note().has_tag(self.tag)
        note().on_tag_updated(self.tag, self._on_tag_updated)

    def _on_tag_updated(self) -> None:
        self._value = self._note().has_tag(self.tag)

    def is_set(self) -> bool: return self._value

    def set_to(self, set_: bool) -> None:
        if set_: self._note().set_tag(self.tag)
        else: self._note().remove_tag(self.tag)
        self._value = set_

    @override
    def __repr__(self) -> str: return f"""{self.tag}: {self.is_set()}"""
</file>

<file path="note/noteutils.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from anki.consts import QUEUE_TYPE_SUSPENDED
from anki.notes import NoteId
from note.note_constants import NoteTypes
from sysutils import typed
from sysutils.typed import non_optional, str_

if TYPE_CHECKING:
    from anki.cards import Card
    from anki.collection import Collection
    from anki.dbproxy import Row
    from anki.notes import Note
    from qt_utils.task_runner_progress_dialog import ITaskRunner

_studying_status_cache: dict[NoteId, dict[str, bool]] = {}

def _is_being_studied(card: Card) -> bool:
    return card.queue != QUEUE_TYPE_SUSPENDED  # and card.queue != QUEUE_TYPE_NEW

def _card_type(card: Card) -> str:
    return str_(card.template()["name"])

def remove_from_studying_cache(note_id: NoteId) -> None:
    if note_id in _studying_status_cache:
        _studying_status_cache.pop(note_id)

def clear_studying_cache() -> None:
    _studying_status_cache.clear()

def has_card_being_studied_cached(note: Note, card_type: str = "") -> bool:
    _ensure_card_status_is_cached(note)

    if card_type:
        cached = _studying_status_cache[note.id]
        return cached.get(card_type, False)

    return any(_studying_status_cache[note.id].values())

def _ensure_card_status_is_cached(note: Note) -> None:
    if note.id not in _studying_status_cache:
        _studying_status_cache[note.id] = {_card_type(card): _is_being_studied(card) for card in note.cards()}

def initialize_studying_cache(col: Collection, task_runner: ITaskRunner) -> None:
    query = f"""
    SELECT cards.nid as note_id, templates.name as card_type, cards.queue as queue
    FROM cards
    JOIN notes ON cards.nid = notes.id
    JOIN notetypes on notetypes.id = notes.mid
    JOIN templates ON templates.ntid = notes.mid and templates.ord = cards.ord
    WHERE notetypes.name COLLATE NOCASE IN ('{NoteTypes.Sentence}', '{NoteTypes.Vocab}', '{NoteTypes.Kanji}')
"""

    studying_status_rows: list[Row] = task_runner.run_on_background_thread_with_spinning_progress_dialog("Fetching card studying status from Anki db", lambda: non_optional(col.db).all(query))

    clear_studying_cache()

    def cache_card(row: Row) -> None:
        note_id = NoteId(typed.int_(row[0])) # pyright: ignore[reportAny]
        card_type = typed.str_(row[1])  # pyright: ignore[reportAny]
        queue = typed.int_(row[2])  # pyright: ignore[reportAny]
        if note_id not in _studying_status_cache: _studying_status_cache[note_id] = {}
        _studying_status_cache[note_id][card_type] = queue != QUEUE_TYPE_SUSPENDED

    task_runner.process_with_progress(studying_status_rows, cache_card, "Populating studying status cache")
</file>

<file path="note/queue_manager.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from aqt import dialogs, mw
from aqt.browser import Browser  # pyright: ignore[reportPrivateImportUsage]
from note.cardutils import CardUtils
from sysutils import typed

if TYPE_CHECKING:
    from collections.abc import Sequence

    from anki.cards import CardId

def refresh_search() -> None:
    browser: Browser = typed.checked_cast(Browser, dialogs.open("Browser", mw))  # pyright: ignore[reportAny]
    browser.onSearchActivated()  # pyright: ignore[reportUnknownMemberType]

def prioritize_selected_cards(card_ids: Sequence[CardId]) -> None:
    cards = [app.anki_collection().get_card(card_id) for card_id in card_ids]
    for card in cards:
        CardUtils.prioritize(card)

    refresh_search()
</file>

<file path="note/sentences/caching_sentence_configuration_field.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from ex_autoslot import AutoSlots
from note.note_constants import SentenceNoteFields
from note.notefields.mutable_string_field import MutableStringField
from note.sentences.sentence_configuration import SentenceConfiguration
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from note.sentences.sentencenote import SentenceNote
    from note.sentences.word_exclusion_set import WordExclusionSet
    from note.vocabulary.vocabnote import VocabNote

class CachingSentenceConfigurationField(WeakRefable, AutoSlots):
    def __init__(self, sentence: WeakRef[SentenceNote]) -> None:
        self._sentence: WeakRef[SentenceNote] = sentence
        self.field: MutableStringField = MutableStringField(sentence, SentenceNoteFields.configuration)

        weakrefthis = WeakRef(self)
        string_field = weakrefthis().field
        self._value: Lazy[SentenceConfiguration] = Lazy(lambda: SentenceConfiguration.serializer.deserialize(string_field.value, weakrefthis()._save))

    @property
    def configuration(self) -> SentenceConfiguration:
        return self._value()

    @property
    def incorrect_matches(self) -> WordExclusionSet: return self._value().incorrect_matches

    @property
    def hidden_matches(self) -> WordExclusionSet: return self._value().hidden_matches

    def highlighted_words(self) -> set[str]: return self._value().highlighted_words

    @property
    def highlighted_vocab(self) -> set[VocabNote]:
        return {vocab for vocab in (app.col().vocab.with_any_form_in(list(self.highlighted_words())))
                if vocab.get_id() in self._sentence().parsing_result.get().matched_vocab_ids}

    def remove_highlighted_word(self, word: str) -> None:
        self.highlighted_words().discard(word)
        self._save()

    def reset_highlighted_words(self) -> None:
        self._value().highlighted_words.clear()
        self._save()

    def add_highlighted_word(self, vocab: str) -> None:
        self.highlighted_words().add(vocab.strip())
        self._save()

    def _save(self) -> None:
        self.field.set(SentenceConfiguration.serializer.serialize(self._value()))
        self._sentence().update_parsed_words(force=True)
</file>

<file path="note/sentences/parsed_word.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from anki.notes import NoteId
from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch
from note.sentences.serialization.parsed_word_serializer import ParsedWordSerializer

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match

class ParsedMatch(AutoSlots):
    serializer: ParsedWordSerializer = ParsedWordSerializer()
    def __init__(self, variant: str, start_index: int, is_displayed: bool, word: str, information_string: str, vocab_id: NoteId) -> None:
        self.start_index: int = start_index
        self.is_displayed: bool = is_displayed
        self.variant: str = variant
        self.parsed_form: str = word
        self.vocab_id: NoteId = vocab_id
        self.information_string: str = information_string

    @property
    def end_index(self) -> int: return self.start_index + len(self.parsed_form)

    @classmethod
    def from_match(cls, match: Match) -> ParsedMatch:
        return ParsedMatch("S" if match.variant.is_surface else "B",
                           match.start_index,
                           match.is_displayed,
                           match.parsed_form,
                           f"""{" ".join(match.failure_reasons)}{" # " if match.failure_reasons else ""}{" ".join(match.hiding_reasons)} {"highlighted" if match.is_highlighted else ""}""",
                           match.vocab.get_id() if isinstance(match, VocabMatch) else NoteId(-1))

    @override
    def __repr__(self) -> str: return self.serializer.to_row(self)
</file>

<file path="note/sentences/parsing_result.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from note.sentences.parsed_word import ParsedMatch
from note.sentences.serialization.parsing_result_serializer import ParsingResultSerializer

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis

class ParsingResult(AutoSlots):
    serializer: ParsingResultSerializer = ParsingResultSerializer()
    def __init__(self, words: list[ParsedMatch], sentence: str, parser_version: str) -> None:
        self.parsed_words: list[ParsedMatch] = words
        self.sentence: str = sentence
        self.parser_version: str = parser_version
        self.matched_vocab_ids: set[int] = {parsed.vocab_id for parsed in self.parsed_words if parsed.vocab_id != -1}

    def parsed_words_strings(self) -> list[str]: return [parsed.parsed_form for parsed in self.parsed_words]

    @classmethod
    def from_analysis(cls, analysis: TextAnalysis) -> ParsingResult:
        return ParsingResult([ParsedMatch.from_match(match) for match in analysis.valid_word_variant_valid_matches],
                             analysis.text,
                             analysis.version)
</file>

<file path="note/sentences/sentence_configuration.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.sentences.serialization.sentence_configuration_serializer import SentenceConfigurationSerializer
from note.sentences.word_exclusion_set import WordExclusionSet
from sysutils.debug_repr_builder import SkipFalsyValuesDebugReprBuilder

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.word_exclusion import WordExclusion

class SentenceConfiguration(AutoSlots):
    serializer: SentenceConfigurationSerializer = SentenceConfigurationSerializer()

    def __init__(self, highlighted_words: set[str], incorrect_matches: WordExclusionSet, hidden_matches: WordExclusionSet) -> None:
        self.highlighted_words: set[str] = highlighted_words
        self.incorrect_matches: WordExclusionSet = incorrect_matches
        self.hidden_matches: WordExclusionSet = hidden_matches

    @classmethod
    def from_incorrect_matches(cls, incorrect_matches: list[WordExclusion]) -> SentenceConfiguration:
        return cls.from_values(set(), incorrect_matches, [])

    @classmethod
    def from_hidden_matches(cls, incorrect_matches: list[WordExclusion]) -> SentenceConfiguration:
        return cls.from_values(set(), [], incorrect_matches)

    @classmethod
    def from_values(cls, highlighted: set[str], incorrect_matches: list[WordExclusion], hidden_matches: list[WordExclusion]) -> SentenceConfiguration:
        return cls(highlighted,
                   WordExclusionSet(lambda: None, incorrect_matches),
                   WordExclusionSet(lambda: None, hidden_matches))

    @classmethod
    def empty(cls) -> SentenceConfiguration: return cls.from_values(set(), [], [])

    @override
    def __repr__(self) -> str: return (SkipFalsyValuesDebugReprBuilder()
                                       .prop("highlighted_words", self.highlighted_words)
                                       .prop("incorrect_matches", self.incorrect_matches)
                                       .prop("hidden_matches", self.hidden_matches)
                                       .repr)
</file>

<file path="note/sentences/sentencenote.py">
from __future__ import annotations

from typing import TYPE_CHECKING, cast, override

from anki.notes import Note
from ankiutils import app
from ex_autoslot import AutoSlots
from note.jpnote import JPNote
from note.note_constants import ImmersionKitSentenceNoteFields, NoteFields, NoteTypes, SentenceNoteFields, Tags
from note.notefields.audio_field import WritableAudioField
from note.notefields.json_object_field import MutableSerializedObjectField
from note.notefields.mutable_string_field import MutableStringField
from note.notefields.sentence_question_field import SentenceQuestionField
from note.notefields.strip_html_on_read_fallback_string_field import StripHtmlOnReadFallbackStringField
from note.sentences.caching_sentence_configuration_field import CachingSentenceConfigurationField
from note.sentences.parsing_result import ParsingResult
from note.sentences.serialization.parsing_result_serializer import ParsingResultSerializer
from note.sentences.user_fields import SentenceUserFields
from sysutils import ex_str, kana_utils
from sysutils.weak_ref import WeakRef

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis
    from note.vocabulary.vocabnote import VocabNote
    from queryablecollections.collections.q_list import QList

class SentenceNote(JPNote, AutoSlots):
    def __init__(self, note: Note) -> None:
        super().__init__(note)
        self.weakref_sentence: WeakRef[SentenceNote] = cast(WeakRef[SentenceNote], self.weakref)

        self.id: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.id)
        self.reading: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.reading)

        self.active_question: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.active_question)
        self.active_answer: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.active_answer)

        self._source_answer: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.source_answer)
        self.source_question: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.source_question)
        self.source_comments: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.source_comments)

        self.user: SentenceUserFields = SentenceUserFields(self.weakref_sentence)

        self.question: SentenceQuestionField = SentenceQuestionField(self.user.question, self.source_question)
        self.answer: StripHtmlOnReadFallbackStringField = StripHtmlOnReadFallbackStringField(self.user.answer, self._source_answer)
        self._screenshot: MutableStringField = MutableStringField(self.weakref, SentenceNoteFields.screenshot)
        self.audio: WritableAudioField = WritableAudioField(self.weakref, SentenceNoteFields.audio)
        self.configuration: CachingSentenceConfigurationField = CachingSentenceConfigurationField(self.weakref_sentence)
        self.parsing_result: MutableSerializedObjectField[ParsingResult] = MutableSerializedObjectField[ParsingResult](self.weakref, SentenceNoteFields.parsing_result, ParsingResultSerializer())

    @override
    def get_question(self) -> str: return self.question.get()
    @override
    def get_answer(self) -> str: return self.answer.get()

    def is_studying_read(self) -> bool: return self.is_studying(NoteFields.SentencesNoteType.Card.Reading)
    def is_studying_listening(self) -> bool: return self.is_studying(NoteFields.SentencesNoteType.Card.Listening)

    def get_valid_parsed_non_child_words_strings(self) -> QList[str]:
        return self.get_valid_parsed_non_child_words().select(lambda word: word.form).to_list()  #[w.form for w in self.get_valid_parsed_non_child_words()]

    def get_valid_parsed_non_child_words(self) -> QList[CandidateWordVariant]:
        return self.create_analysis().display_word_variants

    def create_analysis(self) -> TextAnalysis:
        from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis
        return TextAnalysis(self.get_question(), self.configuration.configuration)

    @override
    def get_direct_dependencies(self) -> set[JPNote]:
        highlighted = self.configuration.highlighted_vocab
        displayed_vocab = {voc for voc in {app.col().vocab.with_id_or_none(voc.vocab_id)
                                           for voc in self.parsing_result.get().parsed_words
                                           if voc.is_displayed}
                           if voc is not None}
        kanji = set(self.collection.kanji.with_any_kanji_in(self.extract_kanji()))
        return set(highlighted | displayed_vocab | kanji)

    def get_words(self) -> set[str]: return (set(self.parsing_result.get().parsed_words_strings()) | set(self.configuration.highlighted_words())) - self.configuration.incorrect_matches.words()

    def get_parsed_words_notes(self) -> list[VocabNote]:
        return (self.get_valid_parsed_non_child_words_strings()
                .select_many(app.col().vocab.with_question).to_list()) #ex_sequence.flatten([app.col().vocab.with_question(q) for q in self.get_valid_parsed_non_child_words_strings()])

    @override
    def update_generated_data(self) -> None:
        super().update_generated_data()
        self.update_parsed_words()
        self.active_answer.set(self.get_answer())
        self.active_question.set(self.get_question())

    def update_parsed_words(self, force: bool = False) -> None:
        from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis
        parsing_result = self.parsing_result.get()
        if not force and parsing_result and parsing_result.sentence == self.get_question() and parsing_result.parser_version == TextAnalysis.version:
            return

        analysis = self.create_analysis()
        self.parsing_result.set(ParsingResult.from_analysis(analysis))

    def extract_kanji(self) -> list[str]:
        clean = ex_str.strip_html_and_bracket_markup(self.get_question())
        return [char for char in clean if kana_utils.character_is_kanji(char)]

    @classmethod
    def create_test_note(cls, question: str, answer: str) -> SentenceNote:
        inner_note = Note(app.anki_collection(), app.anki_collection().models.by_name(NoteTypes.Sentence))
        note = SentenceNote(inner_note)
        note.source_question.set(question)
        note.user.answer.set(answer)
        note.update_generated_data()
        app.col().sentences.add(note)
        return note

    @classmethod
    def add_sentence(cls, question: str, answer: str, audio: str = "", screenshot: str = "", highlighted_vocab: set[str] | None = None, tags: set[str] | None = None) -> SentenceNote:
        inner_note = Note(app.anki_collection(), app.anki_collection().models.by_name(NoteTypes.Sentence))
        note = SentenceNote(inner_note)
        note.source_question.set(question)
        note._source_answer.set(answer)
        note._screenshot.set(screenshot)
        note.update_generated_data()

        if not audio.strip():
            note.set_tag(Tags.TTSAudio)
        else:
            audio1 = audio.strip()
            note.audio.set_raw_value(audio1)

        if highlighted_vocab:
            for vocab in highlighted_vocab:
                note.configuration.add_highlighted_word(vocab)

        if tags:
            for tag in tags:
                note.set_tag(tag)

        app.col().sentences.add(note)
        return note

    @classmethod
    def import_immersion_kit_sentence(cls, immersion_kit_note: Note) -> SentenceNote:
        created = cls.add_sentence(question=immersion_kit_note[ImmersionKitSentenceNoteFields.question],
                                   answer=immersion_kit_note[ImmersionKitSentenceNoteFields.answer],
                                   audio=immersion_kit_note[ImmersionKitSentenceNoteFields.audio],
                                   screenshot=immersion_kit_note[ImmersionKitSentenceNoteFields.screenshot],
                                   tags={Tags.immersion_kit})

        created.id.set(immersion_kit_note[ImmersionKitSentenceNoteFields.id])
        created.reading.set(immersion_kit_note[ImmersionKitSentenceNoteFields.reading])

        return created

    @classmethod
    def create(cls, question: str) -> SentenceNote:
        inner_note = Note(app.anki_collection(), app.anki_collection().models.by_name(NoteTypes.Sentence))
        note = SentenceNote(inner_note)
        note.source_question.set(question)
        note.update_generated_data()
        app.col().sentences.add(note)
        return note
</file>

<file path="note/sentences/serialization/parsed_word_serializer.py">
from __future__ import annotations

from typing import TYPE_CHECKING, cast

from anki.notes import NoteId
from ex_autoslot import AutoSlots
from sysutils.ex_str import invisible_space

if TYPE_CHECKING:
    from note.sentences.parsed_word import ParsedMatch

class ParsedWordSerializer(AutoSlots):
    separator: str = f" {invisible_space} "

    @staticmethod
    def to_row(parsed_word: ParsedMatch) -> str: return ParsedWordSerializer.separator.join([
        parsed_word.variant,  # 0
        str(parsed_word.start_index),  # 1
        str(1 if parsed_word.is_displayed else 0),  # 2
        parsed_word.parsed_form,  # 3
        parsed_word.information_string or "#",  # 4
        str(parsed_word.vocab_id),  # 5
    ])

    @staticmethod
    def from_row(serialized: str) -> ParsedMatch:
        from note.sentences.parsed_word import ParsedMatch
        values = serialized.split(ParsedWordSerializer.separator)

        return ParsedMatch(values[0],
                           int(values[1]),
                           values[2] != "0",
                           values[3],
                           values[4],
                           cast(NoteId, int(values[5])))
</file>

<file path="note/sentences/serialization/parsing_result_serializer.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

import mylog
from ex_autoslot import AutoSlots
from note.notefields.json_object_field import ObjectSerializer
from note.sentences.parsed_word import ParsedMatch
from sysutils.ex_str import invisible_space, newline

if TYPE_CHECKING:
    from note.sentences.parsing_result import ParsingResult

class ParsingResultSerializer(ObjectSerializer["ParsingResult"], AutoSlots):
    newline_replacement: str = f"NEWLINE{invisible_space}"
    @override
    def deserialize(self, serialized: str) -> ParsingResult:
        from note.sentences.parsing_result import ParsingResult

        rows = serialized.split(newline)
        if len(rows) < 2: return ParsingResult([], "", "")

        # noinspection PyBroadException
        try:
            return ParsingResult([ParsedMatch.serializer.from_row(row) for row in rows[2:]],
                                 self._restore_newline(rows[1]),
                                 rows[0]) if serialized else ParsingResult([], "", "")
        except Exception as ex:
            mylog.warning(f"""Failed to deserialize ParsingResult:
            message:
{ex}
{serialized}""")
            return ParsingResult([], "", "")

    def _replace_newline(self, value: str) -> str:
        return value.replace(newline, self.newline_replacement)

    def _restore_newline(self, serialized_value: str) -> str:
        return serialized_value.replace(self.newline_replacement, newline)

    @override
    def serialize(self, instance: ParsingResult) -> str:
        return newline.join([instance.parser_version,
                             self._replace_newline(instance.sentence)]
                            + [ParsedMatch.serializer.to_row(word) for word in instance.parsed_words])
</file>

<file path="note/sentences/serialization/sentence_configuration_serializer.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.word_exclusion import WordExclusion
from note.sentences.word_exclusion_set import WordExclusionSet
from sysutils.json import ex_json
from sysutils.json.json_reader import JsonReader

if TYPE_CHECKING:
    from collections.abc import Callable

    from note.sentences.sentence_configuration import SentenceConfiguration

class SentenceConfigurationSerializer(AutoSlots):
    @staticmethod
    def deserialize(json: str, save_callback: Callable[[], None]) -> SentenceConfiguration:
        from note.sentences.sentence_configuration import SentenceConfiguration
        if not json: return SentenceConfiguration(set(), WordExclusionSet(save_callback, []), WordExclusionSet(save_callback, []))

        reader = JsonReader.from_json(json)
        return SentenceConfiguration(reader.string_set("highlighted_words"),
                                     WordExclusionSet(save_callback, reader.object_list("incorrect_matches", WordExclusion.from_reader)),
                                     WordExclusionSet(save_callback, reader.object_list("hidden_matches", WordExclusion.from_reader)))

    @staticmethod
    def serialize(config: SentenceConfiguration) -> str:
        return ex_json.dict_to_json({"highlighted_words": list(config.highlighted_words),
                                     "incorrect_matches": [exclusion.to_dict() for exclusion in config.incorrect_matches.get()],
                                     "hidden_matches": [exclusion.to_dict() for exclusion in config.hidden_matches.get()]})
</file>

<file path="note/sentences/user_fields.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from note.note_constants import SentenceNoteFields
from note.notefields.mutable_string_field import MutableStringField

if TYPE_CHECKING:
    from note.sentences.sentencenote import SentenceNote
    from sysutils.weak_ref import WeakRef

class SentenceUserFields(AutoSlots):
    def __init__(self, sentence: WeakRef[SentenceNote]) -> None:
        self._sentence: WeakRef[SentenceNote] = sentence
        self.comments: MutableStringField = MutableStringField(sentence, SentenceNoteFields.user_comments)
        self.question: MutableStringField = MutableStringField(sentence, SentenceNoteFields.user_question)
        self.answer: MutableStringField = MutableStringField(sentence, SentenceNoteFields.user_answer)
        self.answer_analysis: MutableStringField = MutableStringField(sentence, SentenceNoteFields.user_answer_analysis)
</file>

<file path="note/sentences/word_exclusion_set.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.word_exclusion import WordExclusion

if TYPE_CHECKING:
    from collections.abc import Callable

class WordExclusionSet(AutoSlots):
    def __init__(self, save_callback: Callable[[], None], exclusions: list[WordExclusion]) -> None:
        self._save: Callable[[], None] = save_callback
        self._exclusions: set[WordExclusion] = set(exclusions)
        self._excluded_words: set[str] = self._extract_words()

    def get(self) -> set[WordExclusion]: return self._exclusions

    def _extract_words(self) -> set[str]:
        return {exclusion.word for exclusion in self._exclusions}

    def words(self) -> set[str]:
        return self._excluded_words

    def reset(self) -> None:
        self._exclusions = set()
        self._excluded_words = set()
        self._save()

    def add_global(self, vocab: str) -> None:
        self.add(WordExclusion.global_(vocab))

    def add(self, exclusion: WordExclusion) -> None:
        self._exclusions.add(exclusion)
        self._excluded_words = self._extract_words()
        self._save()

    def remove(self, exclusion: WordExclusion) -> None:
        self._exclusions.remove(exclusion)
        self._excluded_words = self._extract_words()
        self._save()

    def remove_string(self, to_remove: str) -> None:
        for exclusion in [ex for ex in self._exclusions if ex.word == to_remove]:
            self._exclusions.remove(exclusion)
        self._excluded_words = self._extract_words()
        self._save()

    def excludes_at_index(self, word: str, index: int) -> bool:
        return any(exclusion for exclusion in self._exclusions if exclusion.excludes_form_at_index(word, index))

    @override
    def __repr__(self) -> str: return ", ".join(exclusion.__repr__() for exclusion in self._exclusions)
</file>

<file path="note/vocabnote_cloner.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from anki.notes import Note
from ankiutils import anki_module_import_issues_fix_just_import_this_module_before_any_other_anki_modules, app  # noqa  # pyright: ignore[reportUnusedImport]
from ex_autoslot import AutoSlots
from language_services import conjugator
from note.note_constants import NoteTypes, Tags

if TYPE_CHECKING:
    from collections.abc import Callable

    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef

class VocabCloner(AutoSlots):
    def __init__(self, note: WeakRef[VocabNote]) -> None:
        self._note_ref: WeakRef[VocabNote] = note

    @property
    def note(self) -> VocabNote: return self._note_ref()

    def create_prefix_version(self, prefix: str, speech_type: str = "expression", set_compounds: bool = True, truncate_characters: int = 0) -> VocabNote:
        return self._create_postfix_prefix_version(prefix, speech_type, is_prefix=True, set_compounds=set_compounds, truncate_characters=truncate_characters)

    def create_suffix_version(self, suffix: str, speech_type: str = "expression", set_compounds: bool = True, truncate_characters: int = 0) -> VocabNote:
        return self._create_postfix_prefix_version(suffix, speech_type, set_compounds=set_compounds, truncate_characters=truncate_characters)

    def _create_postfix_prefix_version(self, addendum: str, speech_type: str, is_prefix: bool = False, set_compounds: bool = True, truncate_characters: int = 0) -> VocabNote:
        def append_prepend_addendum(base: str) -> str:
            if not is_prefix:
                return base + addendum if truncate_characters == 0 else base[0:-truncate_characters] + addendum
            return addendum + base if truncate_characters == 0 else base[truncate_characters:] + addendum

        vocab_note = self.note
        new_vocab = self._create_new_vocab_with_some_data_copied(question=append_prepend_addendum(self.note.get_question()),
                                                                 answer=self.note.get_answer(),
                                                                 readings=[append_prepend_addendum(reading) for reading in vocab_note.readings.get()])

        if set_compounds:
            if not is_prefix:
                compounds = [self.note.get_question(), addendum]
                new_vocab.compound_parts.set(compounds)
            else:
                compounds1 = [addendum, self.note.get_question()]
                new_vocab.compound_parts.set(compounds1)

        new_vocab.parts_of_speech.set_raw_string_value(speech_type)
        new_vocab.forms.set_list([append_prepend_addendum(form) for form in self.note.forms.all_set()])
        return new_vocab

    def create_na_adjective(self) -> VocabNote:
        return self._create_postfix_prefix_version("な", "na-adjective")

    def create_no_adjective(self) -> VocabNote:
        return self._create_postfix_prefix_version("の", "expression, no-adjective")

    def create_ni_adverb(self) -> VocabNote:
        return self._create_postfix_prefix_version("に", "adverb")

    def create_to_adverb(self) -> VocabNote:
        return self._create_postfix_prefix_version("と", "to-adverb")

    def create_te_prefixed_word(self) -> VocabNote:
        return self._create_postfix_prefix_version("て", "auxiliary", is_prefix=True)

    def create_o_prefixed_word(self) -> VocabNote:
        note = self.note
        return self._create_postfix_prefix_version("お", note.parts_of_speech.raw_string_value(), is_prefix=True)

    def create_n_suffixed_word(self) -> VocabNote:
        return self._create_postfix_prefix_version("ん", "expression")

    def create_ka_suffixed_word(self) -> VocabNote:
        return self._create_postfix_prefix_version("か", "expression")

    def create_suru_verb(self, shimasu: bool = False) -> VocabNote:
        suru_verb = self._create_postfix_prefix_version("する" if not shimasu else "します", "suru verb")

        forms = list(suru_verb.forms.all_set()) + [form.replace("する", "をする") for form in suru_verb.forms.all_set()]
        suru_verb.forms.set_list(forms)

        note = self.note
        if note.parts_of_speech.is_transitive():
            value = suru_verb.parts_of_speech.raw_string_value() + ", transitive"
            suru_verb.parts_of_speech.set_raw_string_value(value)
        vocab_note = self.note
        if vocab_note.parts_of_speech.is_intransitive():
            value1 = suru_verb.parts_of_speech.raw_string_value() + ", intransitive"
            suru_verb.parts_of_speech.set_raw_string_value(value1)

        return suru_verb

    def create_shimasu_verb(self) -> VocabNote: return self.create_suru_verb(shimasu=True)

    def clone(self) -> VocabNote:
        from note.vocabulary.vocabnote import VocabNote

        clone_backend_note = Note(app.anki_collection(), app.anki_collection().models.by_name(NoteTypes.Vocab))

        for i in range(len(self.note.backend_note.fields)):
            clone_backend_note.fields[i] = self.note.backend_note.fields[i]

        clone = VocabNote(clone_backend_note)

        self._copy_vocab_tags_to(clone)

        for related in clone.related_notes.synonyms.strings():
            clone.related_notes.synonyms.add(related)

        app.col().vocab.add(clone)

        return clone

    def _copy_vocab_tags_to(self, target: VocabNote) -> None:
        for tag in [tag for tag in self.note.get_tags() if tag.startswith(Tags.Vocab.root)]:
            target.set_tag(tag)

    def clone_to_form(self, form: str) -> VocabNote:
        clone = self.clone()
        clone.question.set(form)

        return clone

    def create_ku_form(self) -> VocabNote:
        return self._create_postfix_prefix_version("く", "adverb", set_compounds=True, truncate_characters=1)

    def create_sa_form(self) -> VocabNote:
        return self._create_postfix_prefix_version("さ", "noun", set_compounds=True, truncate_characters=1)

    def clone_to_derived_form(self, form_suffix: str, create_form_root: Callable[[VocabNote, str], str]) -> VocabNote:
        def create_full_form(form: str) -> str: return create_form_root(self.note, form) + form_suffix

        clone = self._create_new_vocab_with_some_data_copied(question=create_full_form(self.note.get_question()), answer=self.note.get_answer(), readings=[])
        clone.forms.set_list([create_full_form(form) for form in self.note.forms.all_list()])
        vocab_note = self.note
        readings = [create_full_form(reading) for reading in vocab_note.readings.get()]
        clone.readings.set(readings)
        clone.parts_of_speech.set_raw_string_value("expression")
        compounds = [self.note.get_question(), form_suffix]
        clone.compound_parts.set(compounds)
        return clone

    def _create_preview_form(self, form_suffix: str, create_form_root: Callable[[VocabNote, str], str]) -> str:
        return create_form_root(self.note, self.note.get_question()) + form_suffix

    def suffix_to_a_stem(self, form_suffix: str) -> VocabNote:
        return self.clone_to_derived_form(form_suffix, conjugator.get_a_stem_vocab)

    def suffix_to_a_stem_preview(self, form_suffix: str) -> str:
        return self._create_preview_form(form_suffix, conjugator.get_a_stem_vocab)

    def suffix_to_i_stem(self, form_suffix: str) -> VocabNote:
        return self.clone_to_derived_form(form_suffix, conjugator.get_i_stem_vocab)

    def suffix_to_i_stem_preview(self, form_suffix: str) -> str:
        return self._create_preview_form(form_suffix, conjugator.get_i_stem_vocab)

    def suffix_to_e_stem(self, form_suffix: str) -> VocabNote:
        return self.clone_to_derived_form(form_suffix, conjugator.get_e_stem_vocab)

    def suffix_to_e_stem_preview(self, form_suffix: str) -> str:
        return self._create_preview_form(form_suffix, conjugator.get_e_stem_vocab)

    def suffix_to_te_stem(self, form_suffix: str) -> VocabNote:
        return self.clone_to_derived_form(form_suffix, conjugator.get_te_stem_vocab)

    def suffix_to_te_stem_preview(self, form_suffix: str) -> str:
        return self._create_preview_form(form_suffix, conjugator.get_te_stem_vocab)

    def create_masu_form(self) -> VocabNote:
        return self.suffix_to_i_stem("ます")

    def create_te_form(self) -> VocabNote:
        return self.suffix_to_te_stem("て")

    def create_ta_form(self) -> VocabNote:
        return self.suffix_to_te_stem("た")

    def create_ba_form(self) -> VocabNote:
        return self.suffix_to_e_stem("ば")

    def create_receptive_form(self) -> VocabNote:
        result = self.suffix_to_a_stem("れる")
        compound_parts = result.compound_parts.all()
        compound_parts[-1] = "あれる"
        result.compound_parts.set(compound_parts)
        return result

    def create_causative_form(self) -> VocabNote:
        result = self.suffix_to_a_stem("せる")
        compound_parts = result.compound_parts.all()
        compound_parts[-1] = "あせる"
        result.compound_parts.set(compound_parts)
        return result

    def create_nai_form(self) -> VocabNote:
        return self.suffix_to_a_stem("ない")

    def create_imperative(self) -> VocabNote:
        def create_imperative(form: str) -> str: return conjugator.get_imperative(form, self.note.parts_of_speech.is_ichidan(), self.note.parts_of_speech.is_godan())

        clone = self._create_new_vocab_with_some_data_copied(question=create_imperative(self.note.get_question()), answer=self.note.get_answer(), readings=[])
        clone.forms.set_list([create_imperative(form) for form in self.note.forms.all_list()])
        vocab_note = self.note
        readings = [create_imperative(reading) for reading in vocab_note.readings.get()]
        clone.readings.set(readings)
        clone.parts_of_speech.set_raw_string_value("expression")
        return clone

    def create_potential_godan(self) -> VocabNote:
        clone = self.suffix_to_e_stem("る")
        clone.compound_parts.set([self.note.get_question(), "える"])
        return clone

    def _create_new_vocab_with_some_data_copied(self, question: str, answer: str, readings: list[str], copy_vocab_tags: bool = True, copy_matching_rules: bool = True) -> VocabNote:
        from note.vocabulary.vocabnote import VocabNote
        clone = VocabNote.factory.create(question, answer, readings)
        if copy_vocab_tags:
            self._copy_vocab_tags_to(clone)

        if copy_matching_rules:
            clone.matching_configuration.configurable_rules.overwrite_with(self.note.matching_configuration.configurable_rules)
        return clone
</file>

<file path="note/vocabulary/related_vocab/Antonyms.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ankiutils.app import col
from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.notefields.json_object_field import MutableSerializedObjectField
    from note.vocabulary.related_vocab.related_vocab_data import RelatedVocabData
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef

class Antonyms(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote], data: MutableSerializedObjectField[RelatedVocabData]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        self._data: MutableSerializedObjectField[RelatedVocabData] = data

    def strings(self) -> set[str]: return self._data.get().antonyms
    def notes(self) -> list[VocabNote]:
        return col().vocab.with_any_form_in_prefer_exact_match(list(self.strings()))

    def add(self, antonym: str) -> None:
        self.strings().add(antonym)

        for similar in app.col().vocab.with_question(antonym):
            if self._vocab().get_question() not in similar.related_notes.antonyms.strings():
                similar.related_notes.antonyms.add(self._vocab().get_question())

        self._data.save()

    def remove(self, to_remove: str) -> None:
        self.strings().remove(to_remove)

        for similar in app.col().vocab.with_question(to_remove):
            if self._vocab().get_question() in similar.related_notes.antonyms.strings():
                similar.related_notes.antonyms.remove(self._vocab().get_question())

        self._data.save()

    @override
    def __repr__(self) -> str: return self._data.__repr__()
</file>

<file path="note/vocabulary/related_vocab/ergative_twin.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.notefields.json_object_field import MutableSerializedObjectField
    from note.vocabulary.related_vocab.related_vocab_data import RelatedVocabData
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef

class ErgativeTwin(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote], data: MutableSerializedObjectField[RelatedVocabData]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        self._data: MutableSerializedObjectField[RelatedVocabData] = data

    def get(self) -> str: return self._data.get().ergative_twin

    def set(self, value: str) -> None:
        self._data.get().ergative_twin = value

        for twin in app.col().vocab.with_question(value):
            if twin.related_notes.ergative_twin.get() != self._vocab().get_question():
                twin.related_notes.ergative_twin.set(self._vocab().get_question())

        self._data.save()

    def remove(self) -> None:
        for twin in app.col().vocab.with_question(self._vocab().get_question()):
            if twin.related_notes.ergative_twin.get() == self._vocab().get_question():
                twin.related_notes.ergative_twin.remove()

        self._data.get().ergative_twin = ""

        self._data.save()
</file>

<file path="note/vocabulary/related_vocab/perfect_synonyms.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ex_autoslot import AutoSlots
from queryablecollections.collections.q_set import QSet

if TYPE_CHECKING:
    from note.notefields.auto_save_wrappers.set_wrapper import FieldSetWrapper
    from note.vocabulary.vocabnote import VocabNote
    from queryablecollections.collections.q_list import QList
    from sysutils.weak_ref import WeakRef

class PerfectSynonyms(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote], data: FieldSetWrapper[str]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        self._value: FieldSetWrapper[str] = data
        vocab().user.answer.on_change(self.push_answer_to_other_synonyms)

    def notes(self) -> QList[VocabNote]: return app.col().vocab.with_any_question_in(list(self._value.get()))

    def push_answer_to_other_synonyms(self) -> None:
        for synonym in self.notes():
            synonym.user.answer.set(self._vocab().user.answer.value)

    def get(self) -> set[str]: return self._value.get()

    def _remove_internal(self, synonym: str) -> None:
        self._value.discard(synonym)
        self._vocab().related_notes.synonyms.add(synonym)

    def _add_internal(self, synonym: str) -> None:
        if synonym == self._vocab().get_question(): return
        self._value.add(synonym)
        self._vocab().related_notes.synonyms.add(synonym)

    def _resolve_whole_web(self) -> QSet[VocabNote]:
        found: QSet[VocabNote] = QSet()
        def recurse_into(syn: VocabNote) -> None:
            found.add(syn)
            for related in syn.related_notes.perfect_synonyms.notes():
                if related not in found:
                    recurse_into(related)

        recurse_into(self._vocab())
        for synonym in self.notes():
            recurse_into(synonym)
        return found

    def _ensure_all_perfect_synonyms_are_connected(self) -> None:
        whole_web = self._resolve_whole_web()

        all_questions = whole_web.select(lambda syn: syn.get_question()).to_set()
        for synonym in whole_web:
            for question in all_questions:
                synonym.related_notes.perfect_synonyms._add_internal(question)

    def add_overwriting_the_answer_of_the_added_synonym(self, added_question: str) -> None:
        if added_question == self._vocab().get_question(): return
        self._add_internal(added_question)
        self._ensure_all_perfect_synonyms_are_connected()
        self.push_answer_to_other_synonyms()

    def remove(self, synonym_to_remove: str) -> None:
        for to_remove in app.col().vocab.with_question(synonym_to_remove):
            to_remove.related_notes.perfect_synonyms._value.clear()
        for syn in self._resolve_whole_web():
            syn.related_notes.perfect_synonyms._remove_internal(synonym_to_remove)

    @override
    def __repr__(self) -> str: return self._value.__repr__()
</file>

<file path="note/vocabulary/related_vocab/related_vocab_data_serializer.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.notefields.auto_save_wrappers.value_wrapper import ValueWrapper
from note.notefields.json_object_field import ObjectSerializer
from sysutils.json import ex_json
from sysutils.json.json_reader import JsonReader

if TYPE_CHECKING:
    from note.vocabulary.related_vocab.related_vocab_data import RelatedVocabData

class RelatedVocabDataSerializer(ObjectSerializer["RelatedVocabData"], AutoSlots):
    @override
    def deserialize(self, serialized: str) -> RelatedVocabData:
        from note.vocabulary.related_vocab.related_vocab_data import RelatedVocabData
        if not serialized: return RelatedVocabData("", ValueWrapper(""), set(), set(), set(), set(), set())

        reader = JsonReader.from_json(serialized)
        return RelatedVocabData(reader.string("ergative_twin"),
                                ValueWrapper(reader.string("derived_from")),
                                reader.string_set("perfect_synonyms", default=set()),
                                reader.string_set("synonyms"),
                                reader.string_set("antonyms"),
                                reader.string_set("confused_with"),
                                reader.string_set("see_also"))

    @override
    def serialize(self, instance: RelatedVocabData) -> str:
        return ex_json.dict_to_json({"ergative_twin": instance.ergative_twin,
                                     "derived_from": instance.derived_from.get(),
                                     "synonyms": list(instance.synonyms),
                                     "perfect_synonyms": list(instance.perfect_synonyms),
                                     "antonyms": list(instance.antonyms),
                                     "confused_with": list(instance.confused_with),
                                     "see_also": list(instance.see_also)})
</file>

<file path="note/vocabulary/related_vocab/related_vocab_data.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.vocabulary.related_vocab.related_vocab_data_serializer import RelatedVocabDataSerializer

if TYPE_CHECKING:
    from note.notefields.auto_save_wrappers.value_wrapper import ValueWrapper


class RelatedVocabData(AutoSlots):
    serializer: RelatedVocabDataSerializer = RelatedVocabDataSerializer()
    def __init__(self, ergative_twin: str, derived_from: ValueWrapper[str], perfect_synonyms: set[str], similar: set[str], antonyms: set[str], confused_with: set[str], see_also:set[str]) -> None:
        self.ergative_twin: str = ergative_twin
        self.derived_from: ValueWrapper[str] = derived_from

        self.synonyms: set[str] = similar
        self.perfect_synonyms: set[str] = perfect_synonyms
        self.antonyms: set[str] = antonyms
        self.confused_with: set[str] = confused_with
        self.see_also:set[str] = see_also

    @override
    def __repr__(self) -> str: return f"RelatedVocabData(ergative_twin={self.ergative_twin}, derived_from={self.derived_from}, perfect_synonyms={self.perfect_synonyms}, synonyms={self.synonyms}, antonyms={self.antonyms}, confused_with={self.confused_with}, see_also={self.see_also})"
</file>

<file path="note/vocabulary/related_vocab/related_vocab.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from ex_autoslot import AutoSlots
from note.note_constants import NoteFields
from note.notefields.auto_save_wrappers.field_wrapper import FieldWrapper
from note.notefields.auto_save_wrappers.set_wrapper import FieldSetWrapper
from note.notefields.json_object_field import MutableSerializedObjectField
from note.vocabulary.related_vocab.Antonyms import Antonyms
from note.vocabulary.related_vocab.ergative_twin import ErgativeTwin
from note.vocabulary.related_vocab.perfect_synonyms import PerfectSynonyms
from note.vocabulary.related_vocab.related_vocab_data import RelatedVocabData
from note.vocabulary.related_vocab.SeeAlso import SeeAlso
from note.vocabulary.related_vocab.Synonyms import Synonyms
from queryablecollections.q_iterable import query
from sysutils.lazy import Lazy

if TYPE_CHECKING:
    from note.jpnote import JPNote
    from note.kanjinote import KanjiNote
    from note.vocabulary.vocabnote import VocabNote
    from queryablecollections.collections.q_set import QSet
    from sysutils.weak_ref import WeakRef

class RelatedVocab(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab

        self._data: MutableSerializedObjectField[RelatedVocabData] = MutableSerializedObjectField(vocab, NoteFields.Vocab.related_vocab, RelatedVocabData.serializer)

        self.ergative_twin: ErgativeTwin = ErgativeTwin(vocab, self._data)
        self.synonyms: Synonyms = Synonyms(vocab, self._data)
        self.perfect_synonyms: PerfectSynonyms = PerfectSynonyms(vocab, FieldSetWrapper[str].for_json_object_field(self._data, self._data.get().perfect_synonyms))
        self.antonyms: Antonyms = Antonyms(vocab, self._data)
        self.see_also: SeeAlso = SeeAlso(vocab, self._data)
        self.derived_from: FieldWrapper[str, RelatedVocabData] = FieldWrapper(self._data, self._data.get().derived_from)

        self.confused_with: FieldSetWrapper[str] = FieldSetWrapper.for_json_object_field(self._data, self._data.get().confused_with)  # pyright: ignore[reportUnknownMemberType]

        self._in_compound_ids: Lazy[set[int]] = Lazy(lambda: {voc.get_id() for voc in vocab().related_notes.in_compounds()})

    @property
    def in_compound_ids(self) -> set[int]: return self._in_compound_ids()

    def in_compounds(self) -> list[VocabNote]:
        return app.col().vocab.with_compound_part(self._vocab().question.without_noise_characters)

    def homophones_notes(self) -> QSet[VocabNote]:
        return (query(self._vocab().readings.get())
                .select_many(app.col().vocab.with_reading)
                .where(lambda homophone: homophone != self._vocab())
                .to_set())

    def stems_notes(self) -> QSet[VocabNote]:
        return (self._vocab().conjugator.get_stems_for_primary_form()
                .select_many(app.col().vocab.with_question)
                .to_set())  # ex_sequence.flatten([app.col().vocab.with_question(stem) for stem in (_vocab_note.conjugator.get_stems_for_primary_form())])

    @property
    def _main_form_kanji_notes(self) -> QSet[KanjiNote]:
        return app.col().kanji.with_any_kanji_in(self._vocab().kanji.extract_main_form_kanji()).to_set()

    def get_direct_dependencies(self) -> set[JPNote]:
        return set(self._main_form_kanji_notes | self._vocab().compound_parts.all_notes())
</file>

<file path="note/vocabulary/related_vocab/SeeAlso.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ankiutils.app import col
from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.notefields.json_object_field import MutableSerializedObjectField
    from note.vocabulary.related_vocab.related_vocab_data import RelatedVocabData
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef


class SeeAlso(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote], data: MutableSerializedObjectField[RelatedVocabData]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        self._data: MutableSerializedObjectField[RelatedVocabData] = data

    def strings(self) -> set[str]: return self._data.get().see_also
    def notes(self) -> list[VocabNote]:
        return col().vocab.with_any_form_in_prefer_exact_match(list(self.strings()))

    def add(self, to_add: str) -> None:
        self.strings().add(to_add)

        for added_note in app.col().vocab.with_question(to_add):
            if self._vocab().get_question() not in added_note.related_notes.see_also.strings():
                added_note.related_notes.see_also.add(self._vocab().get_question())

        self._data.save()

    def remove(self, to_remove: str) -> None:
        self.strings().remove(to_remove)

        for removed_note in app.col().vocab.with_question(to_remove):
            if self._vocab().get_question() in removed_note.related_notes.see_also.strings():
                removed_note.related_notes.see_also.remove(self._vocab().get_question())

        self._data.save()

    @override
    def __repr__(self) -> str: return self._data.__repr__()
</file>

<file path="note/vocabulary/related_vocab/Synonyms.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.notefields.json_object_field import MutableSerializedObjectField
    from note.vocabulary.related_vocab.related_vocab_data import RelatedVocabData
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef

class Synonyms(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote], data: MutableSerializedObjectField[RelatedVocabData]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        self._data: MutableSerializedObjectField[RelatedVocabData] = data

    def strings(self) -> set[str]: return self._data.get().synonyms

    def _save(self) -> None:
        self.strings().discard(self._vocab().get_question())  # todo: this is cleanup after a bug. Remove soon
        self._data.save()

    def notes(self) -> list[VocabNote]:
        return app.col().vocab.with_any_form_in_prefer_exact_match(list(self.strings()))

    def add(self, synonym: str) -> None:
        if synonym == self._vocab().get_question(): return
        self.strings().add(synonym)

        for similar in app.col().vocab.with_question(synonym):
            if self._vocab().get_question() not in similar.related_notes.synonyms.strings():
                similar.related_notes.synonyms.add(self._vocab().get_question())

        self._save()

    def add_transitively_one_level(self, synonym: str) -> None:
        new_synonym_notes = app.col().vocab.with_any_form_in_prefer_exact_match([synonym])

        for synonym_note in new_synonym_notes:
            for my_synonym in self.strings():
                synonym_note.related_notes.synonyms.add(my_synonym)

        synonyms_of_new_synonym_strings = (new_synonym_notes.select_many(lambda new_synonym_note: new_synonym_note.related_notes.synonyms.strings()).to_set()  # set(ex_sequence.flatten([list(note.related_notes.synonyms.strings()) for note in new_synonym_notes]))
                                           | new_synonym_notes.select(lambda new_synonym_note: new_synonym_note.get_question()).to_set())  # {note.get_question() for note in new_synonym_notes}
        for new_synonym in synonyms_of_new_synonym_strings:
            self.add(new_synonym)

    def remove(self, to_remove: str) -> None:
        self.strings().remove(to_remove)

        for similar in app.col().vocab.with_question(to_remove):
            if self._vocab().get_question() in similar.related_notes.synonyms.strings():
                similar.related_notes.synonyms.remove(self._vocab().get_question())

        self._save()
</file>

<file path="note/vocabulary/serialization/matching_rules_serializer.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.notefields.json_object_field import ObjectSerializer
from sysutils.json import ex_json
from sysutils.json.json_reader import JsonReader

if TYPE_CHECKING:
    from note.vocabulary.vocabnote_matching_rules import VocabNoteMatchingRulesData

class VocabNoteMatchingRulesSerializer(ObjectSerializer["VocabNoteMatchingRulesData"], AutoSlots):
    @override
    def deserialize(self, serialized: str) -> VocabNoteMatchingRulesData:
        from note.vocabulary.vocabnote_matching_rules import VocabNoteMatchingRulesData
        if not serialized: return VocabNoteMatchingRulesData(set(), set(), set(), set(), set())

        reader = JsonReader.from_json(serialized)
        return VocabNoteMatchingRulesData(reader.string_set("surface_is_not"),
                                          reader.string_set("prefix_is_not"),
                                          reader.string_set("suffix_is_not", set()),
                                          reader.string_set("required_prefix"),
                                          reader.string_set("yield_to_surface", set()))

    @override
    def serialize(self, instance: VocabNoteMatchingRulesData) -> str:
        return ex_json.dict_to_json({"surface_is_not": list(instance.surface_is_not),
                                     "prefix_is_not": list(instance.prefix_is_not),
                                     "suffix_is_not": list(instance.suffix_is_not),
                                     "required_prefix": list(instance.required_prefix),
                                     "yield_to_surface": list(instance.yield_to_surface)})
</file>

<file path="note/vocabulary/vocabnote_audio.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.note_constants import NoteFields
from note.notefields.audio_field import WritableAudioField

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef

class VocabNoteAudio(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        self.first: WritableAudioField = WritableAudioField(vocab, NoteFields.Vocab.Audio_b)
        self.second: WritableAudioField = WritableAudioField(vocab, NoteFields.Vocab.Audio_g)
        self.tts: WritableAudioField = WritableAudioField(vocab, NoteFields.Vocab.Audio_TTS)

    def get_primary_audio_path(self) -> str:
        return self.first.first_audiofile_path() or self.second.first_audiofile_path() or self.tts.first_audiofile_path() or ""

    def get_primary_audio(self) -> str:
        return self.first.raw_walue() or self.second.raw_walue() or self.tts.raw_walue() or ""

    @override
    def __repr__(self) -> str: return f"""first: {self.first}, second: {self.second}, tts: {self.tts}"""
</file>

<file path="note/vocabulary/vocabnote_conjugator.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from language_services import conjugator
from queryablecollections.q_iterable import query

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from queryablecollections.collections.q_list import QList
    from sysutils.weak_ref import WeakRef

class VocabNoteConjugator(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.__vocab = vocab

    @property
    def _vocab(self) -> VocabNote: return self.__vocab()

    def _get_stems_for_form(self, form: str) -> QList[str]:
        return (query(conjugator.get_word_stems(form, is_ichidan_verb=self._vocab.parts_of_speech.is_ichidan(), is_godan=self._vocab.parts_of_speech.is_godan()))
                .where(lambda stem: stem != form)
                .to_list())

    def get_stems_for_primary_form(self) -> QList[str]:
        return (self._get_stems_for_form(self._vocab.get_question())
                .distinct()
                .to_list())  # ex_sequence.remove_duplicates_while_retaining_order(self._get_stems_for_form(self._vocab.get_question()))

    def get_stems_for_all_forms(self) -> QList[str]:
        return self._vocab.forms.all_set().select_many(self._get_stems_for_form).distinct().to_list()  # ex_sequence.flatten([self._get_stems_for_form(form) for form in self._vocab.forms.all_set()])
</file>

<file path="note/vocabulary/vocabnote_factory.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from anki.notes import Note
from ankiutils import app
from ex_autoslot import AutoSlots
from language_services.jamdict_ex.dict_lookup import DictLookup
from note.note_constants import NoteTypes

if TYPE_CHECKING:
    from collections.abc import Callable

    from note.vocabulary.vocabnote import VocabNote

class VocabNoteFactory(AutoSlots):
    @staticmethod
    def create_with_dictionary(question: str) -> VocabNote:
        from note.vocabulary.vocabnote import VocabNote
        dict_entry = DictLookup.lookup_word(question)
        if not dict_entry.found_words():
            return VocabNote.factory.create(question, "TODO", [])
        readings = dict_entry.entries.select_many(lambda entry: entry.kana_forms()).to_list()  #list(set(ex_sequence.flatten([ent.kana_forms() for ent in dict_entry.entries])))
        created = VocabNote.factory.create(question, "TODO", readings)
        created.update_generated_data()
        created.generate_and_set_answer()
        return created

    @staticmethod
    def create(question: str, answer: str, readings: list[str], initializer: Callable[[VocabNote], None] | None = None) -> VocabNote:
        from note.vocabulary.vocabnote import VocabNote
        backend_note = Note(app.anki_collection(), app.anki_collection().models.by_name(NoteTypes.Vocab))
        note = VocabNote(backend_note)
        note.question.set(question)
        note.user.answer.set(answer)
        note.readings.set(readings)
        if initializer is not None: initializer(note)
        app.col().vocab.add(note)
        return note
</file>

<file path="note/vocabulary/vocabnote_forms.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ankiutils.app import col
from ex_autoslot import AutoSlots
from note.note_constants import Mine, NoteFields
from note.notefields.comma_separated_strings_list_field_de_duplicated import MutableCommaSeparatedStringsListFieldDeDuplicated
from queryablecollections.q_iterable import query
from sysutils import ex_str
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from queryablecollections.collections.q_list import QList
    from queryablecollections.collections.q_set import QSet

class VocabNoteForms(WeakRefable, AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        field = MutableCommaSeparatedStringsListFieldDeDuplicated(vocab, NoteFields.Vocab.Forms)
        self._field: MutableCommaSeparatedStringsListFieldDeDuplicated = field
        weakrefthis = WeakRef(self)
        self._all_raw_set: Lazy[set[str]] = Lazy(lambda: set(weakrefthis()._field.get()))

        self._all_list: Lazy[QList[str]] = field.lazy_reader(lambda: query(weakrefthis()._field.get()).select(ex_str.strip_brackets).to_list()) # [ex_str.strip_brackets(form) for form in weakrefthis()._field.get()]
        self._all_set: Lazy[QSet[str]] = field.lazy_reader(lambda: weakrefthis()._all_list().to_set())
        self._owned_forms: Lazy[set[str]] = field.lazy_reader(lambda: {weakrefthis()._vocab().get_question()} | {ex_str.strip_brackets(form) for form in weakrefthis()._all_raw_set() if form.startswith("[")})
        self._not_owned_by_other_vocab: Lazy[QSet[str]] = field.lazy_reader(lambda: weakrefthis().___not_owned_by_other_vocab())

    def is_owned_form(self, form: str) -> bool: return form in self._owned_forms()

    def all_list(self) -> QList[str]: return self._all_list()
    def all_set(self) -> QSet[str]: return self._all_set()
    def all_raw_string(self) -> str: return self._field.raw_string_value()

    def all_list_notes(self) -> QList[VocabNote]:
        return self._all_list().select_many(app.col().vocab.with_question).to_list() #ex_sequence.flatten([app.col().vocab.with_question(form) for form in self.all_list()])


    def all_list_notes_by_sentence_count(self) -> list[VocabNote]:
        def prefer_more_sentences(vocab: VocabNote) -> int:
            return -vocab.sentences.counts().total

        return sorted(self.all_list_notes(), key=prefer_more_sentences)

    def not_owned_by_other_vocab(self) -> QSet[str]: return self._not_owned_by_other_vocab()

    def ___not_owned_by_other_vocab(self) -> QSet[str]:
        vocab_note = self._vocab()

        def is_not_owned_by_other_form_note(form: str) -> bool:
            return (app.col().vocab.with_question(form)
                    .where(lambda form_owning_vocab:
                           form_owning_vocab != vocab_note
                           and vocab_note.get_question() in form_owning_vocab.forms.all_set())
                    .none())
            # return not any(owner for owner in app.col().vocab.with_question(form)
            #                if owner != vocab_note and vocab_note.get_question() in owner.forms.all_set())

        return vocab_note.forms.all_set().where(is_not_owned_by_other_form_note).to_set()  # {form for form in vocab_note.forms.all_set() if not is_owned_by_other_form_note(form)}

    def without_noise_characters(self) -> list[str]:
        return [self._strip_noise_characters(form) for form in self.all_list()]

    @staticmethod
    def _strip_noise_characters(string: str) -> str:
        return string.replace(Mine.VocabPrefixSuffixMarker, "")

    def set_set(self, forms: set[str]) -> None: self.set_list(list(forms))

    def set_list(self, forms: list[str]) -> None: self._field.set(forms)

    def remove(self, remove: str) -> None:
        self._field.remove(remove)

        for remove_note in [voc for voc in col().vocab.with_question(remove) if self._vocab().get_question() in voc.forms.all_set()]:
            remove_note.forms.remove(self._vocab().get_question())

    def add(self, add: str) -> None:
        self._field.add(add)

        for add_note in [voc for voc in col().vocab.with_question(add) if self._vocab().get_question() not in voc.forms.all_set()]:
            add_note.forms.add(self._vocab().get_question())

    @override
    def __repr__(self) -> str: return self._field.__repr__()
</file>

<file path="note/vocabulary/vocabnote_generated_data.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from note.note_constants import Tags
from sysutils import kana_utils

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote

def update_generated_data(vocab: VocabNote) -> None:
    vocab.meta_data.sentence_count.set(len(vocab.sentences.all()))
    vocab.active_answer.set(vocab.get_answer())
    vocab.related_notes.perfect_synonyms.push_answer_to_other_synonyms()

    from language_services.jamdict_ex.dict_lookup import DictLookup

    question = vocab.question.without_noise_characters.strip()
    readings = ",".join(vocab.readings.get())

    if not readings and kana_utils.is_only_kana(question):
        vocab.readings.set([question])
        vocab.set_tag(Tags.UsuallyKanaOnly)

    if len(vocab.compound_parts.all()) == 0 and vocab.parts_of_speech.is_suru_verb_included():
        compounds = [question[:-2], "する"]
        vocab.compound_parts.set(compounds)

    if vocab.get_question():
        speech_types = vocab.parts_of_speech.get() - {"Unknown",
                                                      "Godan verbIchidan verb",
                                                      "Ichidan verbGodan verb" # crap inserted by bug in yomitan
                                                      }

        if vocab.readings.get():  # if we don't have a reading, the lookup will be too unreliable
            lookup = DictLookup.lookup_vocab_word_or_name(vocab)
            if lookup.is_uk() and not vocab.has_tag(Tags.DisableKanaOnly):
                vocab.set_tag(Tags.UsuallyKanaOnly)

            if not vocab.forms.all_set():
                if lookup.found_words():
                    vocab.forms.set_set(lookup.valid_forms(vocab.parts_of_speech.is_uk()))

                if vocab.parts_of_speech.is_uk() and vocab.readings.get()[0] not in vocab.forms.all_set():
                    vocab.forms.set_set(vocab.forms.all_set() | set(vocab.readings.get()))

            if len(speech_types) == 0:
                vocab.parts_of_speech.set_automatically_from_dictionary()

        if vocab.get_question() not in vocab.forms.all_set():
            vocab.forms.set_set(vocab.forms.all_set() | {vocab.get_question()})
</file>

<file path="note/vocabulary/vocabnote_kanji.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from queryablecollections.q_iterable import query
from sysutils import ex_str, kana_utils

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from queryablecollections.collections.q_list import QList
    from sysutils.weak_ref import WeakRef

class VocabNoteKanji(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.__vocab = vocab

    @property
    def _vocab(self) -> VocabNote: return self.__vocab()

    def extract_main_form_kanji(self) -> QList[str]:
        clean = ex_str.strip_html_and_bracket_markup(self._vocab.get_question())
        return query(clean).where(kana_utils.character_is_kanji).to_list() #[char for char in clean if kana_utils.character_is_kanji(char)]

    def extract_all_kanji(self) -> set[str]:
        clean = ex_str.strip_html_and_bracket_markup(self._vocab.get_question() + self._vocab.forms.all_raw_string())
        return {char for char in clean if kana_utils.character_is_kanji(char)}

    @override
    def __repr__(self) -> str: return f"""main: {self.extract_main_form_kanji()}, all: {self.extract_all_kanji()}"""
</file>

<file path="note/vocabulary/vocabnote_matching_rules_is_inflecting_word.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.note_constants import Tags
from note.notefields.tag_flag_field import TagFlagField
from sysutils.lazy import Lazy

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef

class IsInflectingWord(TagFlagField, AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        super().__init__(vocab, Tags.Vocab.Matching.is_inflecting_word)
        self._vocab: WeakRef[VocabNote] = vocab
        self._is_active: Lazy[bool] = Lazy(lambda: self.is_set() or self._vocab().parts_of_speech.is_inflecting_word_type())

    @property
    def is_active(self) -> bool: return self._is_active()

    @override
    def __repr__(self) -> str: return f"""{self.tag}: {self.is_active}"""
</file>

<file path="note/vocabulary/vocabnote_matching_rules_yield_last_token_to_next_compound.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ex_autoslot import AutoSlots
from note.note_constants import Tags
from note.notefields.require_forbid_flag_field import RequireForbidFlagField
from sysutils.lazy import Lazy
from sysutils.simple_string_builder import SimpleStringBuilder

if TYPE_CHECKING:
    from configuration.configuration_value import ConfigurationValueBool
    from note.vocabulary.vocabnote import VocabNote
    from note.vocabulary.vocabnote_parts_of_speech import VocabNotePartsOfSpeech
    from sysutils.weak_ref import WeakRef

class YieldLastTokenToOverlappingCompound(RequireForbidFlagField, AutoSlots):
    automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound: ConfigurationValueBool = app.config().automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound
    automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound: ConfigurationValueBool = app.config().automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound
    automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound: ConfigurationValueBool = app.config().automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound



    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        super().__init__(vocab, Tags.Vocab.Matching.yield_last_token_to_overlapping_compound, Tags.Vocab.Matching.Forbids.auto_yielding)
        self._pos:VocabNotePartsOfSpeech = vocab().parts_of_speech
        self._required:Lazy[bool] = Lazy(self._decide_if_required)

    def _decide_if_required(self) -> bool:
        return (super().is_required
                or (not self.is_forbidden
                    and (  # na adjectives
                            self._pos.is_complete_na_adjective()
                            # suru verb compounds
                            or (self.automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound.get_value()
                                and self._pos.is_suru_verb_included()
                                and not self._pos.is_ni_suru_ga_suru_ku_suru_compound())
                            # passive verb compounds
                            or (self.automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound.get_value()
                                and self._pos.is_passive_verb_compound())
                            # causative verb compounds
                            or (self.automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound.get_value()
                                and self._pos.is_causative_verb_compound())
                    )))

    @property
    @override
    def is_required(self) -> bool: return self._required()

    @override
    def __repr__(self) -> str: return (SimpleStringBuilder()
                                       .append_if(self.is_required, "required")
                                       .append_if(self.is_forbidden, "forbidden")
                                       .build())
</file>

<file path="note/vocabulary/vocabnote_matching_rules.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.note_constants import NoteFields, Tags
from note.notefields.auto_save_wrappers.set_wrapper import FieldSetWrapper
from note.notefields.json_object_field import MutableSerializedObjectField
from note.notefields.require_forbid_flag_field import RequireForbidFlagField
from note.notefields.tag_flag_field import TagFlagField
from note.vocabulary.serialization.matching_rules_serializer import VocabNoteMatchingRulesSerializer
from note.vocabulary.vocabnote_matching_rules_is_inflecting_word import IsInflectingWord
from note.vocabulary.vocabnote_matching_rules_yield_last_token_to_next_compound import YieldLastTokenToOverlappingCompound
from sysutils.debug_repr_builder import SkipFalsyValuesDebugReprBuilder
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote

class VocabNoteMatchingRulesData(AutoSlots):
    serializer: VocabNoteMatchingRulesSerializer = VocabNoteMatchingRulesSerializer()
    def __init__(self, surface_is_not: set[str], prefix_is_not: set[str], suffix_is_not: set[str], required_prefix: set[str], yield_to_surface: set[str]) -> None:
        self.prefix_is_not: set[str] = prefix_is_not
        self.suffix_is_not: set[str] = suffix_is_not
        self.surface_is_not: set[str] = surface_is_not
        self.yield_to_surface: set[str] = yield_to_surface
        self.required_prefix: set[str] = required_prefix

class VocabNoteMatchingRules(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self._data: MutableSerializedObjectField[VocabNoteMatchingRulesData] = MutableSerializedObjectField(vocab, NoteFields.Vocab.matching_rules, VocabNoteMatchingRulesData.serializer)
        self.surface_is_not: FieldSetWrapper[str] = FieldSetWrapper.for_json_object_field(self._data, self._data.get().surface_is_not)  # pyright: ignore[reportUnknownMemberType]
        self.yield_to_surface: FieldSetWrapper[str] = FieldSetWrapper.for_json_object_field(self._data, self._data.get().yield_to_surface)  # pyright: ignore[reportUnknownMemberType]
        self.prefix_is_not: FieldSetWrapper[str] = FieldSetWrapper.for_json_object_field(self._data, self._data.get().prefix_is_not)  # pyright: ignore[reportUnknownMemberType]
        self.suffix_is_not: FieldSetWrapper[str] = FieldSetWrapper.for_json_object_field(self._data, self._data.get().suffix_is_not)  # pyright: ignore[reportUnknownMemberType]
        self.required_prefix: FieldSetWrapper[str] = FieldSetWrapper.for_json_object_field(self._data, self._data.get().required_prefix)  # pyright: ignore[reportUnknownMemberType]

    def overwrite_with(self, other: VocabNoteMatchingRules) -> None:
        self.surface_is_not.overwrite_with(other.surface_is_not)
        self.yield_to_surface.overwrite_with(other.yield_to_surface)
        self.prefix_is_not.overwrite_with(other.prefix_is_not)
        self.suffix_is_not.overwrite_with(other.suffix_is_not)
        self.required_prefix.overwrite_with(other.required_prefix)

    @override
    def __repr__(self) -> str: return (SkipFalsyValuesDebugReprBuilder()
                                       .prop("surface_is_not", self.surface_is_not.get())
                                       .prop("prefix_is_not", self.prefix_is_not.get())
                                       .prop("suffix_is_not", self.suffix_is_not.get())
                                       .prop("required_prefix", self.required_prefix.get()).repr)

class VocabMatchingRulesConfigurationRequiresForbidsFlags(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.e_stem: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.e_stem, Tags.Vocab.Matching.Forbids.e_stem)
        self.a_stem: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.a_stem, Tags.Vocab.Matching.Forbids.a_stem)
        self.past_tense_stem: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.past_tense_stem, Tags.Vocab.Matching.Forbids.past_tense_stem)
        self.te_form_stem: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.te_form_stem, Tags.Vocab.Matching.Forbids.te_form_stem)
        self.single_token: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.single_token, Tags.Vocab.Matching.Requires.compound)
        self.sentence_end: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.sentence_end, Tags.Vocab.Matching.Forbids.sentence_end)
        self.sentence_start: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.sentence_start, Tags.Vocab.Matching.Forbids.sentence_start)
        self.exact_match: RequireForbidFlagField = RequireForbidFlagField(vocab, Tags.Vocab.Matching.Requires.exact_match, Tags.Vocab.Matching.Forbids.exact_match)
        self.yield_last_token: RequireForbidFlagField = YieldLastTokenToOverlappingCompound(vocab)

class VocabMatchingRulesConfigurationBoolFlags(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.is_inflecting_word: IsInflectingWord = IsInflectingWord(vocab)
        self.is_poison_word: TagFlagField = TagFlagField(vocab, Tags.Vocab.Matching.is_poison_word)
        self.match_with_preceding_vowel: TagFlagField = TagFlagField(vocab, Tags.Vocab.Matching.Todo.with_preceding_vowel)
        self.question_overrides_form: TagFlagField = TagFlagField(vocab, Tags.Vocab.question_overrides_form)

    @override
    def __repr__(self) -> str: return f"""{self.is_inflecting_word}, {self.is_poison_word}, {self.match_with_preceding_vowel}, {self.question_overrides_form}"""

class VocabNoteMatchingConfiguration(WeakRefable, AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.vocab: WeakRef[VocabNote] = vocab
        self.weakref: WeakRef[VocabNoteMatchingConfiguration] = WeakRef(self)
        self._rules: Lazy[VocabNoteMatchingRules] = Lazy(lambda: VocabNoteMatchingRules(vocab))

        self.requires_forbids: VocabMatchingRulesConfigurationRequiresForbidsFlags = VocabMatchingRulesConfigurationRequiresForbidsFlags(vocab)
        self.bool_flags: VocabMatchingRulesConfigurationBoolFlags = VocabMatchingRulesConfigurationBoolFlags(vocab)

    @property
    def configurable_rules(self) -> VocabNoteMatchingRules: return self._rules()

    @override
    def __repr__(self) -> str: return (SkipFalsyValuesDebugReprBuilder()
                                       .flag("requires_exact_match", self.requires_forbids.exact_match.is_required)
                                       .flag("forbids_exact_match", self.requires_forbids.exact_match.is_forbidden)
                                       .flag("requires_single_token", self.requires_forbids.single_token.is_required)
                                       .flag("requires_compound", self.requires_forbids.single_token.is_forbidden)
                                       .flag("requires_a_stem", self.requires_forbids.a_stem.is_required)
                                       .flag("forbids_a_stem", self.requires_forbids.a_stem.is_forbidden)
                                       .flag("requires_e_stem", self.requires_forbids.e_stem.is_required)
                                       .flag("forbids_e_stem", self.requires_forbids.e_stem.is_forbidden)
                                       .flag("requires_past_tense_stem", self.requires_forbids.past_tense_stem.is_required)
                                       .flag("forbids_past_tense_stem", self.requires_forbids.past_tense_stem.is_forbidden)
                                       .flag("requires_t_form_stem", self.requires_forbids.te_form_stem.is_required)
                                       .flag("forbids_t_form_stem", self.requires_forbids.te_form_stem.is_forbidden)
                                       .flag("requires_sentence_end", self.requires_forbids.sentence_end.is_required)
                                       .flag("forbids_sentence_end", self.requires_forbids.sentence_end.is_forbidden)
                                       .flag("requires_sentence_start", self.requires_forbids.sentence_start.is_required)
                                       .flag("forbids_sentence_start", self.requires_forbids.sentence_start.is_forbidden)
                                       .flag("is_inflecting_word", self.bool_flags.is_inflecting_word.is_set())
                                       .flag("is_poison_word", self.bool_flags.is_poison_word.is_set())
                                       .flag("yield_last_token_to_overlapping_compound", self.requires_forbids.yield_last_token.is_required)
                                       .flag("forbid_auto_yielding_last_token", self.requires_forbids.yield_last_token.is_forbidden)
                                       .flag("match_with_preceding_vowel", self.bool_flags.match_with_preceding_vowel.is_set())
                                       .flag("question_overrides_form", self.bool_flags.question_overrides_form.is_set())
                                       .include(self.configurable_rules).repr)
</file>

<file path="note/vocabulary/vocabnote_meta_tag.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote



class VocabMetaTag(AutoSlots):
    def __init__(self, name: str, display: str, tooltip: str) -> None:
        self.name: str = name
        self.display: str = display
        self.tooltip: str = tooltip

def get_meta_tags_html(vocab: VocabNote, display_extended_sentence_statistics: bool = True, no_sentense_statistics: bool = False) -> str:
    tags = set(vocab.get_tags())
    meta: list[VocabMetaTag] = []
    tos = {t.lower().strip() for t in vocab.parts_of_speech.raw_string_value().split(",")}

    if not no_sentense_statistics:
        def max_nine_number(value: int) -> str: return f"""{value}""" if value < 10 else "+"
        highlighted_in = vocab.sentences.user_highlighted()
        meta.append(VocabMetaTag("highlighted_in_sentences", f"""{max_nine_number(len(highlighted_in))}""", f"""highlighted in {len(highlighted_in)} sentences"""))


        counts = vocab.sentences.counts()
        if counts.total > 0:
            tooltip_text = f"""in {counts.total} sentences. Studying-listening:{counts.studying_listening}, Studying-reading:{counts.studying_reading}"""
            if counts.studying_listening > 0 or counts.studying_reading > 0:
                if display_extended_sentence_statistics:
                    meta.append(VocabMetaTag("in_studying_sentences", f"""{counts.studying_listening}:{counts.studying_reading}/{counts.total}""", tooltip_text))
                else:
                    def create_display_text() -> str:
                        if counts.studying_listening > 9 and counts.studying_reading > 9:
                            return "+"
                        return f"{max_nine_number(counts.studying_listening)}:{max_nine_number(counts.studying_reading)}/{max_nine_number(counts.total)}"

                    meta.append(VocabMetaTag("in_studying_sentences", create_display_text(), tooltip_text))
            else:
                meta.append(VocabMetaTag("in_sentences", f"""{counts.total}""", tooltip_text))
        else:
            meta.append(VocabMetaTag("in_no_sentences", f"""{counts.total}""", f"""in {counts.total} sentences"""))

    # overarching info
    if "_uk" in tags: meta.append(VocabMetaTag("uk", "uk", "usually written using kana only"))
    if "expression" in tos: meta.append(VocabMetaTag("expression", "x", "expression"))
    if "abbreviation" in tos: meta.append(VocabMetaTag("abbreviation", "abbr", "abbreviation"))
    if "auxiliary" in tos: meta.append(VocabMetaTag("auxiliary", "aux", "auxiliary"))
    if "prefix" in tos: meta.append(VocabMetaTag("prefix", "頭", "prefix"))
    if "suffix" in tos: meta.append(VocabMetaTag("suffix", "尾", "suffix"))

    # nouns
    if "proper noun" in tos: meta.append(VocabMetaTag("proper-noun", "p-名", "proper noun"))
    elif "pronoun" in tos: meta.append(VocabMetaTag("pronoun", "pr-名", "pronoun"))
    elif "noun" in tos: meta.append(VocabMetaTag("noun", "名", "noun"))
    if "adverbial noun" in tos: meta.append(VocabMetaTag("adverbial-noun", "副-名", "adverbial noun"))
    if "independent noun" in tos: meta.append(VocabMetaTag("independent-noun", "i-名", "independent noun"))

    # verbs
    if "ichidan verb" in tos: meta.append(_create_verb_meta_tag("ichidan", "1", "ichidan verb", tos))
    if "godan verb" in tos: meta.append(_create_verb_meta_tag("godan", "5", "godan verb", tos))
    if "suru verb" in tos or "verbal noun" in tos or "する verb" in tos: meta.append(_create_verb_meta_tag("suru-verb", "為", "suru verb", tos))
    if "kuru verb" in tos: meta.append(_create_verb_meta_tag("kuru-verb", "k-v", "kuru verb", tos))
    if "auxiliary verb" in tos: meta.append(_create_verb_meta_tag("auxiliary-verb", "aux-v", "auxiliary verb", tos))

    # adverbs
    if "と adverb" in tos or "to-adverb" in tos: meta.append(VocabMetaTag("to-adverb", "と", "adverbial noun taking the と particle to act as adverb"))
    elif "adverb" in tos: meta.append(VocabMetaTag("adverb", "副", "adverb"))
    elif "adverbial" in tos: meta.append(VocabMetaTag("adverbial", "副", "adverbial"))

    # adjectives
    if "い adjective" in tos or "i-adjective" in tos: meta.append(VocabMetaTag("i-adjective", "い", "true adjective ending on the い copula"))
    if "な adjective" in tos or "na-adjective" in tos: meta.append(VocabMetaTag("na-adjective", "な", "adjectival noun taking the な particle to act as adjective"))
    if "の adjective" in tos or "no-adjective" in tos: meta.append(VocabMetaTag("no-adjective", "の", "adjectival noun taking the の particle to act as adjective"))
    if "auxiliary adjective" in tos: meta.append(VocabMetaTag("auxiliary-adjective", "a-い", "auxiliary adjective"))

    # ???
    if "in compounds" in tos: meta.append(VocabMetaTag("in-compounds", "i-c", "in compounds"))

    # misc

    if "counter" in tos: meta.append(VocabMetaTag("counter", "ctr", "counter"))
    if "numeral" in tos: meta.append(VocabMetaTag("numeral", "num", "numeral"))
    if "interjection" in tos: meta.append(VocabMetaTag("interjection", "int", "interjection"))
    if "conjunction" in tos: meta.append(VocabMetaTag("conjunction", "conj", "conjunction"))
    if "particle" in tos: meta.append(VocabMetaTag("particle", "prt", "particle"))

    # my own inventions
    if "masu-suffix" in tos: meta.append(VocabMetaTag("masu-suffix", "連", "follows the 連用形/masu-stem form of a verb"))

    return """<ol class="vocab_tag_list">""" + "".join([f"""<li class="vocab_tag vocab_tag_{tag.name}" title="{tag.tooltip}">{tag.display}</li>""" for tag in meta]) + "</ol>"

def _create_verb_meta_tag(name: str, display: str, tooltip: str, tos: set[str]) -> VocabMetaTag:
    tag = VocabMetaTag(name, display, tooltip)

    if "intransitive verb" in tos or "intransitive" in tos:
        tag.display += "i"
        tag.tooltip = "intransitive " + tag.tooltip
    if "transitive verb" in tos or "transitive" in tos:
        tag.display += "t"
        tag.tooltip = "transitive " + tag.tooltip

    return tag
</file>

<file path="note/vocabulary/vocabnote_metadata.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from language_services.jamdict_ex.priority_spec import PrioritySpec
from note.note_constants import NoteFields
from note.notefields.integer_field import IntegerField
from note.vocabulary import vocabnote_meta_tag

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef


class VocabNoteMetaData(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.__vocab: WeakRef[VocabNote] = vocab
        self.sentence_count: IntegerField = IntegerField(vocab, NoteFields.Vocab.sentence_count)

    @property
    def _vocab(self) -> VocabNote: return self.__vocab()

    def meta_tags_html(self, display_extended_sentence_statistics: bool = True, no_sentense_statistics: bool = False) -> str:
        return vocabnote_meta_tag.get_meta_tags_html(self._vocab, display_extended_sentence_statistics, no_sentense_statistics)

    def priority_spec(self) -> PrioritySpec:
        from language_services.jamdict_ex.dict_lookup import DictLookup
        lookup = DictLookup.lookup_vocab_word_or_name(self._vocab)
        return lookup.priority_spec() if lookup else PrioritySpec(set())
</file>

<file path="note/vocabulary/vocabnote_parts_of_speech.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.note_constants import NoteFields, Tags
from note.notefields.comma_separated_strings_set_field import MutableCommaSeparatedStringsSetField

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef

class VocabNotePartsOfSpeech(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.__vocab = vocab
        self._field: MutableCommaSeparatedStringsSetField = MutableCommaSeparatedStringsSetField(vocab, NoteFields.Vocab.parts_of_speech)

    @property
    def _vocab(self) -> VocabNote: return self.__vocab()

    def raw_string_value(self) -> str:
        return self._field.raw_string_value()

    def set_raw_string_value(self, value: str) -> None:
        self._field.set_raw_string_value(value)

    def get(self) -> set[str]:
        return self._field.get()

    def is_ichidan(self) -> bool:
        return "ichidan" in self.raw_string_value().lower()

    def is_godan(self) -> bool:
        return "godan" in self.raw_string_value().lower()

    def is_inflecting_word_type(self) -> bool:
        return self.is_godan() or self.is_ichidan()

    def is_suru_verb_included(self) -> bool:
        question = self._vocab.question.without_noise_characters
        return len(question) > 2 and question[-2:] == "する"

    _ga_suru_ni_suru_endings: set[str] = {"がする", "にする", "くする"}
    def is_ni_suru_ga_suru_ku_suru_compound(self) -> bool:
        question = self._vocab.question.without_noise_characters
        return len(question) > 3 and question[-3:] in self._ga_suru_ni_suru_endings

    def is_uk(self) -> bool: return self._vocab.has_tag(Tags.UsuallyKanaOnly)

    _transitive_string_values: list[str] = ["transitive", "transitive verb"]
    _intransitive_string_values: list[str] = ["intransitive", "intransitive verb"]
    def is_transitive(self) -> bool: return any(val for val in self._transitive_string_values if val in self.get())
    def is_intransitive(self) -> bool: return any(val for val in self._intransitive_string_values if val in self.get())

    def set_automatically_from_dictionary(self) -> None:
        from language_services.jamdict_ex.dict_lookup import DictLookup

        lookup = DictLookup.lookup_vocab_word_or_name(self._vocab)
        if lookup.found_words():
            value = ", ".join(lookup.parts_of_speech())
            self.set_raw_string_value(value)
        elif self.is_suru_verb_included():
            question = self._vocab.question.without_noise_characters[:-2]
            readings = [reading[:-2] for reading in self._vocab.readings.get()]
            lookup = DictLookup.lookup_word_or_name_with_matching_reading(question, readings)
            pos = lookup.parts_of_speech() & {"transitive", "intransitive"}
            value1 = "suru verb, " + ", ".join(pos)
            self.set_raw_string_value(value1)

    # todo in terms of using this for yielding compounds, される is apt not to work since in for instance 左右されます, され is tokenized as する、れる so two tokens would need to be yielded, not one. How to fix?
    _passive_verb_endings: set[str] = {"あれる", "られる", "される"}
    def is_passive_verb_compound(self) -> bool:
        compounds = self._vocab.compound_parts.primary()
        if len(compounds) == 0: return False
        return compounds[-1] in self._passive_verb_endings

    _causative_verb_endings: set[str] = {"あせる", "させる", "あす", "さす"}
    def is_causative_verb_compound(self) -> bool:
        compounds = self._vocab.compound_parts.primary()
        if len(compounds) == 0: return False
        return compounds[-1] in self._causative_verb_endings

    _na_adjective_tos_names: set[str] = {"な adjective", "na-adjective"}
    def is_complete_na_adjective(self) -> bool:
        return self.__vocab().question.raw.endswith("な") and any(na for na in self._na_adjective_tos_names if na in self.get())

    @override
    def __repr__(self) -> str: return self._field.__repr__()
</file>

<file path="note/vocabulary/vocabnote_question.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from language_services import conjugator
from note.note_constants import Mine, NoteFields
from note.notefields.mutable_string_field import MutableStringField

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.lazy import Lazy
    from sysutils.weak_ref import WeakRef

class VocabStems(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab

    def masu_stem(self) -> str | None:
        masu_stem = conjugator.get_i_stem_vocab(self._vocab())
        return masu_stem if masu_stem != self._vocab().question.raw else None

class VocabNoteQuestion(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self._vocab: WeakRef[VocabNote] = vocab
        field = MutableStringField(vocab, NoteFields.Vocab.question)
        self._field: MutableStringField = field
        self._raw: Lazy[str] = field.lazy_reader(lambda: field.value)
        self._without_noise_characters: Lazy[str] = field.lazy_reader(lambda: field.value.replace(Mine.VocabPrefixSuffixMarker, ""))

    @property
    def raw(self) -> str: return self._raw()
    @property
    def without_noise_characters(self) -> str: return self._without_noise_characters()

    def stems(self) -> VocabStems: return VocabStems(self._vocab)

    def set(self, value: str) -> None:
        self._field.set(value)
        if value not in self._vocab().forms.all_set():
            self._vocab().forms.set_set(self._vocab().forms.all_set() | {value})

    @override
    def __repr__(self) -> str: return self.raw
</file>

<file path="note/vocabulary/vocabnote_sentences.py">
from __future__ import annotations

import time
from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from note.note_constants import NoteFields
from sysutils.lazy import Lazy
from sysutils.weak_ref import WeakRef, WeakRefable

if TYPE_CHECKING:
    from note.collection.jp_collection import JPCollection
    from note.sentences.sentencenote import SentenceNote
    from note.vocabulary.vocabnote import VocabNote

class SentenceCounts(AutoSlots):
    def __init__(self, parent: WeakRef[VocabNoteSentences]) -> None:
        self._parent: WeakRef[VocabNoteSentences] = parent
        self._studying_reading: int = 0
        self._studying_listening: int = 0
        self._total: int = 0
        self._last_update_time: float = 0
        self._cache_seconds: int = 0

    @property
    def total(self) -> int: return self._up_to_date_self()._total
    @property
    def studying_reading(self) -> int: return self._up_to_date_self()._studying_reading
    @property
    def studying_listening(self) -> int: return self._up_to_date_self()._studying_listening

    def _up_to_date_self(self) -> SentenceCounts:
        def _how_long_to_cache_for() -> int:
            if self._total < 10: return 60
            if self._total < 100: return 600
            return 6000

        def get_studying_sentence_count(card: str = "") -> int:
            return len([sentence for sentence in sentences if sentence.is_studying(card)])

        if time.time() - self._last_update_time > self._cache_seconds:
            self._last_update_time = time.time()
            sentences = self._parent().with_owned_form()
            self._studying_reading = get_studying_sentence_count(NoteFields.VocabNoteType.Card.Reading)
            self._studying_listening = get_studying_sentence_count(NoteFields.VocabNoteType.Card.Listening)
            self._total = len(sentences)
            self._cache_seconds = _how_long_to_cache_for()
        return self

class VocabNoteSentences(WeakRefable, AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.__vocab = vocab
        weakrefthis = WeakRef(self)
        self.counts: Lazy[SentenceCounts] = Lazy(lambda: SentenceCounts(weakrefthis))

    @property
    def _vocab(self) -> VocabNote: return self.__vocab()

    @property
    def _collection(self) -> JPCollection: return self._vocab.collection

    def all(self) -> list[SentenceNote]:
        return self._collection.sentences.with_vocab(self._vocab)

    def with_owned_form(self) -> list[SentenceNote]:
        return self._collection.sentences.with_vocab_owned_form(self._vocab)

    def with_primary_form(self) -> list[SentenceNote]:
        return self._collection.sentences.with_form(self._vocab.get_question())

    def user_highlighted(self) -> list[SentenceNote]:
        return list(self._collection.sentences.with_highlighted_vocab(self._vocab))

    def studying(self) -> list[SentenceNote]:
        return [sentence for sentence in self.all() if sentence.is_studying()]
</file>

<file path="note/vocabulary/vocabnote_sorting.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from queryablecollections.collections.q_list import QList

if TYPE_CHECKING:
    from collections.abc import Iterable

    from note.vocabulary.vocabnote import VocabNote

def sort_vocab_list_by_studying_status(vocabs: Iterable[VocabNote], primary_voc: list[str] | None = None, preferred_kanji: str | None = None) -> QList[VocabNote]:
    def prefer_primary_vocab_in_order(local_vocab: VocabNote) -> int:
        for index, primary in enumerate(_primary_voc):
            if local_vocab.get_question() == primary or local_vocab.question.without_noise_characters == primary or (local_vocab.readings.get() and local_vocab.readings.get()[0] == primary):
                return index

        return 1000

    def prefer_vocab_with_kanji(local_vocab: VocabNote) -> int:
        return 0 if preferred_kanji is None or preferred_kanji in local_vocab.get_question() else 1

    def prefer_studying_vocab(local_vocab: VocabNote) -> int:
        return 1 if local_vocab.is_studying() else 2

    def prefer_studying_sentences(local_vocab: VocabNote) -> int:
        return 1 if local_vocab.sentences.studying() else 2

    def prefer_more_sentences(local_vocab: VocabNote) -> int:
        return -len(local_vocab.sentences.all())

    def prefer_high_priority(_vocab: VocabNote) -> int:
        return _vocab.meta_data.priority_spec().priority

    _primary_voc = primary_voc if primary_voc else []

    result = QList(vocabs)

    result.sort(key=lambda local_vocab: (prefer_vocab_with_kanji(local_vocab),
                                         prefer_primary_vocab_in_order(local_vocab),
                                         prefer_studying_vocab(local_vocab),
                                         prefer_studying_sentences(local_vocab),
                                         prefer_more_sentences(local_vocab),
                                         prefer_high_priority(local_vocab),
                                         local_vocab.get_question()))

    return result
</file>

<file path="note/vocabulary/vocabnote_usercompoundparts.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ex_autoslot import AutoSlots
from language_services.jamdict_ex.dict_lookup import DictLookup
from language_services.janome_ex.word_extraction.word_exclusion import WordExclusion
from note.note_constants import NoteFields
from note.notefields.comma_separated_strings_list_field import MutableCommaSeparatedStringsListField
from note.sentences.sentence_configuration import SentenceConfiguration
from queryablecollections.q_iterable import query

if TYPE_CHECKING:
    from note.collection.jp_collection import JPCollection
    from note.vocabulary.vocabnote import VocabNote
    from queryablecollections.collections.q_list import QList
    from queryablecollections.collections.q_set import QSet
    from sysutils.weak_ref import WeakRef

class VocabNoteUserCompoundParts(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.__vocab = vocab
        self._field: MutableCommaSeparatedStringsListField = MutableCommaSeparatedStringsListField(vocab, NoteFields.Vocab.user_compounds)

    @property
    def _vocab(self) -> VocabNote: return self.__vocab()

    @property
    def _collection(self) -> JPCollection: return self._vocab.collection

    def primary(self) -> QList[str]: return query(self._field.get()).where(lambda part: not part.startswith("[")).to_list() #[part for part in self._field.get() if not part.startswith("[")]
    def all(self) -> QList[str]: return query(self._field.get()).select(self._strip_brackets).to_list() #[self._strip_brackets(part) for part in self._field.get()]
    def set(self, value: list[str]) -> None: self._field.set(value)

    def all_notes(self) -> QSet[VocabNote]:
        return self.all().select_many(app.col().vocab.with_question).to_set()

    def primary_parts_notes(self) -> QSet[VocabNote]:
        return self.primary().select_many(app.col().vocab.with_form_prefer_exact_match).to_set()

    def auto_generate(self) -> None:
        from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis
        from note.vocabulary.vocabnote import VocabNote
        analysis = TextAnalysis(self._vocab.get_question(), SentenceConfiguration.from_incorrect_matches([WordExclusion.global_(form) for form in self._vocab.forms.all_set()]))
        compound_parts = [a.form for a in analysis.display_word_variants if a.form not in self._vocab.forms.all_set()]
        if not len(compound_parts) > 1:  # time to brute force it
            word = self._vocab.get_question()
            all_substrings = [word[i:j] for i in range(len(word)) for j in range(i + 1, len(word) + 1) if word[i:j] != word]
            all_word_substrings = [w for w in all_substrings if DictLookup.is_dictionary_or_collection_word(w)]
            compound_parts = [segment for segment in all_word_substrings if not any(parent for parent in all_word_substrings if segment in parent and parent != segment)]

        segments_missing_vocab = [segment for segment in compound_parts if not self._collection.vocab.is_word(segment)]
        for missing in segments_missing_vocab:
            created = VocabNote.factory.create_with_dictionary(missing)
            created.suspend_all_cards()

        self.set(compound_parts)

    @staticmethod
    def _strip_brackets(part: str) -> str:
        return part.replace("[", "").replace("]", "")

    @override
    def __repr__(self) -> str: return self._field.__repr__()
</file>

<file path="note/vocabulary/vocabnote_userfields.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from note.note_constants import NoteFields
from note.notefields.mutable_string_field import MutableStringField

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from sysutils.weak_ref import WeakRef


class VocabNoteUserfields(AutoSlots):
    def __init__(self, vocab: WeakRef[VocabNote]) -> None:
        self.mnemonic: MutableStringField = MutableStringField(vocab, NoteFields.Vocab.user_mnemonic)
        self.answer: MutableStringField = MutableStringField(vocab, NoteFields.Vocab.user_answer)
        self.explanation: MutableStringField = MutableStringField(vocab, NoteFields.Vocab.user_explanation)
        self.explanation_long: MutableStringField = MutableStringField(vocab, NoteFields.Vocab.user_explanation_long)


    @override
    def __repr__(self) -> str: return f"Answer: {self.answer.value}, Mnemonic: {self.mnemonic.value}, Explanation: {self.explanation.value}, Explanation Long: {self.explanation_long.value}"
</file>

<file path="note/vocabulary/vocabnote.py">
from __future__ import annotations

from typing import TYPE_CHECKING, cast, override

from ankiutils import anki_module_import_issues_fix_just_import_this_module_before_any_other_anki_modules  # noqa  # pyright: ignore[reportUnusedImport]
from ex_autoslot import AutoSlots
from note.jpnote import JPNote
from note.note_constants import NoteFields
from note.notefields.comma_separated_strings_list_field import MutableCommaSeparatedStringsListField
from note.notefields.mutable_string_field import MutableStringField
from note.vocabnote_cloner import VocabCloner
from note.vocabulary import vocabnote_generated_data
from note.vocabulary.related_vocab.related_vocab import RelatedVocab
from note.vocabulary.vocabnote_audio import VocabNoteAudio
from note.vocabulary.vocabnote_conjugator import VocabNoteConjugator
from note.vocabulary.vocabnote_factory import VocabNoteFactory
from note.vocabulary.vocabnote_forms import VocabNoteForms
from note.vocabulary.vocabnote_kanji import VocabNoteKanji
from note.vocabulary.vocabnote_matching_rules import VocabNoteMatchingConfiguration
from note.vocabulary.vocabnote_metadata import VocabNoteMetaData
from note.vocabulary.vocabnote_parts_of_speech import VocabNotePartsOfSpeech
from note.vocabulary.vocabnote_question import VocabNoteQuestion
from note.vocabulary.vocabnote_sentences import VocabNoteSentences
from note.vocabulary.vocabnote_usercompoundparts import VocabNoteUserCompoundParts
from note.vocabulary.vocabnote_userfields import VocabNoteUserfields
from sysutils.weak_ref import WeakRef

if TYPE_CHECKING:
    from anki.notes import Note


class VocabNote(JPNote, AutoSlots):
    factory: VocabNoteFactory = VocabNoteFactory()
    def __init__(self, note: Note) -> None:
        super().__init__(note)
        self.weakref_vocab: WeakRef[VocabNote] = cast(WeakRef[VocabNote], self.weakref)

        self.question: VocabNoteQuestion = VocabNoteQuestion(self.weakref_vocab)

        self.readings: MutableCommaSeparatedStringsListField = MutableCommaSeparatedStringsListField(self.weakref, NoteFields.Vocab.Reading)

        self.user: VocabNoteUserfields = VocabNoteUserfields(self.weakref_vocab)

        self._source_answer: MutableStringField = MutableStringField(self.weakref, NoteFields.Vocab.source_answer)
        self.active_answer: MutableStringField = MutableStringField(self.weakref, NoteFields.Vocab.active_answer)

        self.cloner: VocabCloner = VocabCloner(self.weakref_vocab)
        self.related_notes: RelatedVocab = RelatedVocab(self.weakref_vocab)
        self.audio: VocabNoteAudio = VocabNoteAudio(self.weakref_vocab)
        self.sentences: VocabNoteSentences = VocabNoteSentences(self.weakref_vocab)
        self.forms: VocabNoteForms = VocabNoteForms(self.weakref_vocab)
        self.parts_of_speech: VocabNotePartsOfSpeech = VocabNotePartsOfSpeech(self.weakref_vocab)
        self.compound_parts: VocabNoteUserCompoundParts = VocabNoteUserCompoundParts(self.weakref_vocab)
        self.conjugator: VocabNoteConjugator = VocabNoteConjugator(self.weakref_vocab)
        self.kanji: VocabNoteKanji = VocabNoteKanji(self.weakref_vocab)
        self.meta_data: VocabNoteMetaData = VocabNoteMetaData(self.weakref_vocab)
        self.matching_configuration: VocabNoteMatchingConfiguration = VocabNoteMatchingConfiguration(self.weakref_vocab)

    @override
    def get_direct_dependencies(self) -> set[JPNote]:
        return self.related_notes.get_direct_dependencies()

    @override
    def update_generated_data(self) -> None:
        super().update_generated_data()
        vocabnote_generated_data.update_generated_data(self)

    def generate_and_set_answer(self) -> None:
        from language_services.jamdict_ex.dict_lookup import DictLookup
        dict_lookup = DictLookup.lookup_vocab_word_or_name(self)
        if dict_lookup.found_words():
            generated = dict_lookup.entries[0].generate_answer()
            self.user.answer.set(generated)

        self.update_generated_data()

    @override
    def get_answer(self) -> str:
        field = self.user.answer
        string_field = self._source_answer
        return field.value or string_field.value
</file>

<file path="qt_utils/ex_qmenu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from PyQt6.QtWidgets import QMenu


class ExQmenu(AutoSlots):
    @classmethod
    def disable_empty_submenus(cls, menu: QMenu | None) -> None:
        if menu is None:
            return

        for action in menu.actions():
            submenu: QMenu | None = action.menu()  # pyright: ignore[reportUnknownVariableType, reportUnknownMemberType]
            if submenu:
                cls.disable_empty_submenus(submenu)  # pyright: ignore[reportUnknownArgumentType]

                if len(submenu.actions()) == 0: # Disable submenu if it's empty  # pyright: ignore[reportUnknownArgumentType, reportUnknownMemberType]
                    action.setEnabled(False)
                else: # Disable submenu if all its actions are disabled
                    all_disabled = True
                    for submenu_action in submenu.actions():  # pyright: ignore[reportUnknownVariableType, reportUnknownMemberType]
                        if submenu_action.isEnabled():  # pyright: ignore[reportUnknownMemberType]
                            all_disabled = False
                            break
                    if all_disabled:
                        action.setEnabled(False)
</file>

<file path="qt_utils/task_runner_progress_dialog.py">
from __future__ import annotations

import time
from typing import TYPE_CHECKING, override

import mylog
from aqt import QLabel
from ex_autoslot import AutoSlots
from PyQt6.QtCore import Qt
from PyQt6.QtWidgets import QApplication, QProgressDialog
from sysutils import app_thread_pool, ex_thread, timeutil
from sysutils.timeutil import StopWatch
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from collections.abc import Callable

class ITaskRunner:
    def process_with_progress[TInput, TOutput](self, items: list[TInput], process_item: Callable[[TInput], TOutput], message: str) -> list[TOutput]: raise NotImplementedError()  # pyright: ignore
    def set_label_text(self, text: str) -> None: raise NotImplementedError()  # pyright: ignore
    def close(self) -> None: raise NotImplementedError()
    def run_on_background_thread_with_spinning_progress_dialog[TResult](self, message: str, action: Callable[[], TResult]) -> TResult: raise NotImplementedError()  # pyright: ignore

class TaskRunner(AutoSlots):
    @staticmethod
    def create(window_title: str, label_text: str, visible: bool) -> ITaskRunner:
        if not visible:
            return InvisibleTaskRunner(window_title, label_text)
        return QtTaskProgressRunner(window_title, label_text)

class InvisibleTaskRunner(ITaskRunner, AutoSlots):
    # noinspection PyUnusedLocal
    def __init__(self, window_title: str, label_text: str) -> None:  # pyright: ignore
        pass

    @override
    def process_with_progress[TInput, TOutput](self, items: list[TInput], process_item: Callable[[TInput], TOutput], message: str) -> list[TOutput]:
        result = [process_item(item) for item in items]
        total_items = len(items)
        watch = StopWatch()
        mylog.info(f"##--InvisibleTaskRunner--## Finished {message} in {watch.elapsed_formatted()} handled {total_items} items")
        return result
    @override
    def set_label_text(self, text: str) -> None: pass  # pyright: ignore
    @override
    def close(self) -> None: pass
    @override
    def run_on_background_thread_with_spinning_progress_dialog[TResult](self, message: str, action: Callable[[], TResult]) -> TResult:  # pyright: ignore
        watch = StopWatch()
        result = action()
        mylog.info(f"##--QtTaskProgressRunner--## Finished {message} in {watch.elapsed_formatted()}")
        return result

class QtTaskProgressRunner(ITaskRunner, AutoSlots):
    def __init__(self, window_title: str, label_text: str) -> None:
        dialog = QProgressDialog(f"""{window_title}...""", None, 0, 0)
        self.dialog: QProgressDialog = dialog
        dialog.setWindowTitle(f"""{window_title}""")
        dialog.setFixedWidth(600)
        non_optional(dialog.findChild(QLabel)).setAlignment(Qt.AlignmentFlag.AlignLeft | Qt.AlignmentFlag.AlignVCenter)
        dialog.setWindowModality(Qt.WindowModality.ApplicationModal)
        self._set_spinning_with_message(label_text)
        dialog.show()
        QApplication.processEvents()

    @override
    def set_label_text(self, text: str) -> None:
        self.dialog.setLabelText(text)
        QApplication.processEvents()

    def _set_spinning_with_message(self, message: str) -> None:
        self.dialog.setLabelText(message)
        self.dialog.setRange(0, 0)

    @override
    def run_on_background_thread_with_spinning_progress_dialog[TResult](self, message: str, action: Callable[[], TResult]) -> TResult:
        watch = StopWatch()
        self._set_spinning_with_message(message)

        future = app_thread_pool.pool.submit(action)

        while not future.done():
            QApplication.processEvents()
            ex_thread.sleep_thread_not_doing_the_current_work(0.05)

        mylog.info(f"##--QtTaskProgressRunner--## Finished {message} in {watch.elapsed_formatted()}")
        return future.result()

    @override
    def process_with_progress[TInput, TOutput](self, items: list[TInput], process_item: Callable[[TInput], TOutput], message: str) -> list[TOutput]:
        self.set_label_text(f"{message} 0 of ?? Remaining: ??")  # len may take a while so make sure we set the label first
        total_items = len(items)
        watch = StopWatch()
        start_time = time.time()
        results: list[TOutput] = []
        self.dialog.setRange(0, total_items + 1)  # add one to keep the dialog open
        original_label = self.dialog.labelText()
        self.set_label_text(f"{message} 0 of {total_items} Remaining: ??")

        last_refresh = 0.0

        for current_item, item in enumerate(items):
            results.append(process_item(item))
            if time.time() - last_refresh > 0.1 or current_item == total_items - 1:
                last_refresh = time.time()
                self.dialog.setValue(current_item + 1)
                elapsed_time = time.time() - start_time
                if current_item > 0:
                    estimated_total_time = (elapsed_time / current_item) * total_items
                    estimated_remaining_time = estimated_total_time - elapsed_time
                    self.set_label_text(f"{message} {current_item} of {total_items} Remaining: {timeutil.format_seconds_as_hh_mm_ss(estimated_remaining_time)}")

                QApplication.processEvents()

        mylog.info(f"##--QtTaskProgressRunner--## Finished {message} in {watch.elapsed_formatted()} handled {total_items} items")

        self.dialog.setRange(0, 0)
        self.set_label_text(original_label)
        QApplication.processEvents()

        return results
    @override
    def close(self) -> None:
        self.dialog.close()
        self.dialog.deleteLater()
</file>

<file path="sysutils/app_thread_pool.py">
from __future__ import annotations

from concurrent.futures.thread import ThreadPoolExecutor
from typing import TYPE_CHECKING

from ankiutils import app
from PyQt6.QtCore import QCoreApplication, QThread
from sysutils import ex_thread
from sysutils.typed import non_optional

if TYPE_CHECKING:

    from sysutils.standard_type_aliases import Action, Func

pool = ThreadPoolExecutor()

def current_is_ui_thread() -> bool:
    return bool(QCoreApplication.instance() and non_optional(QCoreApplication.instance()).thread() == QThread.currentThread())

def run_on_ui_thread_synchronously[T](func: Func[T]) -> T:
    if app.is_testing or current_is_ui_thread():
        return func()

    done_running: list[T] = []

    from aqt import mw
    mw.taskman.run_on_main(lambda: done_running.append(func()))

    while not len(done_running) > 0:
        ex_thread.sleep_thread_not_doing_the_current_work(0.001)

    return done_running[0]


def run_on_ui_thread_fire_and_forget(func: Action) -> None:
    if app.is_testing or current_is_ui_thread():
        func()

    return app.main_window().taskman.run_on_main(func)
</file>

<file path="sysutils/collections/default_dict_case_insensitive.py">
from __future__ import annotations

import collections
from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from sysutils import typed

if TYPE_CHECKING:
    from sysutils.standard_type_aliases import Func
    pass

class DefaultDictCaseInsensitive[VT](collections.defaultdict[str, VT], AutoSlots):
    def __init__(self, default_factory: Func[VT], **kwargs: object) -> None:
        super().__init__(default_factory, **{key.lower(): value for key, value in kwargs.items()})

    @override
    def __getitem__(self, key: str) -> VT:
        return super().__getitem__(key.lower())

    @override
    def __setitem__(self, key: str, value: VT) -> None:
        super().__setitem__(key.lower(), value)

    @override
    def __contains__(self, key: object) -> bool:
        return super().__contains__(typed.str_(key).lower())
</file>

<file path="sysutils/collections/recent_items.py">
from __future__ import annotations

from collections import deque

from ex_autoslot import AutoSlots


class RecentItems[T](AutoSlots):
    def __init__(self, max_size: int) -> None:
        self.items: deque[T] = deque(maxlen=max_size)
        self.max_size: int = max_size

    def is_recent(self, item: T) -> bool:
        seen = item in self.items
        if not seen: self.items.append(item)
        return seen
</file>

<file path="sysutils/debug_repr_builder.py">
from __future__ import annotations

from ex_autoslot import AutoSlots


class SkipFalsyValuesDebugReprBuilder(AutoSlots):
    def __init__(self) -> None:
        self.repr: str = ""

    def flag(self, name: str, value: bool) -> SkipFalsyValuesDebugReprBuilder:
        self.repr += f"{name} " if value else ""
        return self

    def include(self, value: object) -> SkipFalsyValuesDebugReprBuilder:
        self.repr += value.__repr__() + " | "
        return self

    def prop(self, name: str, value: object) -> SkipFalsyValuesDebugReprBuilder:
        self.repr += f"{name}: {value} " if value else ""
        return self
</file>

<file path="sysutils/ex_assert.py">
from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from queryablecollections._private_implementation_details.type_aliases import Func


def that(condition: bool, message: str | Func[str] = "assertion failed") -> None:
    if not condition:
        if callable(message): message = message()
        raise AssertionError(message)

def equal(value: object, expected:object, message: str = "assertion failed") -> None:
    that(value == expected, message)

def not_none(value:object, message:str = "") -> None:  # noqa: ANN401
    if not value: raise AssertionError(message)
</file>

<file path="sysutils/ex_gc.py">
from __future__ import annotations

import gc

import mylog
from ankiutils import app
from sysutils import app_thread_pool


def collect_on_on_ui_thread_if_collection_during_batches_enabled(display: bool = True) -> bool:
    if app.config().enable_garbage_collection_during_batches.get_value():
        if display:
            collect_on_ui_thread_and_display_message()
        else:
            app_thread_pool.run_on_ui_thread_synchronously(lambda: gc.collect())
        return True

    return False

def collect_on_ui_thread_and_display_message(message: str = "Garbage collecting") -> None:
    def collect_with_progress() -> None:
        mylog.info("collect_with_progress")
        import gc
        app.get_ui_utils().tool_tip(message, 6000)
        gc.collect()

    app_thread_pool.run_on_ui_thread_synchronously(collect_with_progress)
</file>

<file path="sysutils/ex_lambda.py">
from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from collections.abc import Callable

def bind1[TParam1, TResult](func: Callable[[TParam1], TResult], param: TParam1) -> Callable[[], TResult]:
    return lambda: func(param)

def bind2[TParam1, TParam2, TResult](func: Callable[[TParam1, TParam2], TResult], p1: TParam1, p2: TParam2) -> Callable[[], TResult]:
    return lambda: func(p1, p2)

def bind3[TParam1, TParam2, TParam3, TResult](func: Callable[[TParam1, TParam2, TParam3], TResult], p1: TParam1, p2: TParam2, p3: TParam3) -> Callable[[], TResult]:
    return lambda: func(p1, p2, p3)

def bind4[TParam1, TParam2, TParam3, TParam4, TResult](func: Callable[[TParam1, TParam2, TParam3, TParam4], TResult], p1: TParam1, p2: TParam2, p3: TParam3, p4: TParam4) -> Callable[[], TResult]:
    return lambda: func(p1, p2, p3, p4)
</file>

<file path="sysutils/ex_str.py">
from __future__ import annotations

import re

newline = "\n"
invisible_space = "​"
full_width_space = "　"

def pad_to_length(value: str, target_length: int, space_scaling: float = 1.0) -> str:
    padding = max(0, target_length - len(value))
    return value + " " * int(padding * space_scaling)

_commaSeparatedPattern = f"""[{''.join(map(re.escape, (",", "、")))}]"""
_commaSeparatedCompiled = re.compile(_commaSeparatedPattern)
def extract_comma_separated_values(string: str) -> list[str]:
    return [item for item in (item.strip() for item in _commaSeparatedCompiled.split(string.strip())) if item]

_html_bracket_pattern = re.compile('<.*?>|\[.*?\]') # noqa  # pyright: ignore[reportInvalidStringEscapeSequence] don't know what's going on here but it has been working for ages
def strip_html_and_bracket_markup(string: str) -> str:
    return _html_bracket_pattern.sub("", string)

def replace_html_and_bracket_markup_with(string: str, replacement:str) -> str:
    return _html_bracket_pattern.sub(replacement, string)

_html_pattern = re.compile("<.*?>|&nbsp;")
def strip_html_markup(string: str) -> str:
    return _html_pattern.sub("", string)

html_bracket_noise_pattern = re.compile('<.*?>|\[.*?\]|[〜]') # noqa  # pyright: ignore[reportInvalidStringEscapeSequence] don't know what's going on here but it has been working for ages
def strip_html_and_bracket_markup_and_noise_characters(string: str) -> str:
    return html_bracket_noise_pattern.sub("", string)

def strip_brackets(string:str) -> str:
    return string.replace("[","").replace("]","")

_first_number_pattern = re.compile(r"\d+")
def first_number(string:str) -> int:
    match = _first_number_pattern.search(string)
    assert match
    return int(match.group())

def replace_word(word:str, replacement:str, text:str) -> str:
    return re.sub(rf"\b{re.escape(word)}\b", replacement, text)
</file>

<file path="sysutils/ex_thread.py">
from __future__ import annotations

import time


def sleep_thread_not_doing_the_current_work(seconds: float) -> None:
    time.sleep(seconds)
</file>

<file path="sysutils/json/ex_json.py">
from __future__ import annotations

from typing import Any

from sysutils.json.json_library_shim_builtin import JsonLibraryShimBuiltInJson

#from sysutils.json.json_library_shim_orjson import JsonLibraryShimOrjson

json_library_shim = JsonLibraryShimBuiltInJson()
# To use orjson instead, uncomment the following line:
#_json_library_shim = JsonLibraryShimOrjson()


def dict_to_json(object_dict: dict[str, Any], indent: int | None = None) -> str:  # pyright: ignore[reportExplicitAny]
    return json_library_shim.dumps(object_dict, indent=indent)
</file>

<file path="sysutils/json/json_library_shim_builtin.py">
from __future__ import annotations

import json
from typing import Any, cast, override

from ex_autoslot import AutoSlots
from sysutils.json.json_library_shim import JsonLibraryShim


# noinspection PyUnusedClass,PyUnusedFunction
class JsonLibraryShimBuiltInJson(JsonLibraryShim, AutoSlots):
    @override
    def loads(self, json_str: str) -> dict[str, Any]: return cast(dict[str, Any], json.loads(json_str))  # pyright: ignore[reportExplicitAny]
    @override
    def dumps(self, object_dict: dict[str, Any], indent: int | None = None) -> str: return json.dumps(object_dict, indent=indent, ensure_ascii=False)  # pyright: ignore[reportExplicitAny]
</file>

<file path="sysutils/json/json_library_shim_orjson.py">
# from __future__ import annotations
#
# from typing import Any
#
# from ex_autoslot import ProfilableAutoSlots
# from sysutils.json.json_library_shim import JsonLibraryShim
#
#
# # noinspection PyUnusedClass,PyUnusedFunction
# class JsonLibraryShimOrjson(JsonLibraryShim, ProfilableAutoSlots):
#     def __init__(self) -> None:
#         import orjson
#
#         self.orjson = orjson
#
#     def loads(self, json_str: str) -> dict[str, Any]:
#         return self.orjson.loads(json_str)
#
#     def dumps(self, object_dict: dict[str, Any], indent: int | None = None) -> str:
#         options = 0
#         if indent is not None:
#             options = self.orjson.OPT_INDENT_2
#         return self.orjson.dumps(object_dict, option=options).decode("utf-8")
</file>

<file path="sysutils/json/json_library_shim.py">
from __future__ import annotations

from typing import Any

from ex_autoslot import AutoSlots


# noinspection PyUnusedFunction
class JsonLibraryShim(AutoSlots):
    def loads(self, json_str: str) -> dict[str, Any]: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter, reportExplicitAny]
    def dumps(self, object_dict: dict[str, Any], indent: int | None = None) -> str: raise NotImplementedError()  # pyright: ignore[reportUnusedParameter, reportExplicitAny]
</file>

<file path="sysutils/json/json_reader.py">
from __future__ import annotations

from typing import TYPE_CHECKING, Any, cast

from ex_autoslot import AutoSlots
from sysutils import typed
from sysutils.json.ex_json import json_library_shim

if TYPE_CHECKING:
    from collections.abc import Callable

    from sysutils.standard_type_aliases import Selector

class JsonReader(AutoSlots):
    def __init__(self, json_dict: dict[str, Any]) -> None:  # pyright: ignore[reportExplicitAny]
        self._dict: dict[str, Any] = json_dict  # pyright: ignore[reportExplicitAny]

    def _get_prop(self, prop: str | list[str], default: object | None) -> Any:  # noqa: ANN401  # pyright: ignore[reportExplicitAny, reportAny]
        if isinstance(prop, str):
            value = self._dict.get(prop, None)
            if value is None:
                if default is not None: return default
                raise KeyError(f"Property '{prop}' not found in the JSON.")
            return value  # pyright: ignore[reportAny]

        for current_property_name in prop:
            if current_property_name in self._dict:
                return self._dict[current_property_name]  # pyright: ignore[reportAny]

        if default is not None: return default
        raise KeyError(f"None of the following keys were found in the JSON: {prop}")

    def string(self, prop: str | list[str], default: str | None = None) -> str: return cast(str, self._get_prop(prop, default))
    def integer(self, prop: str | list[str], default: int | None = None) -> int: return cast(int, self._get_prop(prop, default))

    def string_list(self, prop: str | list[str], default: list[str] | None = None) -> list[str]:
        return cast(list[str], self._get_prop(prop, default))

    def string_set(self, prop: str | list[str], default: set[str] | None = None) -> set[str]: return set(self.string_list(prop, list(default) if default is not None else None))

    def object_list[TProp](self, prop: str | list[str], factory: Selector[JsonReader, TProp], default: list[TProp] | None = None) -> list[TProp]:
        prop_value = cast(list[dict[str, Any]], self._get_prop(prop, default))  # pyright: ignore[reportExplicitAny]
        reader_list = [JsonReader(json_dict) for json_dict in prop_value]
        return [factory(reader) for reader in reader_list]

    # noinspection PyUnusedFunction
    def object[TProp](self, prop: str, factory: Selector[JsonReader, TProp], default: Callable[[], TProp] | None = None) -> TProp | None:
        reader = self._reader_or_none(prop)
        return default() if reader is None and default is not None else factory(typed.non_optional(reader))

    def _reader_or_none(self, prop: str) -> JsonReader | None:
        dict_ = self._dict.get(prop, None)
        return None if dict_ is None else JsonReader(cast(dict[str, Any], dict_))  # pyright: ignore[reportExplicitAny]

    @classmethod
    def from_json(cls, json_str: str) -> JsonReader:
        return cls(json_library_shim.loads(json_str))
</file>

<file path="sysutils/kana_utils.py">
from __future__ import annotations

import pykakasi

# noinspection PyPackageRequirements
import romkan  # pyright: ignore[reportMissingTypeStubs]
from sysutils import typed
from sysutils.ex_str import full_width_space

# ruff: noqa: PLR2004


def pad_to_length(value: str, target_length: int) -> str:
    padding = max(0, target_length - len(value))
    return value + full_width_space * padding

def character_is_hiragana(char: str) -> bool:
    return 0x3040 <= ord(char) <= 0x309F

def character_is_katakana(char: str) -> bool:
    return 0x30A0 <= ord(char) <= 0x30FF

def character_is_kana(char: str) -> bool:
    return character_is_hiragana(char) or character_is_katakana(char)

def hiragana_to_katakana(hiragana: str) -> str:
    def char_to_katakana(char: str) -> str:
        return chr(ord(char) + 96) if character_is_hiragana(char) else char

    return "".join([char_to_katakana(char) for char in hiragana])

def katakana_to_hiragana(katakana: str) -> str:
    def char_to_hiragana(char: str) -> str:
        return chr(ord(char) - 96) if character_is_katakana(char) else char

    return "".join([char_to_hiragana(char) for char in katakana])

# from: https://www.darrenlester.com/blog/recognising-japanese-characters-with-javascript and http://www.rikai.com/library/kanjitables/kanji_codes.unicode.shtml
# CJK unifed ideographs - Common and uncommon kanji ( 4e00 - 9faf)
# CJK unified ideographs Extension A - Rare kanji ( 3400 - 4dbf)

def character_is_kanji(ch: str) -> bool:
    ordinal = ord(ch)
    return (0x4E00 <= ordinal <= 0x9FAF or
            0x3400 <= ordinal <= 0x4DBF)

def contains_kanji(string: str) -> bool:
    return any(character_is_kanji(c) for c in string)

# def is_kanji(char) -> bool:
#     ordinal = ord(char)
#     return (0x4e00 <= ordinal <= 0x9faf or
#             0x3400 <= ordinal <= 0x4dbf or
#             0x20000 <= ordinal <= 0x2a6df or
#             0x2a700 <= ordinal <= 0x2b73f or
#             0x2b740 <= ordinal <= 0x2b81f or
#             0x2b820 <= ordinal <= 0x2ceaf)

def is_only_kana(text: str) -> bool:
    return not any(not character_is_kana(char) for char in text)

def is_only_hiragana(text: str) -> bool:
    return not any(not character_is_hiragana(char) for char in text)

def is_only_katakana(text: str) -> bool:
    return not any(not character_is_katakana(char) for char in text)

_kakasi = pykakasi.kakasi()  # type: ignore
def romanize(text:str) -> str:
    if text == "": return ""
    if text[-1] == "っ" or text[-1] == "ッ":
        text = text[:-1]

    result = _kakasi.convert(text)
    return "".join([item["hepburn"] for item in result])

def romaji_to_hiragana(string:str) -> str:
    return typed.str_(romkan.to_hiragana(string))  # pyright: ignore[reportUnknownMemberType]

def romaji_to_katakana(string:str) -> str:
    return typed.str_(romkan.to_katakana(string))  # pyright: ignore[reportUnknownMemberType]

def anything_to_hiragana(string:str) -> str:
    return katakana_to_hiragana(string) if is_only_kana(string) else romaji_to_hiragana(string)

def extract_kanji(string:str) -> list[str]:
    return [char for char in string if character_is_kanji(char)]
</file>

<file path="sysutils/lazy.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots

if TYPE_CHECKING:

    from sysutils.standard_type_aliases import Func

class Lazy[T](AutoSlots):
    def __init__(self, factory: Func[T]) -> None:
        self.factory: Func[T] = factory
        self._instance: T | None = None

    def _lazy_get_instance(self) -> T:
        if self._instance is None:
            self._instance = self.factory()
        return self._instance

    def __call__(self) -> T: return self._lazy_get_instance()

    @staticmethod
    def from_value(result: T) -> Lazy[T]:
        return Lazy[T](lambda: result)

    def reset(self) -> None:
        self._instance = None
</file>

<file path="sysutils/object_instance_tracker.py">
from __future__ import annotations

import sys

from ankiutils import app
from ex_autoslot import AutoSlots
from sysutils import ex_gc
from sysutils.ex_str import newline
from sysutils.timeutil import StopWatch

current_instance_count: dict[str, int] = {}

class Snapshot(AutoSlots):
    def __init__(self, current_state: dict[str, int], previous_state: dict[str, int]) -> None:
        self.previous: dict[str, int] = previous_state
        self.current_counts: dict[str, int] = current_state

    def single_line_diff_report(self) -> str:
        diffs: dict[str, int] = {}
        for type_name, count in self.current_counts.items():
            change = count - self.previous.get(type_name, 0)
            if change != 0:
                diffs[type_name] = change
        return f"""{newline.join([f"{type_name.split('.')[-1]}:{diff}" for type_name, diff in diffs.items()])}""" if diffs else "No changes"

snapshots: list[Snapshot] = []

def take_snapshot() -> Snapshot:
    diff = create_transient_snapshot_against_last_snapshot()
    snapshots.append(diff)
    return diff

def create_transient_snapshot_against_last_snapshot() -> Snapshot:
    current_state = current_instance_count.copy()
    previous_state = snapshots[-1].current_counts if len(snapshots) > 0 else {}
    return Snapshot(current_state, previous_state)

def create_transient_snapshot_against_first_snapshot() -> Snapshot:
    current_state = current_instance_count.copy()
    previous_state = snapshots[0].current_counts if len(snapshots) > 0 else {}
    return Snapshot(current_state, previous_state)

def current_snapshot() -> Snapshot:
    if len(snapshots) == 0:
        take_snapshot()
    return snapshots[-1]

class ObjectInstanceTracker(AutoSlots):
    _track_instances_in_memory: bool = app.config().track_instances_in_memory.get_value()
    def __init__(self, cls_type: type[object]) -> None:
        self.type_name: str = self._get_fully_qualified_name(cls_type)
        current_instance_count[self.type_name] = current_instance_count.get(self.type_name, 0) + 1

    def run_gc_if_multiple_instances_and_assert_single_instance_after_gc(self) -> None:
        with StopWatch.log_execution_time():
            if ex_gc.collect_on_on_ui_thread_if_collection_during_batches_enabled(display=False):  # noqa: SIM102
                if not current_instance_count[self.type_name] == 1: raise Exception(f"Expected single instance of {self.type_name}, found {current_instance_count[self.type_name]}")

    def __del__(self) -> None:
        current_instance_count[self.type_name] -= 1

    @staticmethod
    def _get_fully_qualified_name(cls_type: type[object]) -> str:
        if cls_type.__module__ != "__builtin__":
            return sys.intern(cls_type.__module__ + "." + cls_type.__qualname__)
        return sys.intern(cls_type.__qualname__)

    # noinspection PyUnusedFunction
    @staticmethod
    def configured_tracker_for(obj: object) -> object | None: return ObjectInstanceTracker(obj.__class__) if ObjectInstanceTracker._track_instances_in_memory else None

    @staticmethod
    def tracker_for(obj: object) -> ObjectInstanceTracker: return ObjectInstanceTracker(obj.__class__)

def print_instance_counts() -> None:
    print("################### Instance counts ###################")
    for type_name, count in current_instance_count.items():
        print(f"{type_name}: {count}")

def single_line_report() -> str:
    return f"""{', '.join([f"{type_name.split('.')[-1]}:{count}" for type_name, count in current_instance_count.items()])}"""
</file>

<file path="sysutils/progress_display_runner.py">
from __future__ import annotations

import time
from typing import TYPE_CHECKING

from PyQt6.QtCore import Qt
from PyQt6.QtWidgets import QApplication, QProgressDialog
from sysutils import timeutil

if TYPE_CHECKING:
    from sysutils.standard_type_aliases import Action1

def process_with_progress[T](items: list[T], process_item: Action1[T], message: str, allow_cancel: bool = True, display_delay_seconds: float = 0.0) -> None:
    total_items = len(items)
    start_time = time.time()
    progress_dialog: QProgressDialog | None = None
    last_refresh = 0.0

    try:
        for current_item, item in enumerate(items):
            if not progress_dialog and (time.time() - start_time >= display_delay_seconds):
                progress_dialog = QProgressDialog(f"""{message}...""", "Cancel" if allow_cancel else None, 0, total_items)
                progress_dialog.setWindowTitle(f"""{message}""")
                progress_dialog.setWindowModality(Qt.WindowModality.ApplicationModal)
                progress_dialog.show()

            if progress_dialog and progress_dialog.wasCanceled(): break
            process_item(item)
            if time.time() - last_refresh > 0.1 or current_item == total_items - 1:
                last_refresh = time.time()
                if progress_dialog: progress_dialog.setValue(current_item + 1)
                elapsed_time = time.time() - start_time
                if current_item > 0:
                    estimated_total_time = (elapsed_time / current_item) * total_items
                    estimated_remaining_time = estimated_total_time - elapsed_time
                    if progress_dialog: progress_dialog.setLabelText(f"{message} {current_item} of {total_items} Remaining: {timeutil.format_seconds_as_hh_mm_ss(estimated_remaining_time)}")

                QApplication.processEvents()
    finally:
        if progress_dialog: progress_dialog.close()
</file>

<file path="sysutils/simple_string_builder.py">
from __future__ import annotations

from ex_autoslot import AutoSlots
from sysutils.simple_string_list_builder import SimpleStringListBuilder


class SimpleStringBuilder(AutoSlots):
    def __init__(self, auto_separator: str = "") -> None:
        self._builder: SimpleStringListBuilder = SimpleStringListBuilder()
        self.auto_separator: str = auto_separator

    def append(self, text: str) -> SimpleStringBuilder:
        return self.append_if(True, text)

    def append_if(self, condition: bool, text: str) -> SimpleStringBuilder:
        self._builder.append_if(condition, text)
        return self

    def build(self) -> str:
        return self.auto_separator.join(self._builder.value)
</file>

<file path="sysutils/simple_string_list_builder.py">
from __future__ import annotations

from ex_autoslot import AutoSlots


class SimpleStringListBuilder(AutoSlots):
    def __init__(self) -> None:
        self.value: list[str] = []

    def append(self, text: str) -> SimpleStringListBuilder:
        return self.append_if(True, text)

    def append_if(self, condition: bool, text: str) -> SimpleStringListBuilder:
        if condition: self.value.append(text)
        return self

    def concat(self, values: list[str]) -> SimpleStringListBuilder:
        self.value.extend(values)
        return self
</file>

<file path="sysutils/standard_type_aliases.py">
from __future__ import annotations

from collections.abc import Callable

type Action = Callable[[], None]
type Action1[TIn] = Callable[[TIn], None]
type Action2[TIn, TIn2] = Callable[[TIn, TIn2], None]
type Action3[TIn, TIn2, TIn3] = Callable[[TIn, TIn2, TIn3], None]
type Action4[TIn, TIn2, TIn3, TIn4] = Callable[[TIn, TIn2, TIn3, TIn4], None]

type Func[TOut] = Callable[[], TOut]
type Func1[TIn, TOut] = Callable[[TIn], TOut]
type Func2[TIn, TIn2, TOut] = Callable[[TIn, TIn2], TOut]
type Func3[TIn, TIn2, TIn3, TOut] = Callable[[TIn, TIn2, TIn3], TOut]
type Func4[TIn, TIn2, TIn3, TIn4, TOut] = Callable[[TIn, TIn2, TIn3, TIn4], TOut]


type Predicate[TIn] = Callable[[TIn], bool]
type Selector[TIn, TOut] = Callable[[TIn], TOut]
</file>

<file path="sysutils/timeutil.py">
from __future__ import annotations

import sys
import time
from contextlib import contextmanager
from typing import TYPE_CHECKING

import mylog
from ex_autoslot import AutoSlots
from sysutils import typed

if TYPE_CHECKING:
    from collections.abc import Iterator

    from sysutils.standard_type_aliases import Action

SECONDS_PER_DAY = 24 * 60 * 60
MILLISECONDS_PER_SECOND = 1000

def time_execution(callback: Action) -> str:
    start_time = time.time()
    callback()
    elapsed_time = time.time() - start_time
    seconds = int(elapsed_time)
    milliseconds = int((elapsed_time - seconds) * 1000)
    return f"{seconds}:{milliseconds:03}"

def format_seconds_as_hh_mm_ss(seconds: float) -> str:
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    sec = int(seconds % 60)
    return f"{hours:02}:{minutes:02}:{sec:02}"

def format_seconds_as_ss_ttt_ttt(seconds: float) -> str:
    total_milliseconds = int(seconds * 1000)  # Convert seconds to milliseconds
    seconds_part = total_milliseconds // 1000  # Extract the integer part of seconds
    milliseconds_part = total_milliseconds % 1000  # Extract the milliseconds part
    microseconds_part = int((seconds - seconds_part) * 1_000_000)  # Calculate microseconds

    return f"{seconds_part}.{milliseconds_part:03d} {microseconds_part:03d}"

# noinspection PyUnusedFunction
def start_stop_watch() -> StopWatch:
    return StopWatch()

# noinspection PyUnusedFunction
class StopWatch(AutoSlots):
    def __init__(self) -> None:
        self.start_time: float = time.perf_counter()

    def elapsed_seconds(self) -> float:
        return time.perf_counter() - self.start_time

    def elapsed_formatted(self) -> str:
        return format_seconds_as_ss_ttt_ttt(time.perf_counter() - self.start_time)

    @staticmethod
    @contextmanager
    def log_warning_if_slower_than(warn_if_slower_than:float, message:str = "") -> Iterator[None]:
        # noinspection DuplicatedCode
        watch = StopWatch()

        def get_caller_info() -> str:
            caller_frame = sys._getframe(4) # noqa  # pyright: ignore[reportPrivateUsage]
            module_name = typed.str_(caller_frame.f_globals["__name__"])  # pyright: ignore[reportAny]
            function_name = caller_frame.f_code.co_name
            return f"{module_name}..{function_name}"

        def get_message() -> str:
            return f"############## Execution time:{watch.elapsed_formatted()} for {get_caller_info()}#{message} ##############"
        try:
            yield
        finally:
            elapsed_seconds = watch.elapsed_seconds()
            if elapsed_seconds > warn_if_slower_than:
                mylog.warning(get_message())
            elif elapsed_seconds * 2 > warn_if_slower_than:
                mylog.info(get_message())

    @staticmethod
    @contextmanager
    def log_execution_time(message:str = "") -> Iterator[None]:
        # noinspection DuplicatedCode
        watch = StopWatch()

        def get_caller_info() -> str:
            caller_frame = sys._getframe(4) # noqa  # pyright: ignore[reportPrivateUsage]
            module_name:str = typed.str_(caller_frame.f_globals["__name__"])  # pyright: ignore[reportAny]
            function_name = caller_frame.f_code.co_name
            return f"{module_name}..{function_name}"

        def get_message() -> str:
            return f"############## Execution time:{watch.elapsed_formatted()} for {get_caller_info()}#{message} ##############"
        try:
            yield
        finally:
            mylog.info(get_message())
</file>

<file path="sysutils/typed.py">
from __future__ import annotations

from typing import cast, get_origin

from beartype.door import is_bearable


def str_(value: object) -> str: return checked_cast(str, value)  # noqa: ANN401

def int_(value: object) -> int: return checked_cast(int, value)  # noqa: ANN401

def float_(value: object) -> float: return checked_cast(float, value)  # noqa: ANN401

def bool_(value: object) -> bool: return checked_cast(bool, value)  # noqa: ANN401

def checked_cast[CastT](cls: type[CastT], instance: object) -> CastT:
    """ Runtime-check an object for simple built in types and return it cast as such """
    if not isinstance(instance, cls):
        raise TypeError(f"{repr(instance)}: expected {cls.__name__}, not {instance.__class__.__name__}")
    return instance

def checked_cast_generics[CastT](cls: type[CastT], instance: object) -> CastT:
    """ Runtime-check an object for a specific generic type and return it cast as such """
    if not is_bearable(instance, cls):
        msg = f"{repr(instance)}: expected {cls.__name__}, not {instance.__class__.__name__}"  # pyright: ignore[reportUnreachable]
        raise TypeError(msg)

    return cast(CastT, instance)

def _is_compatible_with_isinstance[CastT](cls: type[CastT]) -> bool:
    return cls.__module__ == "builtins" and get_origin(cls) is None

def checked_cast_dynamic[CastT](cls: type[CastT], instance: object) -> CastT:
    """ Runtime-check an object for a specific type and return it cast as such """
    return checked_cast(cls, instance) if _is_compatible_with_isinstance(cls) else checked_cast_generics(cls, instance)

def non_optional[CastT](instance: CastT | None) -> CastT:
    if instance is None: raise AssertionError()
    return instance

def try_cast[CastT](cls: type[CastT], instance: object) -> CastT | None:
    return instance if isinstance(instance, cls) else None
</file>

<file path="sysutils/weak_ref.py">
from __future__ import annotations

import weakref
from typing import TYPE_CHECKING, Generic, TypeVar, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from _weakref import ReferenceType

T = TypeVar("T", covariant=True)

class WeakRef(Generic[T], AutoSlots):  # noqa: UP046 the automatic inference thinks this in invariant even though it is covariant so we need the old syntax
    def __init__(self, obj: T) -> None:
        self._weakreference: ReferenceType[T] = weakref.ref(obj)

    def _weakref_get_instance(self) -> T:
        instance = self._weakreference()
        if instance is None: raise ReferenceError("This WeakRef instance has been destroyed by the reference counting in python. No GC roots referencing it remain.")
        return instance

    def __call__(self) -> T: return self._weakref_get_instance()

    @override
    def __repr__(self) -> str: return f"WeakRef: {self().__repr__()}"

class WeakRefable(AutoSlots):
    __slots__ = ["__weakref__"]  # pyright: ignore[reportUninitializedInstanceVariable, reportUnannotatedClassAttribute]
</file>

<file path="testutils/ex_pytest.py">
from __future__ import annotations

import sys

is_testing = "pytest" in sys.modules
</file>

<file path="ui/__init__.py">
from __future__ import annotations


def init() -> None:
    from ui import garbage_collection_fixes, hooks, menus, timing_hacks, tools_menu, web
    hooks.init()
    timing_hacks.init()
    tools_menu.init()
    web.init()
    menus.init()
    garbage_collection_fixes.init()
</file>

<file path="ui/english_dict/find_english_words_dialog.py">
from __future__ import annotations

import threading
from typing import TYPE_CHECKING, final

from aqt.utils import openLink
from language_services.english_dictionary import english_dict_search
from PyQt6.QtCore import Qt, pyqtBoundSignal
from PyQt6.QtWidgets import QApplication, QDialog, QHBoxLayout, QHeaderView, QLabel, QLineEdit, QTableWidget, QTableWidgetItem, QVBoxLayout, QWidget
from sysutils import typed
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from language_services.english_dictionary.english_dict_search import EnglishWord
@final
class EnglishWordSearchDialog(QDialog): # Cannot inherit Slots for some QT internal reason
    # Singleton instance
    _instance: EnglishWordSearchDialog | None = None

    @classmethod
    def instance(cls) -> EnglishWordSearchDialog:
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self, parent: QWidget | None = None) -> None:
        super().__init__(parent)
        self.setWindowTitle("Find English Words")
        self.resize(800, 600)

        # Set the window to be always on top but not modal
        self.setWindowFlags(self.windowFlags() | Qt.WindowType.WindowStaysOnTopHint)

        self._lock = threading.Lock()
        self._max_results = 100  # Maximum number of results to show

        layout = QVBoxLayout(self)

        # Create search input field
        search_layout = QHBoxLayout()
        search_label = QLabel("Search:")
        self.search_input = QLineEdit()
        self.search_input.setPlaceholderText("Enter the beginning of an English word")
        search_layout.addWidget(search_label)
        search_layout.addWidget(self.search_input)
        layout.addLayout(search_layout)

        # Create results table
        self.results_table = QTableWidget()
        self.results_table.setColumnCount(2)  # Two columns: word and definition
        self.results_table.setHorizontalHeaderLabels(["Word", "Definition"])  # pyright: ignore[reportUnknownMemberType]
        self.results_table.setEditTriggers(QTableWidget.EditTrigger.NoEditTriggers)
        self.results_table.setSelectionBehavior(QTableWidget.SelectionBehavior.SelectRows)
        self.results_table.setSelectionMode(QTableWidget.SelectionMode.SingleSelection)

        # Configure column widths
        header = non_optional(self.results_table.horizontalHeader())
        header.setSectionResizeMode(0, QHeaderView.ResizeMode.ResizeToContents)
        header.setSectionResizeMode(1, QHeaderView.ResizeMode.Stretch)

        layout.addWidget(self.results_table)

        typed.checked_cast(pyqtBoundSignal, self.search_input.textChanged).connect(self.perform_search)  # pyright: ignore[reportUnknownMemberType]
        typed.checked_cast(pyqtBoundSignal, self.results_table.cellDoubleClicked).connect(self.on_cell_double_clicked)  # pyright: ignore[reportUnknownMemberType]

        self.search_input.setFocus()

    def perform_search(self) -> None:
        with self._lock:
            search_text = self.search_input.text().strip()
            if not search_text:
                self.results_table.setRowCount(0)
                return

            matching_words: list[EnglishWord] = english_dict_search.dictionary().words_containing_starting_with_first_then_by_shortest_first(search_text)
            matching_words = matching_words[:self._max_results]

            self._update_results_table(matching_words)

    def _update_results_table(self, words: list[english_dict_search.EnglishWord]) -> None:
        self.results_table.setRowCount(0)  # Clear the table
        self.results_table.setRowCount(len(words))

        for row, word in enumerate(words):
            # Word item
            word_item = QTableWidgetItem(word.word)
            word_item.setData(Qt.ItemDataRole.UserRole, word.word)  # Store the word for double-click
            self.results_table.setItem(row, 0, word_item)

            # Definition item
            definition_item = QTableWidgetItem(word.senses[0].definition)
            self.results_table.setItem(row, 1, definition_item)

    def on_cell_double_clicked(self, row: int, _column: int) -> None:
        word_item = self.results_table.item(row, 0)
        if word_item:
            selected_word = typed.str_(word_item.data(Qt.ItemDataRole.UserRole))  # pyright: ignore[reportAny]
            if selected_word:
                if QApplication.keyboardModifiers() & Qt.KeyboardModifier.ControlModifier:
                    openLink(f"https://www.merriam-webster.com/dictionary/{selected_word}")
                elif QApplication.keyboardModifiers() & Qt.KeyboardModifier.ShiftModifier:
                    openLink(f"https://www.google.com/search?q=define+{selected_word}")
                else:
                    openLink(f"https://www.oed.com/search/dictionary/?scope=Entries&q={selected_word}")

    @classmethod
    def toggle_dialog_visibility(cls) -> None:
        if cls.instance().isVisible():
            cls.instance().hide()
            return

        cls.instance().show()
        cls.instance().raise_()
        cls.instance().activateWindow()
        cls.instance().search_input.setFocus()
</file>

<file path="ui/garbage_collection_fixes.py">
from __future__ import annotations

from typing import TYPE_CHECKING

import aqt
from ankiutils import app
from PyQt6.QtCore import pyqtBoundSignal
from sysutils import ex_gc
from sysutils.typed import checked_cast

if TYPE_CHECKING:
    from aqt.main import AnkiQt
    from PyQt6.QtWidgets import QDialog

def noop_gc_on_dialog_finish(_self: AnkiQt, dialog: QDialog) -> None:
    # Fix anki hanging for several seconds every time a window closes
    checked_cast(pyqtBoundSignal, dialog.finished).connect(dialog.deleteLater)  # pyright: ignore[reportUnknownMemberType]
    app.get_ui_utils().tool_tip("prevented garbage collection", 6000)

def visible_garbage_collection(_self: AnkiQt) -> None:
    if (not app.config().enable_automatic_garbage_collection.get_value()
            and app.is_initialized()):
        ex_gc.collect_on_ui_thread_and_display_message("Garbage collection triggered by anki internal code")

def init() -> None:
    if app.config().prevent_anki_from_garbage_collecting_every_time_a_window_closes.get_value():
        aqt.main.AnkiQt.garbage_collect_on_dialog_finish = noop_gc_on_dialog_finish  # type: ignore  # pyright: ignore[reportAttributeAccessIssue, reportUnknownMemberType]
    aqt.main.AnkiQt.garbage_collect_now = visible_garbage_collection  # type: ignore  # pyright: ignore[reportAttributeAccessIssue, reportUnknownMemberType]
</file>

<file path="ui/hooks/__init__.py">
from __future__ import annotations

from ui.hooks import clear_studying_cache_on_card_suspend_unsuspend, copy_sort_field_to_clipboard, custom_auto_advance_timings, custom_short_term_scheduling, custom_timebox_lengths, global_shortcuts, history_navigator, no_accidental_double_click, note_view_shortcuts, timebox_end_sound


def init() -> None:
    clear_studying_cache_on_card_suspend_unsuspend.init()
    copy_sort_field_to_clipboard.init()
    custom_auto_advance_timings.init()
    custom_short_term_scheduling.init()
    custom_timebox_lengths.init()
    history_navigator.init()
    no_accidental_double_click.init()
    timebox_end_sound.init()
    note_view_shortcuts.init()
    global_shortcuts.init()
</file>

<file path="ui/hooks/clear_studying_cache_on_card_suspend_unsuspend.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from aqt import gui_hooks
from note import noteutils

if TYPE_CHECKING:
    from collections.abc import Sequence

    from anki.cards import CardId
    from anki.collection import OpChanges, OpChangesWithCount


def _monkey_patch(html:str, _card:object, _something_else_again:object) -> str:
    def remove_cards_from_cache(ids: Sequence[CardId]) -> None:
        cards = [app.anki_collection().get_card(card_id) for card_id in ids]
        for card in cards:
            noteutils.remove_from_studying_cache(card.note().id)

    def _monkey_patched_suspend_cards(ids: Sequence[CardId]) -> OpChangesWithCount:
        remove_cards_from_cache(ids)
        return _real_suspend_cards(ids)

    def _monkey_patched_unsuspend_cards(ids: Sequence[CardId]) -> OpChanges:
        remove_cards_from_cache(ids)
        return _real_unsuspend_cards(ids)

    scheduler = app.anki_scheduler()

    #todo this silliness is because I patch the instance instead of the class...
    if not hasattr(scheduler, "is_patched_by_magnus_addon_for_suspend"):
        _real_suspend_cards = scheduler.suspend_cards
        _real_unsuspend_cards = scheduler.unsuspend_cards

        scheduler.suspend_cards = _monkey_patched_suspend_cards  # type: ignore
        scheduler.unsuspend_cards = _monkey_patched_unsuspend_cards  # type: ignore
        scheduler.is_patched_by_magnus_addon_for_suspend = True  # type: ignore  # pyright: ignore[reportAttributeAccessIssue]

    return html




def init() -> None:
    gui_hooks.card_will_show.append(_monkey_patch) #should not be needed, but for some reason fixes a bug where the cache is not updated, my best guess is the scheduler is replaced for some reason. It ONLY happens with my personal profile, not the development profile...
</file>

<file path="ui/hooks/copy_sort_field_to_clipboard.py">
from __future__ import annotations

from typing import TYPE_CHECKING, cast

import pyperclip
from anki.models import NotetypeDict
from ankiutils import app
from ankiutils.app import get_ui_utils
from aqt import gui_hooks
from sysutils import app_thread_pool, ex_str, typed
from sysutils.collections.recent_items import RecentItems
from sysutils.timeutil import StopWatch

if TYPE_CHECKING:
    from anki.cards import Card
    from anki.notes import Note

def copy_card_sort_field_to_clipboard(note: Note) -> None:
    with StopWatch.log_warning_if_slower_than(0.01):
        if app.config().yomitan_integration_copy_answer_to_clipboard.get_value():
            model = cast(NotetypeDict, note.note_type())
            sort_field:int = typed.int_(model["sortf"])  # pyright: ignore[reportAny]
            sort_value = typed.str_(note.fields[sort_field])
            clean_string = ex_str.strip_html_and_bracket_markup_and_noise_characters(sort_value)
            app_thread_pool.pool.submit(lambda: pyperclip.copy(clean_string))

recent_review_answers = RecentItems[int](1)

def on_reviewer_show_answer(card: Card) -> None:
    note = card.note()
    if get_ui_utils().is_edit_current_open() or recent_review_answers.is_recent(note.id):
        return

    copy_card_sort_field_to_clipboard(note)

def init() -> None:
    gui_hooks.reviewer_did_show_answer.append(on_reviewer_show_answer)
</file>

<file path="ui/hooks/custom_auto_advance_timings.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from anki_extentions.card_ex import CardEx
from ankiutils import app, ui_utils
from aqt import gui_hooks, mw
from aqt.reviewer import Reviewer
from note.difficulty_calculator import DifficultyCalculator
from note.jpnote import JPNote
from note.kanjinote import KanjiNote
from note.note_constants import CardTypes
from note.sentences.sentencenote import SentenceNote
from note.vocabulary.vocabnote import VocabNote
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from anki.cards import Card

_real_auto_advance_to_answer_if_enabled = Reviewer._auto_advance_to_answer_if_enabled  # noqa  # pyright: ignore[reportPrivateUsage]

def is_handled_card(card: CardEx) -> bool:
    if card.type().name != CardTypes.reading:
        return False

    note = JPNote.note_from_card(non_optional(mw.reviewer.card))
    return isinstance(note, SentenceNote | VocabNote | KanjiNote)

def seconds_to_show_question(card: CardEx) -> float:
    note = card.note()

    def default_time_allowed() -> float:
        if isinstance(note, SentenceNote):
            return DifficultyCalculator(starting_seconds=app.config().autoadvance_sentence_starting_seconds.get_value(),
                                        hiragana_seconds=app.config().autoadvance_sentence_hiragana_seconds.get_value(),
                                        katakata_seconds=app.config().autoadvance_sentence_katakana_seconds.get_value(),
                                        kanji_seconds=app.config().autoadvance_sentence_kanji_seconds.get_value()).allowed_seconds(note.get_question())
        if isinstance(note, VocabNote):
            return DifficultyCalculator(starting_seconds=app.config().autoadvance_vocab_starting_seconds.get_value(),
                                        hiragana_seconds=app.config().autoadvance_vocab_hiragana_seconds.get_value(),
                                        katakata_seconds=app.config().autoadvance_vocab_katakana_seconds.get_value(),
                                        kanji_seconds=app.config().autoadvance_vocab_kanji_seconds.get_value()).allowed_seconds(note.get_question())
        if isinstance(note, KanjiNote):
            return card.get_deck().get_config().get_seconds_to_show_question()
        raise Exception("We should never get here")

    time_allowed = default_time_allowed()
    if app.config().boost_failed_card_allowed_time.get_value():
        sequential_failures = card.sequential_again_answers_today()
        if sequential_failures > 0:
            config_bostfactor = app.config().boost_failed_card_allowed_time_by_factor.get_value()
            multiplier = config_bostfactor ** sequential_failures

            time_allowed *= multiplier

    return time_allowed

def _auto_advance_to_answer_if_enabled(reviewer: Reviewer) -> None:
    if not app.is_initialized():
        return

    card = CardEx(non_optional(mw.reviewer.card))
    if not is_handled_card(card):
        _real_auto_advance_to_answer_if_enabled(reviewer)
        return

    mw.reviewer._clear_auto_advance_timers()  # noqa  # pyright: ignore[reportPrivateUsage]
    allowed_milliseconds = int(seconds_to_show_question(card) * 1000)

    mw.reviewer._show_answer_timer = mw.reviewer.mw.progress.timer(  # pyright: ignore[reportUnknownMemberType, reportPrivateUsage]
        allowed_milliseconds,
        mw.reviewer._on_show_answer_timeout,  # noqa  # pyright: ignore[reportPrivateUsage]
        repeat=False,
        parent=mw.reviewer.mw,
    )

def _auto_start_auto_advance(html: str, anki_card: Card, _display_type: str) -> str:
    if (app.is_initialized()
            and ui_utils.is_displaytype_displaying_review_question(_display_type)
            and is_handled_card(CardEx(anki_card))
            and not mw.reviewer.auto_advance_enabled):
        mw.reviewer.toggle_auto_advance()

    return html

def init() -> None:
    Reviewer._auto_advance_to_answer_if_enabled = _auto_advance_to_answer_if_enabled # type: ignore  # pyright: ignore[reportAttributeAccessIssue, reportPrivateUsage]
    gui_hooks.card_will_show.append(_auto_start_auto_advance)
</file>

<file path="ui/hooks/custom_short_term_scheduling.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from anki_extentions.card_ex import Card2Ex
from anki_extentions.sheduling_states_ex import SchedulingStatesEx
from ankiutils import app
from aqt.reviewer import V3CardInfo
from sysutils.timeutil import StopWatch

if TYPE_CHECKING:
    from anki.scheduler.v3 import QueuedCards  # pyright: ignore[reportMissingTypeStubs]

_oldMethod = V3CardInfo.from_queue
def set_again_time_for_previously_failed_today_cards(queue:QueuedCards) -> V3CardInfo:
    with StopWatch.log_warning_if_slower_than(0.01):
        info = _oldMethod(queue)

        if app.is_initialized() and app.config().decrease_failed_card_intervals.get_value():
            card = Card2Ex(info.top_card().card)

            if card.last_answer_today_was_fail_db_call():
                SchedulingStatesEx(info.states).again.set_seconds(app.config().decrease_failed_card_intervals_interval.get_value())

        return info

def init() -> None:
    V3CardInfo.from_queue = set_again_time_for_previously_failed_today_cards  # type: ignore  # pyright: ignore[reportAttributeAccessIssue]
</file>

<file path="ui/hooks/custom_timebox_lengths.py">
from __future__ import annotations

from typing import TYPE_CHECKING

import aqt.utils
from anki_extentions.card_ex import CardEx
from anki_extentions.deck_ex import DeckEx
from ankiutils import app
from aqt import gui_hooks
from aqt.overview import Overview
from note.note_constants import CardTypes, NoteTypes
from sysutils import typed

if TYPE_CHECKING:
    from anki.cards import CardId
    from aqt.webview import WebContent


def adjust_timebox(_web_content: WebContent, context: object) -> None:
    if isinstance(context, Overview):
        deck = DeckEx(app.anki_collection().decks.current())
        deck_card_ids = app.anki_collection().find_cards(f"deck:{deck.name}")
        if deck_card_ids:
            first_card_id: CardId = deck_card_ids[0]

            card = CardEx(app.anki_collection().get_card(first_card_id))
            note = card.note()
            notetype = note.get_type()

            card_notetype_timebox_minutes: dict[tuple[str, str], int] = {
                (NoteTypes.Sentence, CardTypes.reading): app.config().timebox_sentence_read.get_value(),
                (NoteTypes.Sentence, CardTypes.listening): app.config().timebox_sentence_listen.get_value(),

                (NoteTypes.Vocab, CardTypes.reading): app.config().timebox_vocab_read.get_value(),
                (NoteTypes.Vocab, CardTypes.listening): app.config().timebox_vocab_listen.get_value(),

                (NoteTypes.Kanji, CardTypes.reading): app.config().timebox_kanji_read.get_value(),
            }

            key = (notetype.name, typed.str_(card.type().name))
            if key in card_notetype_timebox_minutes:
                timebox = card_notetype_timebox_minutes[key]
                app.anki_config().set_timebox_seconds(card_notetype_timebox_minutes[key] * 60)
                aqt.utils.tooltip(f"Set timebox to {timebox} minutes")

def init() -> None:
    gui_hooks.webview_will_set_content.append(adjust_timebox)  # pyright: ignore[reportUnknownMemberType]
</file>

<file path="ui/hooks/global_shortcuts.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from aqt import gui_hooks, mw
from PyQt6.QtCore import pyqtBoundSignal
from PyQt6.QtGui import QKeySequence, QShortcut
from PyQt6.QtWidgets import QWidget
from sysutils import ex_assert, typed
from sysutils.typed import checked_cast
from ui.english_dict.find_english_words_dialog import EnglishWordSearchDialog
from ui.hooks import history_navigator
from ui.open_note.open_note_dialog import NoteSearchDialog

if TYPE_CHECKING:
    from collections.abc import Callable


def init() -> None:
    shortcuts: dict[str, Callable[[], None]] = {
        "Alt+Left": history_navigator.navigator.navigate_back,
        "Alt+Right": history_navigator.navigator.navigate_forward,
        "Ctrl+o": NoteSearchDialog.toggle_dialog_visibility,
        "Ctrl+Shift+o": EnglishWordSearchDialog.toggle_dialog_visibility,
        "F5": lambda: app.get_ui_utils().refresh()
    }

    def set_shortcut(widget: QWidget, shortcut: str, callback: Callable[[], None]) -> None:
        typed.checked_cast(pyqtBoundSignal, QShortcut(QKeySequence(shortcut), widget).activated).connect(callback)  # pyright: ignore[reportUnknownMemberType]

    def bind_universal_shortcuts(widget: QWidget) -> None:
        for key, callback in shortcuts.items():
            set_shortcut(widget, key, callback)

    def disable_escape(widget: QWidget) -> None:
        set_shortcut(widget, "Escape", lambda: None)

    ex_assert.not_none(history_navigator.navigator, "History navigator needs to be initialized before global shortcuts are bound")

    bind_universal_shortcuts(checked_cast(QWidget, mw))
    bind_universal_shortcuts(NoteSearchDialog.instance())
    bind_universal_shortcuts(EnglishWordSearchDialog.instance())

    gui_hooks.previewer_did_init.append(bind_universal_shortcuts)  # pyright: ignore[reportUnknownMemberType]
    gui_hooks.previewer_did_init.append(disable_escape)  # pyright: ignore[reportUnknownMemberType]
    gui_hooks.browser_will_show.append(bind_universal_shortcuts)
    gui_hooks.browser_will_show.append(disable_escape)
</file>

<file path="ui/hooks/history_navigator.py">
from __future__ import annotations

import json
import os

from anki.cards import Card, CardId
from ankiutils import app, query_builder, search_executor, ui_utils
from aqt import gui_hooks, mw
from ex_autoslot import AutoSlots
from sysutils import typed
from sysutils.typed import non_optional
from sysutils.weak_ref import WeakRefable


class CardHistoryNavigator(WeakRefable, AutoSlots):
    def __init__(self) -> None:
        self.card_history: list[CardId] = []  # Stores card IDs
        self.current_position: int = -1

        self._load_history_from_file()

        self._is_navigating: bool = False
        self._last_card_shown_was_navigated_card_id: CardId | None = None
        self._last_card_shown_in_reviewer_card_id: CardId | None = None

    def _set_current_position_to_end_of_history(self) -> None:
        self.current_position = len(self.card_history) - 1

    @staticmethod
    def _get_history_file_path() -> str:
        return os.path.join(app.user_files_dir, "card_history.json")

    def _save_last_hundred_items_to_file(self) -> None:
        history_file_path = self._get_history_file_path()
        last_hundred_items = [int(card_id) for card_id in self.card_history[-100:]]

        with open(history_file_path, "w") as history_file:
            json.dump(last_hundred_items, history_file)

    def _load_history_from_file(self) -> None:
        history_file_path = self._get_history_file_path()

        if os.path.exists(history_file_path):
            with open(history_file_path) as history_file:
                saved_history = typed.checked_cast_generics(list[int], json.load(history_file))  # pyright: ignore[reportAny]
                self.card_history = [CardId(card_id) for card_id in saved_history]
                self._set_current_position_to_end_of_history()

    def on_card_shown(self, html: str, card: Card, _display_type: str) -> str:
        if ui_utils.is_reviewer_display_type(_display_type):
            if self._last_card_shown_in_reviewer_card_id == card.id: return html  # We don't want to pollute the history every single time the currently reviewed card is refreshed by our UI refresh code....
            self._last_card_shown_in_reviewer_card_id = card.id

        if self._is_navigating:  # don't mess up the history when navigating the history. Navigating the history is a read-only operation
            self._is_navigating = False
            self._last_card_shown_was_navigated_card_id = card.id
            return html

        # if we navigate away from a card shown while navigating the history, that was probably the card we were searching for and if we hit back after that, we want that card
        if self._last_card_shown_was_navigated_card_id is not None:
            self._remove_from_history(self._last_card_shown_was_navigated_card_id)
            self.card_history.append(self._last_card_shown_was_navigated_card_id)
            self._last_card_shown_was_navigated_card_id = None

        self._remove_from_history(card.id)  # no duplicates in the history please
        self.card_history.append(card.id)

        self._set_current_position_to_end_of_history()
        self._save_last_hundred_items_to_file()
        return html

    def _remove_from_history(self, card_id: CardId) -> None:
        while card_id in self.card_history: self.card_history.remove(card_id)  # no duplicates in the history please

    @staticmethod
    def _card_exists(card_id: CardId) -> bool:
        # noinspection PyBroadException
        try:
            _discarded = non_optional(mw.col).get_card(card_id)
            return True
        except:  # noqa: E722
            return False

    def navigate_back(self) -> None:
        if self._is_at_start_of_history():
            return

        self.current_position -= 1

        while self.current_position >= 0 and not self._card_exists(self.card_history[self.current_position]):
            self.current_position -= 1

        if self.current_position < 0:
            self.current_position = 0
            return

        self._is_navigating = True
        self._show_card_by_id(self.card_history[self.current_position])

    def navigate_forward(self) -> None:
        if self._is_at_end_of_history():
            return

        self.current_position += 1
        while self.current_position < len(self.card_history) and not self._card_exists(self.card_history[self.current_position]):
            self.current_position += 1

        if self.current_position >= len(self.card_history):
            self.current_position = len(self.card_history) - 1
            return

        self._is_navigating = True
        self._show_card_by_id(self.card_history[self.current_position])

    def _is_at_end_of_history(self) -> bool: return self.current_position >= len(self.card_history) - 1
    def _is_at_start_of_history(self) -> bool: return self.current_position <= 0

    @staticmethod
    def _show_card_by_id(card_id: CardId) -> None: search_executor.do_lookup(query_builder.open_card_by_id(card_id))

navigator: CardHistoryNavigator = CardHistoryNavigator()

def init() -> None:
    gui_hooks.card_will_show.append(navigator.on_card_shown)
</file>

<file path="ui/hooks/no_accidental_double_click.py">
from __future__ import annotations

from typing import Literal

from ankiutils import app
from aqt import mw
from aqt.reviewer import Reviewer
from aqt.utils import tooltip
from sysutils.timeutil import StopWatch
from sysutils.typed import non_optional

# noinspection PyProtectedMember
_real_show_answer = Reviewer._showAnswer  # pyright: ignore[reportPrivateUsage]
# noinspection PyProtectedMember
_real_answer_card = Reviewer._answerCard  # pyright: ignore[reportPrivateUsage]

_stopwatch = StopWatch()

def _show_answer(reviewer: Reviewer) -> None:
    if app.config().prevent_double_clicks.get_value() and mw.reviewer.auto_advance_enabled:
        global _stopwatch
        if non_optional(reviewer.card).time_taken() < app.config().minimum_time_viewing_question.get_value() * 1000:
            tooltip("Blocked accidental doubleclick")
            return
        _stopwatch = StopWatch()
    _real_show_answer(reviewer)

def _answer_card(reviewer: Reviewer, ease: Literal[1, 2, 3, 4]) -> None:
    if app.config().prevent_double_clicks.get_value() and mw.reviewer.auto_advance_enabled:
        global _stopwatch
        if _stopwatch.elapsed_seconds() < app.config().minimum_time_viewing_answer.get_value():
            tooltip("Blocked accidental doubleclick")
            return

    _real_answer_card(reviewer, ease)


def init() -> None:
    Reviewer._showAnswer = _show_answer # type: ignore  # pyright: ignore[reportAttributeAccessIssue, reportPrivateUsage]
    Reviewer._answerCard = _answer_card # type: ignore  # pyright: ignore[reportAttributeAccessIssue, reportPrivateUsage]
</file>

<file path="ui/hooks/note_view_shortcuts.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app, ui_utils
from aqt import gui_hooks
from note.kanjinote import KanjiNote
from note.sentences.sentencenote import SentenceNote
from note.vocabulary.vocabnote import VocabNote
from PyQt6.QtCore import pyqtBoundSignal
from PyQt6.QtGui import QKeySequence, QShortcut
from sysutils import typed
from sysutils.typed import try_cast

if TYPE_CHECKING:
    from collections.abc import Callable

    from aqt.main import MainWindowState
    from PyQt6.QtWidgets import QWidget


def init() -> None:
    def try_get_review_note_of_type[T](note_type: type[T]) -> T | None:
        return try_cast(note_type, ui_utils.try_get_review_note())

    def refresh_shallow() -> None: app.get_ui_utils().refresh(refresh_browser=False)

    def remove_mnemonic() -> None:
        kanji = try_get_review_note_of_type(KanjiNote)
        if kanji:
            kanji.set_user_mnemonic("")
            refresh_shallow()

        vocab = try_get_review_note_of_type(VocabNote)
        if vocab:
            vocab.user.mnemonic.empty()
            refresh_shallow()

    def generate_compound_parts() -> None:
        vocab = try_get_review_note_of_type(VocabNote)
        if vocab:
            vocab.compound_parts.auto_generate()
            refresh_shallow()

    def reset_incorrect_matches() -> None:
        sentence = try_get_review_note_of_type(SentenceNote)
        if sentence:
            sentence.configuration.incorrect_matches.reset()
            refresh_shallow()

    def reset_source_comments() -> None:
        sentence = try_get_review_note_of_type(SentenceNote)
        if sentence:
            sentence.source_comments.empty()
            refresh_shallow()

    # noinspection DuplicatedCode
    def toggle_show_compound_parts_in_sentence_breakdown() -> None:
        app.config().show_compound_parts_in_sentence_breakdown.set_value(not app.config().show_compound_parts_in_sentence_breakdown.get_value())
        if app.config().show_compound_parts_in_sentence_breakdown.get_value():
            app.config().show_sentence_breakdown_in_edit_mode.set_value(False)
        refresh_shallow()

    def toggle_show_kanji_in_sentence_breakdown() -> None:
        app.config().show_kanji_in_sentence_breakdown.set_value(not app.config().show_kanji_in_sentence_breakdown.get_value())
        refresh_shallow()

    # noinspection DuplicatedCode
    def toggle_show_sentence_breakdown_in_edit_mode() -> None:
        app.config().show_sentence_breakdown_in_edit_mode.set_value(not app.config().show_sentence_breakdown_in_edit_mode.get_value())
        if app.config().show_sentence_breakdown_in_edit_mode.get_value():
            app.config().show_compound_parts_in_sentence_breakdown.set_value(False)
        refresh_shallow()

    def toggle_yield_last_token_in_suru_verb_compounds_to_overlapping_compound() -> None:
        app.config().automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound.set_value(
            not app.config().automatically_yield_last_token_in_suru_verb_compounds_to_overlapping_compound.get_value())
        refresh_shallow()

    def toggle_yield_last_token_in_passive_verb_compounds_to_overlapping_compound() -> None:
        app.config().automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound.set_value(
            not app.config().automatically_yield_last_token_in_passive_verb_compounds_to_overlapping_compound.get_value())
        refresh_shallow()

    def toggle_yield_last_token_in_causative_verb_compounds_to_overlapping_compound() -> None:
        app.config().automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound.set_value(
            not app.config().automatically_yield_last_token_in_causative_verb_compounds_to_overlapping_compound.get_value())
        refresh_shallow()

    def toggle_all_yield_last_token_flags() -> None:
        app.config().toggle_all_sentence_display_auto_yield_flags()
        refresh_shallow()

    def set_shortcut(widget: QWidget, shortcut: str, callback: Callable[[], None]) -> None:
        typed.checked_cast(pyqtBoundSignal, QShortcut(QKeySequence(shortcut), widget).activated).connect(callback)  # pyright: ignore[reportUnknownMemberType]

    def inject_shortcuts_in_widget(widget: QWidget) -> None:
        for key, callback in stortcuts.items():
            set_shortcut(widget, key, callback)

    def inject_shortcuts_in_reviewer(_state: MainWindowState, state_shortcuts: list[tuple[str, Callable[[], None]]]) -> None:
        def remove_shortcut(string: str) -> None:
            for shortcut in state_shortcuts:
                if shortcut[0] == string:
                    state_shortcuts.remove(shortcut)
                    return

        for char in ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "u"]:
            remove_shortcut(char)

        for key, callback in stortcuts.items():
            state_shortcuts.append((key, callback))

    stortcuts: dict[str, Callable[[], None]] = {"1": toggle_show_compound_parts_in_sentence_breakdown,
                                                "2": toggle_show_sentence_breakdown_in_edit_mode,
                                                "3": toggle_all_yield_last_token_flags,
                                                "4": toggle_show_kanji_in_sentence_breakdown,
                                                "0": remove_mnemonic,
                                                "7": reset_source_comments,
                                                "8": reset_incorrect_matches,
                                                "9": generate_compound_parts,
                                                "Ctrl+Shift+Alt+s": toggle_yield_last_token_in_suru_verb_compounds_to_overlapping_compound,
                                                "Ctrl+Shift+Alt+h": toggle_yield_last_token_in_passive_verb_compounds_to_overlapping_compound,
                                                "Ctrl+Shift+Alt+t": toggle_yield_last_token_in_causative_verb_compounds_to_overlapping_compound,
                                                "Ctrl+Shift+Alt+d": toggle_all_yield_last_token_flags}

    gui_hooks.previewer_did_init.append(inject_shortcuts_in_widget)  # pyright: ignore[reportUnknownMemberType]
    gui_hooks.state_shortcuts_will_change.append(inject_shortcuts_in_reviewer)  # pyright: ignore[reportUnknownMemberType]
</file>

<file path="ui/hooks/timebox_end_sound.py">
from __future__ import annotations

from os.path import dirname

from ankiutils import app
from aqt.reviewer import Reviewer
from aqt.sound import av_player
from aqt.utils import askUserDialog
from PyQt6.QtWidgets import QMessageBox
from sysutils import timeutil, typed

addon_path: str = dirname(__file__)
sound_file: str = addon_path + "/timebox_complete.mp3"

def _check_timebox(reviewer: Reviewer) -> bool:
    elapsed = app.anki_collection().timeboxReached()
    if elapsed:
        av_player.play_file(sound_file)
        assert not isinstance(elapsed, bool)
        cards_studied = elapsed[1]
        seconds_studied = typed.float_(elapsed[0])  # pyright: ignore[reportAny]
        seconds_per_card = float(seconds_studied) / cards_studied

        dialog = askUserDialog(f"""
Studied {cards_studied} cards in {timeutil.format_seconds_as_hh_mm_ss(seconds_studied)}.  # pyright: ignore[reportAny]  # pyright: ignore[reportAny]
{seconds_per_card:.2f} seconds per card.
""", ["OK"])
        dialog.setIcon(QMessageBox.Icon.Information)
        dialog.exec()
        reviewer.mw.moveToState("deckBrowser")
        return True
    return False

def init() -> None:
    Reviewer.check_timebox = _check_timebox  # type: ignore  # pyright: ignore[reportAttributeAccessIssue]
</file>

<file path="ui/menus/__init__.py">
from __future__ import annotations

from . import browser, common
from . import notes as notes
from . import web_search as web_search


def init() -> None:
    common.init()
    browser.init()
</file>

<file path="ui/menus/browser/__init__.py">
from __future__ import annotations

from ui.menus.browser import main


def init() -> None:
    main.init()
</file>

<file path="ui/menus/browser/main.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from aqt import gui_hooks
from note import queue_manager
from note.jpnote import JPNote
from note.sentences.sentencenote import SentenceNote
from sysutils import ex_lambda
from sysutils.typed import non_optional
from ui import menus
from ui.menus.menu_utils import shortcutfinger

if TYPE_CHECKING:
    from collections.abc import Sequence

    from anki.cards import Card, CardId
    from aqt.browser import Browser  # type: ignore[attr-defined]  # pyright: ignore[reportPrivateImportUsage]
    from PyQt6.QtWidgets import QMenu


def spread_due_dates(cards: Sequence[CardId], start_day: int, days: int) -> None:
    anki_col = app.col().anki_collection
    scheduler = anki_col.sched
    for index, card_id in enumerate(cards):
        card: Card = anki_col.get_card(card_id)
        new_due = start_day + (index * days)
        scheduler.set_due_date([card.id], str(new_due))

    app.get_ui_utils().refresh()

def setup_browser_context_menu(browser: Browser, menu: QMenu) -> None:
    magnus_menu: QMenu = non_optional(menu.addMenu("&Magnus"))
    selected_cards = browser.selected_cards()

    if len(selected_cards) == 1:
        magnus_menu.addAction("Prioritize selected cards", lambda: queue_manager.prioritize_selected_cards(selected_cards))  # pyright: ignore[reportUnknownMemberType]

        card = app.anki_collection().get_card(selected_cards[0])
        note = JPNote.note_from_card(card)
        menus.common.build_browser_right_click_menu(non_optional(magnus_menu.addMenu(shortcutfinger.home3("Note"))), note)

    if len(selected_cards) > 0:
        spread_menu: QMenu = non_optional(magnus_menu.addMenu("&Spread selected cards"))
        for start_day in [0,1,2,3,4,5,6,7,8,9]:
            start_day_menu: QMenu = non_optional(spread_menu.addMenu(f"First card in {start_day} days"))
            for days_apart in [1,2,3,4,5,6,7,8,9]:
                start_day_menu.addAction(f"{days_apart} days apart", ex_lambda.bind3(spread_due_dates, selected_cards, start_day, days_apart))  # pyright: ignore[reportUnknownMemberType]

    selected_notes = {app.col().note_from_note_id(note_id) for note_id in browser.selectedNotes()}

    selected_sentences:list[SentenceNote] = [note for note in selected_notes if isinstance(note, SentenceNote)]
    if selected_sentences:
        from batches import local_note_updater

        magnus_menu.addAction("Reparse sentence words", lambda: local_note_updater.reparse_sentences(selected_sentences))  # pyright: ignore[reportUnknownMemberType]


def init() -> None:
    gui_hooks.browser_will_show_context_menu.append(setup_browser_context_menu)
</file>

<file path="ui/menus/common.py">
from __future__ import annotations

import typing

import pyperclip
from ankiutils import app, query_builder, search_executor, ui_utils
from aqt import gui_hooks
from batches import local_note_updater
from note.kanjinote import KanjiNote
from note.note_constants import Mine
from note.sentences.sentencenote import SentenceNote
from note.vocabulary.vocabnote import VocabNote
from qt_utils.ex_qmenu import ExQmenu
from queryablecollections.collections.q_list import QList
from sysutils import ex_lambda, typed
from sysutils.typed import non_optional
from ui import menus
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_ui_action, create_note_action, create_vocab_note_action
from ui.menus.open_in_anki import build_open_in_anki_menu
from ui.menus.web_search import build_web_search_menu

if typing.TYPE_CHECKING:
    from aqt.webview import AnkiWebView
    from note.jpnote import JPNote
    from PyQt6.QtWidgets import QMenu

def build_browser_right_click_menu(root_menu: QMenu, note: JPNote) -> None:
    build_right_click_menu(root_menu, note, "", "")

def build_right_click_menu_webview_hook(view: AnkiWebView, root_menu: QMenu) -> None:
    selection = non_optional(view.page()).selectedText().strip()
    clipboard = pyperclip.paste().strip()
    note = ui_utils.get_note_from_web_view(view)
    build_right_click_menu(root_menu, note, selection, clipboard)

# noinspection PyPep8
def build_right_click_menu(right_click_menu: QMenu, note: JPNote | None, selection: str, clipboard: str) -> None:
    if not app.is_initialized():
        right_click_menu.addAction(Mine.app_still_loading_message)  # pyright: ignore[reportUnknownMemberType]
        return

    selection_menu = non_optional(right_click_menu.addMenu(shortcutfinger.home1(f'''Selection: "{selection[:40]}"'''))) if selection else None
    clipboard_menu = non_optional(right_click_menu.addMenu(shortcutfinger.home2(f'''Clipboard: "{clipboard[:40]}"'''))) if clipboard else None
    note_actions_menu = non_optional(right_click_menu.addMenu(shortcutfinger.home3("Note actions")))
    build_universal_note_actions_menu(non_optional(right_click_menu.addMenu(shortcutfinger.home4("Universal note actions"))), note)
    view_menu = non_optional(right_click_menu.addMenu(shortcutfinger.home5("View")))

    string_note_menu_factory: typing.Callable[[QMenu, str], None] = null_op_factory
    if note:
        if isinstance(note, KanjiNote):
            menus.notes.kanji.main.build_note_menu(note_actions_menu, note)
            menus.notes.kanji.main.build_view_menu(view_menu, note)
            string_note_menu_factory = lambda menu, string: menus.notes.kanji.string_menu.build(menu, typed.checked_cast(KanjiNote, note), string)  # noqa: E731
        elif isinstance(note, VocabNote):
            menus.notes.vocab.main.setup_note_menu(note_actions_menu, note, selection, clipboard)
            menus.notes.vocab.main.build_view_menu(view_menu, note)
            string_note_menu_factory = lambda menu, string: menus.notes.vocab.string_menu.build_string_menu(menu, typed.checked_cast(VocabNote, note), string)  # noqa: E731
        elif isinstance(note, SentenceNote):
            menus.notes.sentence.main.build_note_menu(note_actions_menu, note)
            menus.notes.sentence.main.build_view_menu(view_menu, note)
            string_note_menu_factory = lambda menu, string: menus.notes.sentence.string_menu.build_string_menu(menu, typed.checked_cast(SentenceNote, note), string)  # noqa: E731

    if selection_menu:
        build_string_menu(selection_menu, selection, string_note_menu_factory)
    if clipboard_menu:
        build_string_menu(clipboard_menu, clipboard, string_note_menu_factory)

    ExQmenu.disable_empty_submenus(right_click_menu)

def build_string_menu(menu: QMenu, string: str, string_note_menu_factory: typing.Callable[[QMenu, str], None]) -> None:
    def build_create_note_menu(create_note_menu: QMenu, to_create: str) -> None:
        create_vocab_note_action(create_note_menu, shortcutfinger.home1("vocab"), ex_lambda.bind1(VocabNote.factory.create_with_dictionary, string))
        create_note_action(create_note_menu, shortcutfinger.home2("sentence"), ex_lambda.bind1(SentenceNote.create, to_create))
        create_note_action(create_note_menu, shortcutfinger.home3("kanji"), ex_lambda.bind4(KanjiNote.create, to_create, "TODO", "", ""))

    def build_matching_note_menu(matching_note_menu: QMenu, search_string: str) -> None:
        vocabs = app.col().vocab.with_question(search_string)
        sentences = app.col().sentences.with_question(search_string)
        kanjis = app.col().kanji.with_any_kanji_in([search_string]) if len(search_string) == 1 else QList()

        if not any(vocabs) and not any(sentences) and not any(kanjis):
            return

        build_universal_note_actions_menu(non_optional(matching_note_menu.addMenu(shortcutfinger.home1("Vocab Actions"))), vocabs[0] if vocabs else None)
        build_universal_note_actions_menu(non_optional(matching_note_menu.addMenu(shortcutfinger.home2("Sentence Actions"))), sentences[0] if sentences else None)
        build_universal_note_actions_menu(non_optional(matching_note_menu.addMenu(shortcutfinger.home3("Kanji Actions"))), kanjis[0] if kanjis else None)

    string_note_menu_factory(non_optional(menu.addMenu(shortcutfinger.home1("Current note actions"))), string)
    build_open_in_anki_menu(non_optional(menu.addMenu(shortcutfinger.home2("Open in Anki"))), lambda: string)
    build_web_search_menu(non_optional(menu.addMenu(shortcutfinger.home3("Search Web"))), lambda: string)
    build_matching_note_menu(non_optional(menu.addMenu(shortcutfinger.home4("Exactly matching notes"))), string)
    build_create_note_menu(non_optional(menu.addMenu(shortcutfinger.up1(f"Create: {string[:40]}"))), string)
    add_ui_action(menu, shortcutfinger.down1("Reparse matching sentences"), lambda: local_note_updater.reparse_matching_sentences(string))

def build_universal_note_actions_menu(universal_actions_menu: QMenu, note: JPNote | None) -> None:
    if not note: return

    universal_actions_menu.addAction(shortcutfinger.home1("Open in previewer"), search_executor.lookup_and_show_previewer_promise(lambda: query_builder.notes_lookup([note])))  # pyright: ignore[reportUnknownMemberType]
    note_actions_menu = non_optional(universal_actions_menu.addMenu(shortcutfinger.home2("Note actions")))

    add_ui_action(universal_actions_menu, shortcutfinger.home3("Unsuspend all cards"), note.unsuspend_all_cards, note.has_suspended_cards())
    add_ui_action(universal_actions_menu, shortcutfinger.home4("Suspend all cards"), note.suspend_all_cards, note.has_active_cards())
    add_ui_action(universal_actions_menu, shortcutfinger.up1("Unsuspend all cards and dependencies' cards"), note.unsuspend_all_cards_and_dependencies, confirm=True, enabled=note.has_suspended_cards_or_depencies_suspended_cards())

    if note:
        if isinstance(note, KanjiNote):
            menus.notes.kanji.main.build_note_menu(note_actions_menu, note)
        elif isinstance(note, VocabNote):
            menus.notes.vocab.main.setup_note_menu(note_actions_menu, note, "TODO:selection", "TODO:clipboard")
        elif isinstance(note, SentenceNote):
            menus.notes.sentence.main.build_note_menu(note_actions_menu, note)

def null_op_factory(_menu: QMenu, _string: str) -> None:
    pass

def init() -> None:
    gui_hooks.webview_will_show_context_menu.append(build_right_click_menu_webview_hook)  # pyright: ignore[reportUnknownMemberType]
    gui_hooks.editor_will_show_context_menu.append(build_right_click_menu_webview_hook)  # pyright: ignore[reportUnknownMemberType]
</file>

<file path="ui/menus/menu_utils/ex_qmenu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app, query_builder, search_executor
from ankiutils.app import get_ui_utils
from ankiutils.search_executor import lookup_promise
from aqt import pyqtBoundSignal
from PyQt6.QtGui import QAction
from PyQt6.QtWidgets import QMenu, QMessageBox
from sysutils.typed import checked_cast, non_optional
from ui.menus.menu_utils import shortcutfinger

if TYPE_CHECKING:
    from collections.abc import Callable

    from configuration.configuration_value import ConfigurationValueBool
    from note.jpnote import JPNote
    from note.vocabulary.vocabnote import VocabNote

def build_checkbox_config_section_menu(menu: QMenu, toggles: list[ConfigurationValueBool]) -> None:
    for index, toggle in enumerate(toggles):
        add_checkbox_config(menu, toggle, shortcutfinger.finger_by_priority_order(index, toggle.title))

def add_checkbox_config(menu: QMenu, config_value: ConfigurationValueBool, _title: str) -> None:
    checkbox_action = QAction(_title, app.main_window())
    checkbox_action.setCheckable(True)
    checkbox_action.setChecked(config_value.get_value())

    config_value.on_change(checkbox_action.setChecked)  # when the value changes through another mechanism, make sure the menu changes state

    def set_value(value: bool) -> None:
        config_value.set_value(value)
        app.get_ui_utils().refresh()

    checked_cast(pyqtBoundSignal, checkbox_action.triggered).connect(set_value)  # pyright: ignore[reportUnknownMemberType]
    menu.addAction(checkbox_action)  # pyright: ignore[reportUnknownMemberType]

def _confirm(menu: QMenu, message: str) -> bool:
    message = shortcutfinger.remove_shortcut_text(message)
    return QMessageBox.question(
        menu.parentWidget(),
        f"{message}?",
        f"{message}?",
        QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
        QMessageBox.StandardButton.Yes
    ) == QMessageBox.StandardButton.Yes

def add_ui_action(menu: QMenu, name: str, callback: Callable[[], None], enabled: bool = True, confirm: bool = False) -> QAction:
    def run_ui_action() -> None:
        if not confirm or _confirm(menu, name):
            callback()
            get_ui_utils().refresh()

    action = non_optional(menu.addAction(name, lambda: run_ui_action()))  # pyright: ignore[reportUnknownMemberType]
    action.setEnabled(enabled)
    return action

def add_lookup_action_lambda(menu: QMenu, name: str, search: Callable[[], str]) -> None:
    menu.addAction(name, lookup_promise(search))  # pyright: ignore[reportUnknownMemberType]

def add_lookup_action(menu: QMenu, name: str, search: str) -> None:
    menu.addAction(name, lookup_promise(lambda: search))  # pyright: ignore[reportUnknownMemberType]

def add_single_vocab_lookup_action(menu: QMenu, name: str, vocab: str) -> None:
    menu.addAction(name, lookup_promise(lambda: query_builder.single_vocab_by_form_exact(vocab)))  # pyright: ignore[reportUnknownMemberType]

def add_vocab_dependencies_lookup(menu: QMenu, name: str, vocab: VocabNote) -> None:
    add_lookup_action_lambda(menu, name, lambda: query_builder.vocab_dependencies_lookup_query(vocab))

def create_note_action(menu: QMenu, name: str, callback: Callable[[], JPNote]) -> None:
    def run_ui_action() -> None:
        new_note = callback()
        search_executor.do_lookup_and_show_previewer(query_builder.notes_lookup([new_note]))
        get_ui_utils().refresh()

    menu.addAction(name, lambda: run_ui_action())  # pyright: ignore[reportUnknownMemberType]

def create_vocab_note_action(menu: QMenu, name: str, callback: Callable[[], VocabNote]) -> None:
    def do_it() -> VocabNote:
        new_note = callback()
        from batches import local_note_updater
        local_note_updater.reparse_sentences_for_vocab(new_note)
        return new_note

    create_note_action(menu, name, do_it)
</file>

<file path="ui/menus/menu_utils/shortcutfinger.py">
from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from collections.abc import Callable


def _format_finger(finger: str, text:str) -> str: return f"""&{finger} {text}"""

def home1(text: str) -> str: return _format_finger("u", text)
def home2(text: str) -> str: return _format_finger("e", text)
def home3(text: str) -> str: return _format_finger("o", text)
def home4(text: str) -> str: return _format_finger("a", text)
def home5(text: str) -> str: return _format_finger("i", text)

def up1(text: str) -> str: return _format_finger("p", text)
def up2(text: str) -> str: return _format_finger("ö", text)
def up3(text: str) -> str: return _format_finger("ä", text)
def up4(text: str) -> str: return _format_finger("å", text)
def up5(text: str) -> str: return _format_finger("y", text)

def down1(text: str) -> str: return _format_finger("k", text)
def down2(text: str) -> str: return _format_finger("j", text)
def down3(text: str) -> str: return _format_finger("q", text)
#def down4(text: str) -> str: return _format_finger(".", text)
def down5(text: str) -> str: return _format_finger("x", text)
def down6(text: str) -> str: return _format_finger("b", text)

def none(text:str) -> str: return text

_numpad_functions: list[Callable[[str], str]] = [up4, up3, up2, up1, up5, down3, down2, down1, down5, down6]
def numpad(index:int, text:str) -> str:
    if index < len(_numpad_functions): return _numpad_functions[index](f"""{index + 1} {text}""")
    return none(text)

_finger_by_priority_order: list[Callable[[str], str]] = [home1, home2, home3, home4, home5, up1, up2, up3, up4, up5, down1, down2, down3, down5]
def finger_by_priority_order(index:int, text:str) -> str:
    if index < len(_finger_by_priority_order): return _finger_by_priority_order[index](f"""{text}""")
    return none(text)

def remove_shortcut_text(string:str) -> str: return " " .join(string.split(" ")[1:])
</file>

<file path="ui/menus/notes/__init__.py">
from __future__ import annotations

from . import kanji as kanji
from . import sentence as sentence
from . import vocab as vocab
</file>

<file path="ui/menus/notes/kanji/__init__.py">
from __future__ import annotations

from . import main as main
from . import string_menu as string_menu
</file>

<file path="ui/menus/notes/kanji/main.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app, query_builder
from sysutils import ex_str
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_lookup_action, add_ui_action

if TYPE_CHECKING:
    from note.kanjinote import KanjiNote
    from PyQt6.QtWidgets import QMenu

def build_note_menu(note_menu: QMenu, kanji: KanjiNote) -> None:
    def build_lookup_menu(note_lookup_menu: QMenu) -> None:
        add_lookup_action(note_lookup_menu, shortcutfinger.home1("Primary Vocabs"), query_builder.vocabs_lookup_strings(kanji.get_primary_vocab()))
        add_lookup_action(note_lookup_menu, shortcutfinger.home2("Vocabs"), query_builder.vocab_with_kanji(kanji))
        add_lookup_action(note_lookup_menu, shortcutfinger.home3("Radicals"), query_builder.notes_lookup(kanji.get_radicals_notes()))
        add_lookup_action(note_lookup_menu, shortcutfinger.home4("Kanji"), query_builder.notes_lookup(app.col().kanji.with_radical(kanji.get_question())))
        add_lookup_action(note_lookup_menu, shortcutfinger.home5("Sentences"), query_builder.sentence_search(kanji.get_question(), exact=True))

    build_lookup_menu(non_optional(note_menu.addMenu(shortcutfinger.home1("Open"))))

    add_ui_action(note_menu, shortcutfinger.home5("Reset Primary Vocabs"), lambda: kanji.set_primary_vocab([]))

    if not kanji.get_user_answer():
        add_ui_action(note_menu, shortcutfinger.up1("Accept meaning"), lambda: kanji.set_user_answer(format_kanji_meaning(kanji.get_answer())))

    add_ui_action(note_menu, shortcutfinger.up2("Populate radicals from mnemonic tags"), lambda: kanji.populate_radicals_from_mnemonic_tags())
    add_ui_action(note_menu, shortcutfinger.up3("Bootstrap mnemonic from radicals"), lambda: kanji.bootstrap_mnemonic_from_radicals())
    add_ui_action(note_menu, shortcutfinger.up4("Reset mnemonic"), lambda: kanji.set_user_mnemonic(""))

def build_view_menu(_view_menu:QMenu, _kanji:KanjiNote) -> None:
    pass

def format_kanji_meaning(meaning: str) -> str:
    val = (ex_str.replace_html_and_bracket_markup_with(meaning, "|")
           .lower()
           .replace("||", "|")
           .replace("||", "|")
           .replace("||", "|")
           .replace(", ", "|")
           .replace(" ", "-")
           .replace("-|-", " | "))

    val = val.removesuffix("|")
    return val.removeprefix("|")
</file>

<file path="ui/menus/notes/kanji/string_menu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from sysutils import ex_lambda, kana_utils
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_ui_action

if TYPE_CHECKING:
    import collections.abc

    from note.kanjinote import KanjiNote
    from PyQt6.QtWidgets import QMenu


def build(string_menu: QMenu, kanji: KanjiNote, menu_string: str) -> None:
    def build_highlighted_vocab_menu(highlighted_vocab_menu: QMenu, _vocab_to_add: str) -> None:
        for index, _vocab in enumerate(kanji.get_primary_vocab()):
            add_ui_action(highlighted_vocab_menu, shortcutfinger.numpad(index, f"{_vocab}"), ex_lambda.bind2(kanji.position_primary_vocab, menu_string, index))

        add_ui_action(highlighted_vocab_menu, shortcutfinger.home1("[Last]"), lambda: kanji.position_primary_vocab(_vocab_to_add))

        if _vocab_to_add in kanji.get_primary_vocab():
            add_ui_action(highlighted_vocab_menu, shortcutfinger.home2("Remove"), ex_lambda.bind1(kanji.remove_primary_vocab, _vocab_to_add))

    def add_primary_readings_actions(menu: QMenu, title_factory: collections.abc.Callable[[str], str], string: str) -> None:
        if kana_utils.is_only_katakana(string):
            hiragana_string = kana_utils.katakana_to_hiragana(string)
            if hiragana_string in kanji.get_primary_readings_on():
                add_ui_action(menu, title_factory("Remove primary Onyomi Reading"), lambda: kanji.remove_primary_on_reading(hiragana_string))
            elif hiragana_string in kanji.get_readings_on():
                add_ui_action(menu, title_factory("Make primary Onyomi Reading"), lambda: kanji.add_primary_on_reading(hiragana_string))
        elif kana_utils.is_only_hiragana(string):
            if string in kanji.get_primary_readings_kun():
                add_ui_action(menu, title_factory("Remove primary Kunyomi reading"), lambda: kanji.remove_primary_kun_reading(string))
            elif string in kanji.get_readings_kun():
                add_ui_action(menu, title_factory("Make primary Kunyomi reading"), lambda: kanji.add_primary_kun_reading(string))

    def build_add_menu(add_menu: QMenu) -> None:
        add_ui_action(add_menu, shortcutfinger.home1("Similar meaning"), lambda: kanji.add_user_similar_meaning(menu_string))
        add_ui_action(add_menu, shortcutfinger.home2("Confused with"), lambda: kanji.add_related_confused_with(menu_string))

    build_highlighted_vocab_menu(non_optional(string_menu.addMenu(shortcutfinger.home1("Highlighted Vocab"))), menu_string)
    build_add_menu(non_optional(string_menu.addMenu(shortcutfinger.home2("Add"))))
    add_primary_readings_actions(string_menu, shortcutfinger.home3, menu_string)
</file>

<file path="ui/menus/notes/sentence/__init__.py">
from __future__ import annotations

from . import main as main
from . import string_menu as string_menu
</file>

<file path="ui/menus/notes/sentence/main.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app, query_builder
from note.note_constants import NoteFields, NoteTypes
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_checkbox_config, add_lookup_action, add_ui_action

if TYPE_CHECKING:
    from note.sentences.sentencenote import SentenceNote
    from PyQt6.QtWidgets import QMenu

def build_note_menu(note_menu: QMenu, sentence: SentenceNote) -> None:
    def build_lookup_menu(note_lookup_menu: QMenu) -> None:
        add_lookup_action(note_lookup_menu, shortcutfinger.home1("Highlighted Vocab"), query_builder.vocabs_lookup_strings(list(sentence.configuration.highlighted_words())))
        add_lookup_action(note_lookup_menu, shortcutfinger.home2("Highlighted Vocab Read Card"), query_builder.vocabs_lookup_strings_read_card(list(sentence.configuration.highlighted_words())))
        add_lookup_action(note_lookup_menu, shortcutfinger.home3("Kanji"), f"""note:{NoteTypes.Kanji} ({" OR ".join([f"{NoteFields.Kanji.question}:{kan}" for kan in sentence.extract_kanji()])})""")
        add_lookup_action(note_lookup_menu, shortcutfinger.home4("Parsed words"), query_builder.notes_by_id([voc.get_id() for voc in sentence.get_parsed_words_notes()]))

    def build_remove_menu(remove_menu: QMenu) -> None:
        add_ui_action(remove_menu, shortcutfinger.home1("All highlighted"), lambda: sentence.configuration.reset_highlighted_words()).setEnabled(any(sentence.configuration.highlighted_words()))
        add_ui_action(remove_menu, shortcutfinger.home2("All incorrect matches"), lambda: sentence.configuration.incorrect_matches.reset()).setEnabled(any(sentence.configuration.incorrect_matches.get()))
        add_ui_action(remove_menu, shortcutfinger.home3("All hidden matches"), lambda: sentence.configuration.hidden_matches.reset()).setEnabled(any(sentence.configuration.hidden_matches.get()))
        add_ui_action(remove_menu, shortcutfinger.home4("Source comments"), lambda: sentence.source_comments.empty()).setEnabled(sentence.source_comments.has_value())

    def build_remove_user(remove_user_menu: QMenu) -> None:
        add_ui_action(remove_user_menu, shortcutfinger.home1("comments"), lambda: sentence.user.comments.empty()).setEnabled(sentence.user.comments.has_value())
        add_ui_action(remove_user_menu, shortcutfinger.home2("analysis"), lambda: sentence.user.answer_analysis.empty()).setEnabled(sentence.user.answer_analysis.has_value())
        add_ui_action(remove_user_menu, shortcutfinger.home3("answer"), lambda: sentence.user.question.empty()).setEnabled(sentence.user.answer.has_value())
        add_ui_action(remove_user_menu, shortcutfinger.home4("question"), lambda: sentence.user.question.empty()).setEnabled(sentence.user.question.has_value())

    build_lookup_menu(non_optional(note_menu.addMenu(shortcutfinger.home1("Open"))))
    build_remove_menu(non_optional(note_menu.addMenu(shortcutfinger.home2("Remove"))))
    build_remove_user(non_optional(note_menu.addMenu(shortcutfinger.home3("Remove User"))))

def build_view_menu(view_menu: QMenu, _vocab: SentenceNote) -> None:  # pyright: ignore
    index = 0
    for index, toggle in enumerate(app.config().sentence_view_toggles):
        add_checkbox_config(view_menu, toggle, shortcutfinger.finger_by_priority_order(index, toggle.title))

    add_ui_action(view_menu,
                  shortcutfinger.finger_by_priority_order(index + 1, "Toggle all sentence auto yield compound last token flags (Ctrl+Shift+Alt+d)"),
                  app.config().toggle_all_sentence_display_auto_yield_flags)
</file>

<file path="ui/menus/notes/sentence/string_menu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from language_services.janome_ex.word_extraction.word_exclusion import WordExclusion
from sysutils import ex_lambda
from sysutils.lazy import Lazy
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_ui_action

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant
    from note.sentences.sentencenote import SentenceNote
    from note.sentences.word_exclusion_set import WordExclusionSet
    from PyQt6.QtWidgets import QMenu

def build_string_menu(string_menu: QMenu, sentence: SentenceNote, menu_string: str) -> None:
    def add_add_word_exclusion_action(add_menu: QMenu, exclusion_type_title: str, exclusion_set: WordExclusionSet) -> None:
        menu_string_as_word_exclusion = WordExclusion.global_(menu_string)
        analysis = sentence.create_analysis()
        valid_words = Lazy(lambda: analysis.valid_word_variants) #these little tricks with the Lazy is to capture the analysis in a closure so that the weak references within the analysis don't get cleaned up before the action is invoked in the UI by the user.
        words_excluded_by_menu_string: Lazy[list[CandidateWordVariant]] = Lazy(lambda: [w for w in valid_words() if menu_string_as_word_exclusion.excludes_form_at_index(w.form, w.start_index)])
        if any(words_excluded_by_menu_string()):
            if len(words_excluded_by_menu_string()) == 1:
                add_ui_action(add_menu, exclusion_type_title, lambda: exclusion_set.add(words_excluded_by_menu_string()[0].to_exclusion()))
            else:
                add_exclusion_menu: QMenu = non_optional(add_menu.addMenu(exclusion_type_title))

                for excluded_index, matched in enumerate(words_excluded_by_menu_string()):
                    add_ui_action(add_exclusion_menu, shortcutfinger.finger_by_priority_order(excluded_index, f"{matched.start_index}: {matched.form}"), ex_lambda.bind1(exclusion_set.add, matched.to_exclusion()))
        else:
            add_ui_action(add_menu, exclusion_type_title, lambda: None).setEnabled(False)

    def add_remove_word_exclusion_action(word_exclusion_remove: QMenu, exclusion_type_title: str, exclusion_set: WordExclusionSet) -> None:
        menu_string_as_word_exclusion = WordExclusion.global_(menu_string)
        current_exclusions = exclusion_set.get()
        covered_existing_exclusions = [excl for excl in current_exclusions if menu_string_as_word_exclusion.excludes_all_words_excluded_by(excl)]
        if any(covered_existing_exclusions):
            if len(covered_existing_exclusions) == 1:
                add_ui_action(word_exclusion_remove, exclusion_type_title, lambda: exclusion_set.remove_string(menu_string))
            else:
                remove_at_index_menu: QMenu = non_optional(word_exclusion_remove.addMenu(exclusion_type_title))
                for excluded_index, matched_exclusion in enumerate(covered_existing_exclusions):
                    add_ui_action(remove_at_index_menu, shortcutfinger.finger_by_priority_order(excluded_index, f"{matched_exclusion.index}:{matched_exclusion.word}"), ex_lambda.bind1(exclusion_set.remove, matched_exclusion))
        else:
            add_ui_action(word_exclusion_remove, exclusion_type_title, lambda: None, False)

    def build_add_menu(add_menu: QMenu) -> None:
        add_ui_action(add_menu, shortcutfinger.home1("Highlighted Vocab"), lambda: sentence.configuration.add_highlighted_word(menu_string), menu_string not in sentence.configuration.highlighted_words())
        add_add_word_exclusion_action(add_menu, shortcutfinger.home2("Hidden matches"), sentence.configuration.hidden_matches)
        add_add_word_exclusion_action(add_menu, shortcutfinger.home3("Incorrect matches"), sentence.configuration.incorrect_matches)

    def build_remove_menu(remove_menu: QMenu) -> None:
        add_ui_action(remove_menu, shortcutfinger.home1("Highlighted vocab"), lambda: sentence.configuration.remove_highlighted_word(menu_string)).setEnabled(menu_string in sentence.configuration.highlighted_words())
        add_remove_word_exclusion_action(remove_menu, shortcutfinger.home2("Hidden matches"), sentence.configuration.hidden_matches)
        add_remove_word_exclusion_action(remove_menu, shortcutfinger.home3("Incorrect matches"), sentence.configuration.incorrect_matches)

    build_add_menu(non_optional(string_menu.addMenu(shortcutfinger.home1("Add"))))
    build_remove_menu(non_optional(string_menu.addMenu(shortcutfinger.home2("Remove"))))
    add_ui_action(string_menu, shortcutfinger.home3("Split with word-break tag in question"), lambda: sentence.question.split_token_with_word_break_tag(menu_string), menu_string in sentence.get_question())
</file>

<file path="ui/menus/notes/vocab/__init__.py">
from __future__ import annotations

from . import common as common
from . import create_note_menu as create_note_menu
from . import main as main
from . import string_menu as string_menu
</file>

<file path="ui/menus/notes/vocab/common.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import create_vocab_note_action

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from PyQt6.QtWidgets import QMenu


def build_create_prefix_postfix_note_menu(prefix_postfix_note_menu: QMenu, vocab: VocabNote, addendum: str) -> None:
    def create_suffix_note_menu(suffix_note_menu: QMenu) -> None:
        create_vocab_note_action(suffix_note_menu, shortcutfinger.home1("dictionary-form"), lambda: vocab.cloner.create_suffix_version(addendum))
        create_vocab_note_action(suffix_note_menu, shortcutfinger.home2(f"い-stem {vocab.cloner.suffix_to_i_stem_preview(addendum)}"), lambda: vocab.cloner.suffix_to_i_stem(addendum))
        create_vocab_note_action(suffix_note_menu, shortcutfinger.home3(f"て-stem  {vocab.cloner.suffix_to_te_stem_preview(addendum)}"), lambda: vocab.cloner.suffix_to_te_stem(addendum))
        create_vocab_note_action(suffix_note_menu, shortcutfinger.home4(f"え-stem  {vocab.cloner.suffix_to_e_stem_preview(addendum)}"), lambda: vocab.cloner.suffix_to_e_stem(addendum))
        create_vocab_note_action(suffix_note_menu, shortcutfinger.up1(f"あ-stem  {vocab.cloner.suffix_to_a_stem_preview(addendum)}"), lambda: vocab.cloner.suffix_to_a_stem(addendum))

    create_vocab_note_action(prefix_postfix_note_menu, shortcutfinger.home1(f"prefix-{addendum}{vocab.get_question()}"), lambda: vocab.cloner.create_prefix_version(addendum))

    create_suffix_note_menu(non_optional(prefix_postfix_note_menu.addMenu(shortcutfinger.home2("Suffix-onto"))))
</file>

<file path="ui/menus/notes/vocab/create_note_menu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import create_vocab_note_action
from ui.menus.notes.vocab.common import build_create_prefix_postfix_note_menu

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from PyQt6.QtWidgets import QMenu

def build_create_note_menu(note_create_menu: QMenu, vocab: VocabNote, selection: str, clipboard: str) -> None:
    def build_forms_menu(clone_to_form_menu: QMenu) -> None:
        forms_with_no_vocab = [form for form in vocab.forms.all_set() if not any(app.col().vocab.with_question(form))]

        def add_clone_to_form_action(title:str, form:str) -> None:
            create_vocab_note_action(clone_to_form_menu, title, lambda: vocab.cloner.clone_to_form(form))

        for index, form in enumerate(forms_with_no_vocab):
            add_clone_to_form_action(shortcutfinger.finger_by_priority_order(index, form), form)

    def build_noun_variations_menu(noun_menu: QMenu) -> None:
        create_vocab_note_action(noun_menu, shortcutfinger.home1("する-verb"), lambda: vocab.cloner.create_suru_verb())
        create_vocab_note_action(noun_menu, shortcutfinger.home2("します-verb"), lambda: vocab.cloner.create_shimasu_verb())
        create_vocab_note_action(noun_menu, shortcutfinger.home3("な-adjective"), lambda: vocab.cloner.create_na_adjective())
        create_vocab_note_action(noun_menu, shortcutfinger.home4("の-adjective"), lambda: vocab.cloner.create_no_adjective())
        create_vocab_note_action(noun_menu, shortcutfinger.up1("に-adverb"), lambda: vocab.cloner.create_ni_adverb())
        create_vocab_note_action(noun_menu, shortcutfinger.up2("と-adverb"), lambda: vocab.cloner.create_to_adverb())

    def build_verb_variations_menu(verb_menu: QMenu) -> None:
        create_vocab_note_action(verb_menu, shortcutfinger.home1("ます-form"), lambda: vocab.cloner.create_masu_form())
        create_vocab_note_action(verb_menu, shortcutfinger.home2("て-form"), lambda: vocab.cloner.create_te_form())
        create_vocab_note_action(verb_menu, shortcutfinger.home3("た-form"), lambda: vocab.cloner.create_ta_form())
        create_vocab_note_action(verb_menu, shortcutfinger.home4("ない-form"), lambda: vocab.cloner.create_nai_form())
        create_vocab_note_action(verb_menu, shortcutfinger.home5(f"え-stem/godan-imperative {vocab.cloner.suffix_to_e_stem_preview('')}"), lambda: vocab.cloner.suffix_to_e_stem(""))
        create_vocab_note_action(verb_menu, shortcutfinger.up1("ば-form"), lambda: vocab.cloner.create_ba_form())
        create_vocab_note_action(verb_menu, shortcutfinger.up2("{receptive/passive}-form"), lambda: vocab.cloner.create_receptive_form())
        create_vocab_note_action(verb_menu, shortcutfinger.up3("causative"), lambda: vocab.cloner.create_causative_form())
        create_vocab_note_action(verb_menu, shortcutfinger.up4("imperative"), lambda: vocab.cloner.create_imperative())
        create_vocab_note_action(verb_menu, shortcutfinger.down1("Potential-godan"), lambda: vocab.cloner.create_potential_godan())

    def build_misc_menu(misc_menu: QMenu) -> None:
        create_vocab_note_action(misc_menu, shortcutfinger.home1("く-form-of-い-adjective"), lambda: vocab.cloner.create_ku_form())
        create_vocab_note_action(misc_menu, shortcutfinger.home2("さ-form-of-い-adjective"), lambda: vocab.cloner.create_sa_form())
        create_vocab_note_action(misc_menu, shortcutfinger.home3("て-prefixed"), lambda: vocab.cloner.create_te_prefixed_word())
        create_vocab_note_action(misc_menu, shortcutfinger.home4("お-prefixed"), lambda: vocab.cloner.create_o_prefixed_word())
        create_vocab_note_action(misc_menu, shortcutfinger.home5("ん-suffixed"), lambda: vocab.cloner.create_n_suffixed_word())
        create_vocab_note_action(misc_menu, shortcutfinger.up1("か-suffixed"), lambda: vocab.cloner.create_ka_suffixed_word())

    build_forms_menu(non_optional(note_create_menu.addMenu(shortcutfinger.home1("Clone to form"))))
    build_noun_variations_menu(non_optional(note_create_menu.addMenu(shortcutfinger.home2("Noun variations"))))
    build_verb_variations_menu(non_optional(note_create_menu.addMenu(shortcutfinger.home3("Verb variations"))))
    build_misc_menu(non_optional(note_create_menu.addMenu(shortcutfinger.home4("Misc"))))

    if selection:
        build_create_prefix_postfix_note_menu(non_optional(note_create_menu.addMenu(shortcutfinger.up1("Selection"))), vocab, selection)

    if selection:
        pass

    if clipboard:
        build_create_prefix_postfix_note_menu(non_optional(note_create_menu.addMenu(shortcutfinger.up2("Clipboard"))), vocab, clipboard)
</file>

<file path="ui/menus/notes/vocab/main.py">
from __future__ import annotations

from typing import TYPE_CHECKING

import pyperclip
from ankiutils import app, query_builder
from note.note_constants import NoteFields, NoteTypes
from sysutils import ex_str
from sysutils.ex_str import newline
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_lookup_action, add_single_vocab_lookup_action, add_ui_action, add_vocab_dependencies_lookup
from ui.menus.notes.vocab.create_note_menu import build_create_note_menu
from ui.menus.notes.vocab.matching_settings_menu import build_matching_settings_menu

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from PyQt6.QtWidgets import QMenu

def setup_note_menu(note_menu: QMenu, vocab: VocabNote, selection: str, clipboard: str) -> None:
    def build_copy_menu(note_copy_menu: QMenu) -> None:
        note_copy_menu.addAction(shortcutfinger.home1("Question"), lambda: pyperclip.copy(vocab.get_question()))  # pyright: ignore[reportUnknownMemberType]
        note_copy_menu.addAction(shortcutfinger.home2("Answer"), lambda: pyperclip.copy(vocab.get_answer()))  # pyright: ignore[reportUnknownMemberType]
        note_copy_menu.addAction(shortcutfinger.home3("Definition (question:answer)"), lambda: pyperclip.copy(f"""{vocab.get_question()}: {vocab.get_answer()}"""))  # pyright: ignore[reportUnknownMemberType]
        note_copy_menu.addAction(shortcutfinger.home4("Sentences: max 30"), lambda: pyperclip.copy(newline.join([sent.get_question() for sent in vocab.sentences.all()[0:30]])))  # pyright: ignore[reportUnknownMemberType]

    def build_lookup_menu(note_lookup_menu: QMenu) -> None:
        def build_sentences_lookup_menu(sentences_lookup_menu: QMenu) -> None:
            add_lookup_action(sentences_lookup_menu, shortcutfinger.home1("Sentences I'm Studying"), query_builder.notes_lookup(vocab.sentences.studying()))
            add_lookup_action(sentences_lookup_menu, shortcutfinger.home2("Sentences"), query_builder.notes_lookup(vocab.sentences.all()))
            add_lookup_action(sentences_lookup_menu, shortcutfinger.home3("Sentences with primary form"), query_builder.notes_lookup(vocab.sentences.with_primary_form()))
            add_lookup_action(sentences_lookup_menu, shortcutfinger.home4("Sentences with this word highlighted"), query_builder.notes_lookup(vocab.sentences.user_highlighted()))

        def build_vocab_lookup_menu(vocab_lookup_menu: QMenu) -> None:
            def build_readings_menu(readings_vocab_lookup_menu: QMenu) -> None:
                for index, reading in enumerate(vocab.readings.get()):
                    add_lookup_action(readings_vocab_lookup_menu, shortcutfinger.finger_by_priority_order(index, f"Homonyms: {reading}"), query_builder.notes_lookup(app.col().vocab.with_reading(reading)))

            add_lookup_action(vocab_lookup_menu, shortcutfinger.home1("Forms"), query_builder.notes_lookup(vocab.forms.all_list_notes())) # query_builder.notes_lookup(ex_sequence.flatten([app.col().vocab.with_question(form) for form in vocab.forms.all_set()])))
            add_lookup_action(vocab_lookup_menu, shortcutfinger.home2("Compound parts"), query_builder.vocabs_lookup_strings(vocab.compound_parts.all()))
            add_lookup_action(vocab_lookup_menu, shortcutfinger.home3("In compounds"), query_builder.notes_lookup(vocab.related_notes.in_compounds()))
            add_lookup_action(vocab_lookup_menu, shortcutfinger.home4("Synonyms"), query_builder.notes_lookup(vocab.related_notes.synonyms.notes()))
            add_lookup_action(vocab_lookup_menu, shortcutfinger.home5("See also"), query_builder.notes_lookup(vocab.related_notes.see_also.notes()))
            build_readings_menu(non_optional(vocab_lookup_menu.addMenu(shortcutfinger.up1("Homonyms"))))
            add_vocab_dependencies_lookup(vocab_lookup_menu, shortcutfinger.up2("Dependencies"), vocab)

        build_vocab_lookup_menu(non_optional(note_lookup_menu.addMenu(shortcutfinger.home1("Vocab"))))
        build_sentences_lookup_menu(non_optional(note_lookup_menu.addMenu(shortcutfinger.home2("Sentences"))))

        add_lookup_action(note_lookup_menu, shortcutfinger.home3("Kanji"), f"note:{NoteTypes.Kanji} ( {' OR '.join([f'{NoteFields.Kanji.question}:{char}' for char in vocab.get_question()])} )")
        if vocab.related_notes.ergative_twin.get():
            related_vocab = vocab.related_notes
            add_single_vocab_lookup_action(note_lookup_menu, shortcutfinger.home4("Ergative twin"), related_vocab.ergative_twin.get())

    def build_misc_menu(misc_menu: QMenu) -> None:
        add_ui_action(misc_menu, shortcutfinger.home1("Accept meaning"), lambda: vocab.user.answer.set(format_vocab_meaning(vocab.get_answer())), not vocab.user.answer.value)
        add_ui_action(misc_menu, shortcutfinger.home2("Generate answer"), lambda: vocab.generate_and_set_answer())

        from batches import local_note_updater
        add_ui_action(misc_menu, shortcutfinger.home3("Reparse potentially matching sentences: (Only reparse all sentences is sure to catch everything)"), lambda: local_note_updater.reparse_sentences_for_vocab(vocab))
        add_ui_action(misc_menu, shortcutfinger.home4("Repopulate TOS"), lambda: vocab.parts_of_speech.set_automatically_from_dictionary())

        add_ui_action(misc_menu, shortcutfinger.home5("Autogenerate compounds"), lambda: vocab.compound_parts.auto_generate())


    def build_remove_menu(remove_menu: QMenu) -> None:
        add_ui_action(remove_menu, shortcutfinger.home1("User explanation"), lambda: vocab.user.explanation.empty()).setEnabled(vocab.user.explanation.has_value())
        add_ui_action(remove_menu, shortcutfinger.home2("User explanation long"), lambda: vocab.user.explanation_long.empty()).setEnabled(vocab.user.explanation_long.has_value())
        add_ui_action(remove_menu, shortcutfinger.home3("User mnemonic"), lambda: vocab.user.mnemonic.empty()).setEnabled(vocab.user.mnemonic.has_value())
        add_ui_action(remove_menu, shortcutfinger.home4("User answer"), lambda: vocab.user.answer.empty(), True).setEnabled(vocab.user.answer.has_value())

    build_lookup_menu(non_optional(note_menu.addMenu(shortcutfinger.home1("Open"))))
    build_matching_settings_menu(non_optional(note_menu.addMenu(shortcutfinger.home2("Toggle flags"))), vocab)
    build_create_note_menu(non_optional(note_menu.addMenu(shortcutfinger.home3("Create"))), vocab, selection, clipboard)
    build_copy_menu(non_optional(note_menu.addMenu(shortcutfinger.home4("Copy"))))
    build_misc_menu(non_optional(note_menu.addMenu(shortcutfinger.home5("Misc"))))
    build_remove_menu(non_optional(note_menu.addMenu(shortcutfinger.up1("Remove"))))

def format_vocab_meaning(meaning: str) -> str:
    return ex_str.strip_html_and_bracket_markup(meaning.replace(" SOURCE", "").replace(", ", "/").replace(" ", "-").lower())

def build_view_menu(_view_menu: QMenu, _vocab: VocabNote) -> None:  # pyright: ignore
    pass
</file>

<file path="ui/menus/notes/vocab/matching_settings_menu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from PyQt6.QtCore import pyqtBoundSignal
from sysutils.typed import checked_cast, non_optional
from ui.menus.menu_utils import shortcutfinger

if TYPE_CHECKING:
    from collections.abc import Callable

    from note.notefields.require_forbid_flag_field import RequireForbidFlagField
    from note.notefields.tag_flag_field import TagFlagField
    from note.vocabulary.vocabnote import VocabNote
    from PyQt6.QtWidgets import QMenu

def build_matching_settings_menu(toggle_flags_menu: QMenu, vocab: VocabNote) -> None:
    def add_tag_field_check_box(menu: QMenu, title: str, field: TagFlagField) -> None:
        add_checkbox(menu, title, field.is_set, field.set_to)

    def add_checkbox(menu: QMenu, title: str, getter: Callable[[], bool], setter: Callable[[bool], None], reparse_sentences: bool = True) -> None:
        def set_value(value: bool) -> None:
            setter(value)
            if reparse_sentences:
                from batches import local_note_updater
                local_note_updater.reparse_sentences_for_vocab(vocab)
            app.get_ui_utils().refresh()

        action = non_optional(menu.addAction(title))  # pyright: ignore[reportUnknownMemberType]
        action.setCheckable(True)
        action.setChecked(getter())
        checked_cast(pyqtBoundSignal, action.triggered).connect(set_value)  # pyright: ignore[reportUnknownMemberType]

    def build_requires_forbids_menu(requires_forbids_menu: QMenu) -> None:
        def add_require_forbid_menu(menu: QMenu, title: str, field: RequireForbidFlagField, reparse_sentences: bool = True) -> None:
            toggle_menu = non_optional(menu.addMenu(title))
            add_checkbox(toggle_menu, shortcutfinger.home1("Required"), lambda: field.is_configured_required, field.set_required, reparse_sentences)
            add_checkbox(toggle_menu, shortcutfinger.home2("Forbidden"), lambda: field.is_configured_forbidden, field.set_forbidden, reparse_sentences)

        def build_misc_menu(misc_menu: QMenu) -> None:
            add_require_forbid_menu(misc_menu, shortcutfinger.home1("Sentence start"), vocab.matching_configuration.requires_forbids.sentence_start)
            add_require_forbid_menu(misc_menu, shortcutfinger.home2("Sentence end"), vocab.matching_configuration.requires_forbids.sentence_end)
            add_require_forbid_menu(misc_menu, shortcutfinger.home3("exact match"), vocab.matching_configuration.requires_forbids.exact_match)
            add_require_forbid_menu(misc_menu, shortcutfinger.home4("single token"), vocab.matching_configuration.requires_forbids.single_token)

        def build_stem_menu(stem_menu: QMenu) -> None:
            add_require_forbid_menu(stem_menu, shortcutfinger.home1("e stem"), vocab.matching_configuration.requires_forbids.e_stem)
            add_require_forbid_menu(stem_menu, shortcutfinger.home2("a stem"), vocab.matching_configuration.requires_forbids.a_stem)
            add_require_forbid_menu(stem_menu, shortcutfinger.home3("past tense stem"), vocab.matching_configuration.requires_forbids.past_tense_stem)
            add_require_forbid_menu(stem_menu, shortcutfinger.home4("て-form stem"), vocab.matching_configuration.requires_forbids.te_form_stem)

        add_require_forbid_menu(requires_forbids_menu, shortcutfinger.home1("Display: yield to overlapping following compound"), vocab.matching_configuration.requires_forbids.yield_last_token, reparse_sentences=False)
        build_misc_menu(non_optional(requires_forbids_menu.addMenu(shortcutfinger.home2("Misc matching rules"))))
        build_stem_menu(non_optional(requires_forbids_menu.addMenu(shortcutfinger.home3("Stem matching rules"))))

    def build_is_menu(is_menu: QMenu) -> None:
        add_tag_field_check_box(is_menu, shortcutfinger.home1("Poison word"), vocab.matching_configuration.bool_flags.is_poison_word)
        add_tag_field_check_box(is_menu, shortcutfinger.home2("Inflecting word"), vocab.matching_configuration.bool_flags.is_inflecting_word)

    def build_misc_flags_menu(misc_menu: QMenu) -> None:
        add_tag_field_check_box(misc_menu, shortcutfinger.home1("Question overrides form: Show the question in results even if the match was another form"), vocab.matching_configuration.bool_flags.question_overrides_form)
        add_tag_field_check_box(misc_menu, shortcutfinger.home3("Match with preceding vowel"), vocab.matching_configuration.bool_flags.match_with_preceding_vowel)

    build_requires_forbids_menu(non_optional(toggle_flags_menu.addMenu(shortcutfinger.home1("Requireds/Forbids"))))
    build_is_menu(non_optional(toggle_flags_menu.addMenu(shortcutfinger.home2("Is"))))
    build_misc_flags_menu(non_optional(toggle_flags_menu.addMenu(shortcutfinger.home3("Misc"))))
</file>

<file path="ui/menus/notes/vocab/string_menu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_ui_action
from ui.menus.notes.vocab.common import build_create_prefix_postfix_note_menu

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote
    from PyQt6.QtWidgets import QMenu

def build_string_menu(string_menu: QMenu, vocab: VocabNote, menu_string: str) -> None:
    def build_sentences_menu(sentence_menu: QMenu) -> None:
        def remove_highlight_from_sentences() -> None:
            for sent in sentences: sent.configuration.remove_highlighted_word(vocab.get_question())

        def mark_as_incorrect_match_in_sentences() -> None:
            for sent in sentences: sent.configuration.incorrect_matches.add_global(vocab.get_question())

        has_sentences = len(sentences) > 0

        add_ui_action(sentence_menu, shortcutfinger.home1("Add Highlight"),
                      lambda: sentences[0].configuration.add_highlighted_word(vocab.get_question()),
                      has_sentences and vocab.get_question() not in sentences[0].configuration.highlighted_words())
        add_ui_action(sentence_menu, shortcutfinger.home2("Remove highlight"),
                      lambda: remove_highlight_from_sentences(),
                      has_sentences and vocab.get_question() in sentences[0].configuration.highlighted_words())
        add_ui_action(sentence_menu, shortcutfinger.home3("Remove-sentence: Mark as incorrect match in sentence"),
                      lambda: mark_as_incorrect_match_in_sentences(),
                      has_sentences)

    def build_add_menu(vocab_add_menu: QMenu) -> None:
        def build_add_rule_menu(add_rule_menu: QMenu) -> None:
            add_ui_action(add_rule_menu, shortcutfinger.home1("Surface is not"), lambda: vocab.matching_configuration.configurable_rules.surface_is_not.add(menu_string), menu_string not in vocab.matching_configuration.configurable_rules.surface_is_not.get())
            add_ui_action(add_rule_menu, shortcutfinger.home2("Yield to surface"), lambda: vocab.matching_configuration.configurable_rules.yield_to_surface.add(menu_string), menu_string not in vocab.matching_configuration.configurable_rules.yield_to_surface.get())
            add_ui_action(add_rule_menu, shortcutfinger.home3("Prefix is not"), lambda: vocab.matching_configuration.configurable_rules.prefix_is_not.add(menu_string), menu_string not in vocab.matching_configuration.configurable_rules.prefix_is_not.get())
            add_ui_action(add_rule_menu, shortcutfinger.home4("Required prefix"), lambda: vocab.matching_configuration.configurable_rules.required_prefix.add(menu_string), menu_string not in vocab.matching_configuration.configurable_rules.required_prefix.get())
            add_ui_action(add_rule_menu, shortcutfinger.home5("Suffix is not"), lambda: vocab.matching_configuration.configurable_rules.suffix_is_not.add(menu_string), menu_string not in vocab.matching_configuration.configurable_rules.suffix_is_not.get())

        add_ui_action(vocab_add_menu, shortcutfinger.home1("Synonym"), lambda: vocab.related_notes.synonyms.add(menu_string), menu_string not in vocab.related_notes.synonyms.strings())
        add_ui_action(vocab_add_menu, shortcutfinger.home2("Synonyms transitively one level"), lambda: vocab.related_notes.synonyms.add_transitively_one_level(menu_string))
        add_ui_action(vocab_add_menu, shortcutfinger.home3("Confused with"), lambda: vocab.related_notes.confused_with.add(menu_string), menu_string not in vocab.related_notes.confused_with.get())
        add_ui_action(vocab_add_menu, shortcutfinger.home4("Antonym"), lambda: vocab.related_notes.antonyms.add(menu_string), menu_string not in vocab.related_notes.antonyms.strings())
        build_add_rule_menu(non_optional(vocab_add_menu.addMenu(shortcutfinger.home5("Rule"))))
        add_ui_action(vocab_add_menu, shortcutfinger.up1("Form"), lambda: vocab.forms.add(menu_string), menu_string not in vocab.forms.all_set())
        add_ui_action(vocab_add_menu, shortcutfinger.up2("See also"), lambda: vocab.related_notes.see_also.add(menu_string), menu_string not in vocab.related_notes.see_also.strings())
        add_ui_action(vocab_add_menu, shortcutfinger.down1("Perfect synonym, automatically syncronize answers"), lambda: vocab.related_notes.perfect_synonyms.add_overwriting_the_answer_of_the_added_synonym(menu_string), menu_string not in vocab.related_notes.perfect_synonyms.get())

    def build_remove_menu(vocab_remove_menu: QMenu) -> None:
        def build_remove_rule_menu(remove_rule_menu: QMenu) -> None:
            add_ui_action(remove_rule_menu, shortcutfinger.home1("Surface is not"), lambda: vocab.matching_configuration.configurable_rules.surface_is_not.remove(menu_string), menu_string in vocab.matching_configuration.configurable_rules.surface_is_not.get())
            add_ui_action(remove_rule_menu, shortcutfinger.home2("Yield to surface"), lambda: vocab.matching_configuration.configurable_rules.yield_to_surface.remove(menu_string), menu_string in vocab.matching_configuration.configurable_rules.yield_to_surface.get())
            add_ui_action(remove_rule_menu, shortcutfinger.home3("Prefix is not"), lambda: vocab.matching_configuration.configurable_rules.prefix_is_not.remove(menu_string), menu_string in vocab.matching_configuration.configurable_rules.prefix_is_not.get())
            add_ui_action(remove_rule_menu, shortcutfinger.home4("Required prefix"), lambda: vocab.matching_configuration.configurable_rules.required_prefix.remove(menu_string), menu_string in vocab.matching_configuration.configurable_rules.required_prefix.get())
            add_ui_action(remove_rule_menu, shortcutfinger.home5("Suffix is not"), lambda: vocab.matching_configuration.configurable_rules.suffix_is_not.remove(menu_string), menu_string in vocab.matching_configuration.configurable_rules.suffix_is_not.get())

        add_ui_action(vocab_remove_menu, shortcutfinger.home1("Synonym"), lambda: vocab.related_notes.synonyms.remove(menu_string), menu_string in vocab.related_notes.synonyms.strings())
        add_ui_action(vocab_remove_menu, shortcutfinger.home2("Confused with"), lambda: vocab.related_notes.confused_with.remove(menu_string), menu_string in vocab.related_notes.confused_with.get())
        add_ui_action(vocab_remove_menu, shortcutfinger.home3("Antonym"), lambda: vocab.related_notes.antonyms.remove(menu_string), menu_string in vocab.related_notes.antonyms.strings())
        add_ui_action(vocab_remove_menu, shortcutfinger.home4("Ergative twin"), lambda: vocab.related_notes.ergative_twin.remove(), menu_string == vocab.related_notes.ergative_twin.get())
        build_remove_rule_menu(non_optional(vocab_remove_menu.addMenu(shortcutfinger.home5("Rule"))))
        add_ui_action(vocab_remove_menu, shortcutfinger.up1("Form"), lambda: vocab.forms.remove(menu_string), menu_string in vocab.forms.all_set())
        add_ui_action(vocab_remove_menu, shortcutfinger.up2("See also"), lambda: vocab.related_notes.see_also.remove(menu_string), menu_string in vocab.related_notes.see_also.strings())
        add_ui_action(vocab_remove_menu, shortcutfinger.down1("Perfect synonym"), lambda: vocab.related_notes.perfect_synonyms.remove(menu_string), menu_string in vocab.related_notes.perfect_synonyms.get())
        add_ui_action(vocab_remove_menu, shortcutfinger.down2("Derived from"), lambda: vocab.related_notes.derived_from.set(""), menu_string == vocab.related_notes.derived_from.get())

    def build_set_menu(note_set_menu: QMenu) -> None:
        add_ui_action(note_set_menu, shortcutfinger.home1("Ergative twin"), lambda: vocab.related_notes.ergative_twin.set(menu_string))
        add_ui_action(note_set_menu, shortcutfinger.home2("Derived from"), lambda: vocab.related_notes.derived_from.set(menu_string))

    sentences = app.col().sentences.with_question(menu_string)

    build_add_menu(non_optional(string_menu.addMenu(shortcutfinger.home1("Add"))))
    build_set_menu(non_optional(string_menu.addMenu(shortcutfinger.home2("Set"))))
    build_remove_menu(non_optional(string_menu.addMenu(shortcutfinger.home3("Remove"))))
    build_sentences_menu(non_optional(string_menu.addMenu(shortcutfinger.home4("Sentence"))))
    build_create_prefix_postfix_note_menu(non_optional(string_menu.addMenu(shortcutfinger.up1(f"Create combined {menu_string}"))), vocab, menu_string)
</file>

<file path="ui/menus/open_in_anki.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app, query_builder
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger
from ui.menus.menu_utils.ex_qmenu import add_lookup_action_lambda

if TYPE_CHECKING:
    from collections.abc import Callable

    from PyQt6.QtWidgets import QMenu


def build_open_in_anki_menu(open_in_anki_menu:QMenu, search_string:Callable[[],str]) -> None:
    def build_exact_menu(exact_menu:QMenu) -> None:
        add_lookup_action_lambda(exact_menu, shortcutfinger.home1("Open Exact matches | no sentences | reading cards"), lambda: query_builder.exact_matches_no_sentences_reading_cards(search_string()))
        add_lookup_action_lambda(exact_menu, shortcutfinger.home2("Open Exact matches with sentences"), lambda: query_builder.exact_matches(search_string()))


    def build_kanji_menu(kanji_menu:QMenu) -> None:
        add_lookup_action_lambda(kanji_menu, shortcutfinger.home1("All kanji in string"), lambda: query_builder.kanji_in_string(search_string()))
        add_lookup_action_lambda(kanji_menu, shortcutfinger.home2("By reading part"), lambda: query_builder.kanji_with_reading_part(search_string()))
        add_lookup_action_lambda(kanji_menu, shortcutfinger.home3("By reading exact"), lambda: query_builder.notes_lookup(list(app.col().kanji.with_reading(search_string()))))
        add_lookup_action_lambda(kanji_menu, shortcutfinger.home4("With radicals"), lambda: query_builder.kanji_with_radicals_in_string(search_string()))
        add_lookup_action_lambda(kanji_menu, shortcutfinger.up1("With meaning"), lambda: query_builder.kanji_with_meaning(search_string()))

    def build_vocab_menu(vocab_menu:QMenu) -> None:
        add_lookup_action_lambda(vocab_menu, shortcutfinger.home1("form -"), lambda: query_builder.single_vocab_by_form_exact(search_string()))
        add_lookup_action_lambda(vocab_menu, shortcutfinger.home2("form - read card only"), lambda: query_builder.single_vocab_by_form_exact_read_card_only(search_string()))
        add_lookup_action_lambda(vocab_menu, shortcutfinger.home3("form, reading or answer"), lambda: query_builder.single_vocab_by_question_reading_or_answer_exact(search_string()))
        add_lookup_action_lambda(vocab_menu, shortcutfinger.home4("Wildcard"), lambda: query_builder.single_vocab_wildcard(search_string()))
        add_lookup_action_lambda(vocab_menu, shortcutfinger.up1("Text words"), lambda: query_builder.text_vocab_lookup(search_string()))

    def build_sentence_menu(sentence_menu:QMenu) -> None:
        add_lookup_action_lambda(sentence_menu, shortcutfinger.home1("Parse Vocabulary"), lambda: query_builder.sentence_search(search_string()))
        add_lookup_action_lambda(sentence_menu, shortcutfinger.home2("Exact String"), lambda: query_builder.sentence_search(search_string(), exact=True))

    build_exact_menu(non_optional(open_in_anki_menu.addMenu(shortcutfinger.home1("Exact matches"))))
    build_kanji_menu(non_optional(open_in_anki_menu.addMenu(shortcutfinger.home2("Kanji"))))
    build_vocab_menu(non_optional(open_in_anki_menu.addMenu(shortcutfinger.home3("Vocab"))))
    build_sentence_menu(non_optional(open_in_anki_menu.addMenu(shortcutfinger.home4("Sentence"))))
</file>

<file path="ui/menus/web_search.py">
from __future__ import annotations

from typing import TYPE_CHECKING
from urllib import parse

from aqt.utils import openLink
from sysutils.typed import non_optional
from ui.menus.menu_utils import shortcutfinger

if TYPE_CHECKING:
    from collections.abc import Callable

    from PyQt6.QtWidgets import QMenu


def build_web_search_menu(search_web_menu:QMenu, search_string:Callable[[],str]) -> None:
    def add_web_lookup(menu: QMenu, name: str, url: str) -> None:
        menu.addAction(name, lambda: openLink(url % parse.quote(search_string(), encoding="utf8")))  # pyright: ignore[reportUnknownMemberType]

    def set_up_kanji_menu(kanji_lookup_menu:QMenu) -> None:
        add_web_lookup(kanji_lookup_menu, shortcutfinger.home1("Kanji explosion"), "https://www.kurumi.com/jp/kjbh/?k=%s")
        add_web_lookup(kanji_lookup_menu, shortcutfinger.home2("Kanshudo"), "https://www.kanshudo.com/search?q=%s")
        add_web_lookup(kanji_lookup_menu, shortcutfinger.home3("Kanji map"), "https://thekanjimap.com/%s")

    def set_up_sentences_menu(sentences_lookup_menu:QMenu) -> None:
        add_web_lookup(sentences_lookup_menu, shortcutfinger.home1("Sentences: Immersion Kit"), "https://www.immersionkit.com/dictionary?exact=true&sort=sentence_length%%3Aasc&keyword=%s")
        add_web_lookup(sentences_lookup_menu, shortcutfinger.home2("Sentences: Tatoeba"), "https://tatoeba.org/en/sentences/search?from=jpn&to=eng&query=%s")

    def set_up_misc_menu(misc_lookup_menu:QMenu) -> None:
        def set_up_translate_menu(translate_menu: QMenu) -> None:
            add_web_lookup(translate_menu, shortcutfinger.home1("Translate: Deepl"), "https://www.deepl.com/en/translator#ja/en/%s")
            add_web_lookup(translate_menu, shortcutfinger.home2("Translate: Kanshudo"), "https://www.kanshudo.com/sentence_translate?q=%s")

        def set_up_images_menu(images_menu: QMenu) -> None:
            add_web_lookup(images_menu, shortcutfinger.home1("Images: Google"), "https://www.google.com/search?udm=2&tbs=sur:cl&q=%s")
            add_web_lookup(images_menu, shortcutfinger.home2("Images: Bing"), "https://www.bing.com/images/search?qft=+filterui:licenseType-Any&q=%s")

        def set_up_conjugate_menu(conjugate_menu: QMenu) -> None:
            add_web_lookup(conjugate_menu, shortcutfinger.home1("Conjugate: Japanese verb conjugator"), "https://www.japaneseverbconjugator.com/VerbDetails.asp?Go=Conjugate&txtVerb=%s")
            add_web_lookup(conjugate_menu, shortcutfinger.home2("Conjugate: Verbix"), "https://www.verbix.com/webverbix/japanese/%s")

        def set_up_grammar_menu(grammar_lookup_menu: QMenu) -> None:
            add_web_lookup(grammar_lookup_menu, shortcutfinger.home1("Grammar: Google"), "https://www.google.com/search?q=japanese+grammar+%s")
            add_web_lookup(grammar_lookup_menu, shortcutfinger.home2("Grammar: Japanese with anime"), "https://www.google.com/search?q=site:www.japanesewithanime.com+%s")
            add_web_lookup(grammar_lookup_menu, shortcutfinger.home3("Grammar: Wiktionary"), "https://en.wiktionary.org/wiki/%s")

        set_up_conjugate_menu(non_optional(misc_lookup_menu.addMenu(shortcutfinger.home1("Conjugate"))))
        set_up_translate_menu(non_optional(misc_lookup_menu.addMenu(shortcutfinger.home2("Translate"))))
        set_up_grammar_menu(non_optional(misc_lookup_menu.addMenu(shortcutfinger.home3("Grammar"))))
        set_up_images_menu(non_optional(misc_lookup_menu.addMenu(shortcutfinger.home4("Images"))))

    def set_up_lookup_menu(lookup_menu:QMenu) -> None:
        add_web_lookup(lookup_menu, shortcutfinger.home1("English: Merriam Webster"), "https://www.merriam-webster.com/dictionary/%s")
        add_web_lookup(lookup_menu, shortcutfinger.home2("Wiktionary"), "https://en.wiktionary.org/wiki/%s")
        add_web_lookup(lookup_menu, shortcutfinger.home3("Lookup: Takoboto"), "https://takoboto.jp/?q=%s")
        add_web_lookup(lookup_menu, shortcutfinger.home4("Lookup: Jisho"), "https://jisho.org/search/%s")
        add_web_lookup(lookup_menu, shortcutfinger.up1("Lookup: Wanikani"), "https://www.wanikani.com/search?query=%s")
        add_web_lookup(lookup_menu, shortcutfinger.down1("Lookup: Word Kanshudo"), "https://www.kanshudo.com/searchw?q=%s")

    set_up_kanji_menu(non_optional(search_web_menu.addMenu(shortcutfinger.home1("Kanji"))))
    set_up_sentences_menu(non_optional(search_web_menu.addMenu(shortcutfinger.home2("Sentences"))))
    set_up_misc_menu(non_optional(search_web_menu.addMenu(shortcutfinger.home3("Misc"))))
    set_up_lookup_menu(non_optional(search_web_menu.addMenu(shortcutfinger.home4("Lookup"))))
</file>

<file path="ui/open_note/open_note_dialog.py">
from __future__ import annotations

import threading
from typing import TYPE_CHECKING, cast, final

from anki.notes import NoteId
from ankiutils.app import col
from note.jpnote import JPNote
from note.kanjinote import KanjiNote
from note.sentences.sentencenote import SentenceNote
from note.vocabulary.vocabnote import VocabNote
from PyQt6.QtCore import Qt, pyqtBoundSignal
from PyQt6.QtWidgets import QDialog, QHBoxLayout, QHeaderView, QLabel, QLineEdit, QProgressBar, QTableWidget, QTableWidgetItem, QVBoxLayout, QWidget
from sysutils import ex_str, kana_utils, typed
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from collections.abc import Callable


@final
class NoteSearchDialog(QDialog): # Cannot inherit Slots for some QT internal reason
    # Singleton instance
    _instance: NoteSearchDialog | None = None

    @classmethod
    def instance(cls) -> NoteSearchDialog:
        """Access to the singleton instance, creating it if needed."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self, parent: QWidget | None = None) -> None:
        super().__init__(parent)
        self.setWindowTitle("Find Notes")
        self.resize(1800, 1100)

        # Set the window to be always on top but not modal
        self.setWindowFlags(self.windowFlags() | Qt.WindowType.WindowStaysOnTopHint)

        self.matched_notes: list[JPNote] = []
        self._lock = threading.Lock()
        self._max_results = 100  # Maximum number of results to show

        # Rest of the initialization code...

        layout = QVBoxLayout(self)

        # Create search input field
        search_layout = QHBoxLayout()
        search_label = QLabel("Search:")
        self.search_input = QLineEdit()
        self.search_input.setPlaceholderText("Separate multiple conditions with &&  r:readings-only-condition, a:answer-only-condition, q:question-only-condition")
        search_layout.addWidget(search_label)
        search_layout.addWidget(self.search_input)
        layout.addLayout(search_layout)

        # Create status and progress indicator area right below search
        status_layout = QHBoxLayout()
        self.status_label = QLabel("Hit enter to search")
        self.status_label.setMinimumHeight(25)  # Give it some height to prevent layout shifting

        # Create progress bar for busy indicator
        self.progress_bar = QProgressBar()
        self.progress_bar.setRange(0, 0)  # Indeterminate mode
        self.progress_bar.setMinimumHeight(20)  # Make it a bit taller
        self.progress_bar.hide()  # Hidden by default

        status_layout.addWidget(self.status_label)
        status_layout.addWidget(self.progress_bar, 1)  # 1 = stretch factor to take up available space
        layout.addLayout(status_layout)

        # Create results table
        self.results_table = QTableWidget()
        self.results_table.setColumnCount(3)
        self.results_table.setHorizontalHeaderLabels(["Type", "Question", "Answer"])  # pyright: ignore[reportUnknownMemberType]
        non_optional(self.results_table.horizontalHeader()).setSectionResizeMode(QHeaderView.ResizeMode.Interactive)  # Make all columns resizable
        self.results_table.setColumnWidth(1, 200)  # Set Question column width to 200 pixels
        self.results_table.setEditTriggers(QTableWidget.EditTrigger.NoEditTriggers)
        self.results_table.setSelectionBehavior(QTableWidget.SelectionBehavior.SelectRows)
        layout.addWidget(self.results_table)

        # Connect signals
        typed.checked_cast(pyqtBoundSignal, self.search_input.returnPressed).connect(self.perform_search)  # pyright: ignore[reportUnknownMemberType]
        typed.checked_cast(pyqtBoundSignal, self.results_table.cellDoubleClicked).connect(self.on_cell_double_clicked)  # pyright: ignore[reportUnknownMemberType]
        # Add selection change signal to handle keyboard/mouse selection
        typed.checked_cast(pyqtBoundSignal, self.results_table.itemSelectionChanged).connect(self.on_selection_changed)  # pyright: ignore[reportUnknownMemberType]

        self.search_input.setFocus()

    def perform_search(self) -> None:
        with self._lock:
            """Perform the actual search and update the results table"""
            search_text = self.search_input.text().strip()
            if not search_text:
                self.results_table.setRowCount(0)
                self.matched_notes = []
                return

            # Show the busy indicator
            self.status_label.setText("Searching...")
            self.progress_bar.show()

            # Process events to ensure UI updates
            from PyQt6.QtWidgets import QApplication
            QApplication.processEvents()

            try:
                # Get the JP collection
                matching_notes: list[JPNote] = []

                def kanji_readings(note: KanjiNote) -> str:
                    return " ".join(note.get_readings_clean())

                def vocab_readings(vocab: VocabNote) -> str:
                    return " ".join(vocab.readings.get())

                def vocab_romaji_readings(vocab: VocabNote) -> str:
                    return kana_utils.romanize(vocab_readings(vocab))

                def vocab_forms(vocab: VocabNote) -> str:
                    return " ".join(vocab.forms.without_noise_characters())

                def question_length(note: JPNote) -> int:
                    return len(note.get_question())

                def kanji_romaji_readings(note: KanjiNote) -> str:
                    return note.get_romaji_readings()

                def note_question(note_: JPNote) -> str:
                    return ex_str.strip_html_and_bracket_markup(note_.get_question())

                def note_answer(note_: JPNote) -> str:
                    return ex_str.strip_html_and_bracket_markup(note_.get_answer())

                # Search in kanji notes
                matching_notes.extend(self._search_in_notes(
                    self._max_results - len(matching_notes),
                    col().kanji.all(),
                    search_text,
                    kanji_readings=kanji_readings,
                    kanji_romaji_readings=kanji_romaji_readings,
                    question=note_question,
                    answer=note_answer
                ))

                # Search in vocab notes
                matching_notes.extend(self._search_in_notes(
                    self._max_results - len(matching_notes),
                    sorted(col().vocab.all(), key=question_length),
                    search_text,
                    vocab_readings=vocab_readings,
                    vocab_romaji_readings=vocab_romaji_readings,
                    forms=vocab_forms,
                    question=note_question,
                    answer=note_answer
                ))

                # Search in sentence notes
                matching_notes.extend(self._search_in_notes(
                    self._max_results - len(matching_notes),
                    sorted(col().sentences.all(), key=question_length),
                    search_text,
                    question=note_question,
                    answer=note_answer
                ))

                self.matched_notes = matching_notes
                self._update_results_table()

                # Update status with result count
                result_count = len(matching_notes)
                if result_count == 0:
                    self.status_label.setText("No results found")
                elif result_count >= self._max_results:
                    self.status_label.setText(f"{self._max_results}+ notes found. Showing first {self._max_results}")
                else:
                    self.status_label.setText(f"{result_count} note{'s' if result_count != 1 else ''} found")

            finally:
                # Hide the busy indicator
                self.progress_bar.hide()


    @staticmethod
    def _search_in_notes[TNote: JPNote](max_notes: int, notes: list[TNote], search_text: str, **extractors: Callable[[TNote], str]) -> list[JPNote]:
        matches: list[JPNote] = []

        if max_notes <= 0:
            return []

        # Split search text by " && " to get multiple conditions
        search_conditions = [condition.strip() for condition in search_text.split(" && ")]

        for note in notes:
            all_conditions_match = True

            for condition in search_conditions:
                condition_matches = False
                condition_lower = condition.lower()

                # Check for prefixed search
                if condition.startswith("r:"):
                    # Only search in reading fields
                    reading_value = condition[2:].strip().lower()
                    reading_fields = {extractor_name: extractor for extractor_name, extractor in extractors.items() if "reading" in extractor_name.lower()}

                    if not reading_fields:
                        # Skip this note type if it doesn't have reading fields
                        all_conditions_match = False
                        break

                    for extractor in reading_fields.values():
                        field_text = extractor(note)
                        clean_field = ex_str.strip_html_and_bracket_markup(field_text).lower()
                        if reading_value in clean_field:
                            condition_matches = True
                            break
                elif condition.startswith("a:"):
                    # Only search in answer field
                    answer_value = condition[2:].strip().lower()
                    if "answer" in extractors:
                        field_text = extractors["answer"](note)
                        clean_field = ex_str.strip_html_and_bracket_markup(field_text).lower()
                        if answer_value in clean_field:
                            condition_matches = True
                elif condition.startswith("q:"):
                    # Only search in question field
                    question_value = condition[2:].strip().lower()
                    if "question" in extractors:
                        field_text = extractors["question"](note)
                        clean_field = ex_str.strip_html_and_bracket_markup(field_text).lower()
                        if question_value in clean_field:
                            condition_matches = True
                else:
                    # Standard search in all fields
                    for extractor in extractors.values():
                        field_text = extractor(note)
                        clean_field = ex_str.strip_html_and_bracket_markup(field_text).lower()
                        if condition_lower in clean_field:
                            condition_matches = True
                            break

                if not condition_matches:
                    all_conditions_match = False
                    break

            if all_conditions_match:
                matches.append(note)
                if len(matches) >= max_notes:
                    return matches

        return matches

    @staticmethod
    def _create_item(text: str, is_question: bool = False) -> QTableWidgetItem:
        item = QTableWidgetItem()
        item.setText(ex_str.strip_html_and_bracket_markup(text))

        # Apply special formatting to question column
        if is_question:
            font = item.font()
            font.setFamily("Meiryo UI")
            font.setPointSize(int(font.pointSize() * 1.5))  # Increase font size by 50%
            item.setFont(font)

        return item

    def _update_results_table(self) -> None:
        self.results_table.setRowCount(0)

        for i, note in enumerate(self.matched_notes):
            self.results_table.insertRow(i)

            # Create type column
            self.results_table.setItem(i, 0, QTableWidgetItem(self._get_note_type_display(note)))
            non_optional(self.results_table.item(i, 0)).setData(Qt.ItemDataRole.UserRole, note.get_id())

            self.results_table.setItem(i, 1, self._create_item(note.get_question(), is_question=True))
            self.results_table.setItem(i, 2, self._create_item(note.get_answer()))

        self.results_table.resizeColumnsToContents()

    @staticmethod
    def _get_note_type_display(note: JPNote) -> str:
        """Get a display name for the note type"""
        if isinstance(note, VocabNote):
            return "Vocab"
        if isinstance(note, KanjiNote):
            return "Kanji"
        if isinstance(note, SentenceNote):
            return "Sentence"
        return "Note"

    def on_selection_changed(self) -> None:
        selected_rows = self.results_table.selectedItems()
        if not selected_rows:
            return

        self.open_notes_at_rows({item.row() for item in selected_rows})

    def on_cell_double_clicked(self, row: int, _column: int) -> None:
        self.open_notes_at_rows({row})

    def open_notes_at_rows(self, rows: set[int]) -> None:
        note_ids = []
        for row in rows:
            note_id = cast(NoteId, non_optional(self.results_table.item(row, 0)).data(Qt.ItemDataRole.UserRole))
            if note_id:
                note_ids.append(note_id)  # pyright: ignore[reportUnknownMemberType]

        if note_ids:
            from ankiutils import query_builder, search_executor
            search_executor.do_lookup_and_show_previewer(query_builder.notes_by_id(note_ids))  # pyright: ignore[reportUnknownArgumentType]
            self.instance().activateWindow()  # the search will lose our focus, reactivate it

    @classmethod
    def toggle_dialog_visibility(cls) -> None:
        if cls.instance().isVisible():
            cls.instance().hide()
            return

        cls.instance().show()
        cls.instance().raise_()
        cls.instance().activateWindow()
        cls.instance().search_input.setFocus()
</file>

<file path="ui/timing_hacks.py">
from __future__ import annotations

import time
from typing import TYPE_CHECKING

from ankiutils.audio_suppressor import audio_suppressor
from aqt import gui_hooks
from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from anki.cards import Card
    from anki.notes import Note


class UglyUITimingBasedHacksData(AutoSlots):
    def __init__(self) -> None:
        self._last_editor_typing_time: float = 0.0
        self._last_reviewer_showed_answer_time: float = 0.0

    def typed_in_note(self, _note: Note) -> None:
        audio_suppressor.suppress_for_seconds(.1)
        self._last_editor_typing_time = time.time()

    def reviewer_showed_answer(self, _card:Card) -> None:
        #audio_suppressor.suppress_for_seconds(.3) #I keep not getting audio when quickly moving through audio cards
        self._last_reviewer_showed_answer_time = time.time()

ugly_timing_hacks = UglyUITimingBasedHacksData()

def on_reviewer_show_answer(_card: Card) -> None: ugly_timing_hacks.reviewer_showed_answer(_card)
def typed_in_editor(note:Note) -> None: ugly_timing_hacks.typed_in_note(note)

def init() -> None:
    gui_hooks.reviewer_did_show_answer.append(on_reviewer_show_answer)
    gui_hooks.editor_did_fire_typing_timer.append(typed_in_editor)  # pyright: ignore[reportUnknownMemberType]
</file>

<file path="ui/tools_menu.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app, ui_utils
from ankiutils.app import get_ui_utils, main_window
from batches import local_note_updater
from configuration.configuration import show_japanese_options
from configuration.readings_mapping_dialog import show_readings_mappings
from PyQt6.QtCore import pyqtBoundSignal
from PyQt6.QtGui import QAction
from PyQt6.QtWidgets import QInputDialog, QLineEdit, QMenu
from sysutils import object_instance_tracker
from sysutils.typed import checked_cast, non_optional
from ui.menus.menu_utils import ex_qmenu, shortcutfinger
from ui.menus.open_in_anki import build_open_in_anki_menu
from ui.menus.web_search import build_web_search_menu
from ui.open_note.open_note_dialog import NoteSearchDialog

if TYPE_CHECKING:
    from collections.abc import Callable


def refresh() -> None:
    if not app.is_initialized():
        return

    note = ui_utils.try_get_review_note()
    if note:
        note.update_generated_data()

def add_menu_ui_action(sub_menu: QMenu, heading: str, callback: Callable[[], None], shortcut: str = "") -> None:
    action = QAction(heading, main_window())
    if shortcut: action.setShortcut(shortcut)

    def ui_callback() -> None:
        get_ui_utils().run_ui_action(callback)

    checked_cast(pyqtBoundSignal, action.triggered).connect(ui_callback)  # pyright: ignore[reportUnknownMemberType]
    sub_menu.addAction(action)  # pyright: ignore[reportUnknownMemberType]

def build_main_menu() -> None:
    my_menu = non_optional(main_window().form.menubar.addMenu(shortcutfinger.home1("Japanese")))

    build_config_menu(non_optional(my_menu.addMenu(shortcutfinger.home1("Config"))))
    build_lookup_menu(non_optional(my_menu.addMenu(shortcutfinger.home2("Lookup"))))
    build_local_menu(non_optional(my_menu.addMenu(shortcutfinger.home3("Local Actions"))))
    build_debug_menu(non_optional(my_menu.addMenu(shortcutfinger.home4("Debug"))))

def build_lookup_menu(lookup_menu: QMenu) -> None:
    def get_text_input() -> str:
        text, ok = QInputDialog.getText(None, "input", "enter text", QLineEdit.EchoMode.Normal, "")
        return text if ok and text else ""

    lookup_menu.addAction(shortcutfinger.home1("Open note Ctrl+o"), lambda: NoteSearchDialog.toggle_dialog_visibility())  # pyright: ignore[reportUnknownMemberType]
    build_open_in_anki_menu(non_optional(lookup_menu.addMenu(shortcutfinger.home2("Anki"))), get_text_input)
    build_web_search_menu(non_optional(lookup_menu.addMenu(shortcutfinger.home3("Web"))), get_text_input)

def build_debug_menu(debug_menu: QMenu) -> None:
    debug_menu.addAction(shortcutfinger.home1("Show instance report"), lambda: app.get_ui_utils().tool_tip(object_instance_tracker.single_line_report(), 10000))  # pyright: ignore[reportUnknownMemberType]
    debug_menu.addAction(shortcutfinger.home2("Take Snapshot"), object_instance_tracker.take_snapshot)  # pyright: ignore[reportUnknownMemberType]
    debug_menu.addAction(shortcutfinger.home3("Show current snapshot diff"), lambda: app.get_ui_utils().tool_tip(object_instance_tracker.current_snapshot().single_line_diff_report(), 10000))  # pyright: ignore[reportUnknownMemberType]
    debug_menu.addAction(shortcutfinger.home4("Show diff against first snapshot"), lambda: app.get_ui_utils().tool_tip(object_instance_tracker.create_transient_snapshot_against_first_snapshot().single_line_diff_report(), 10000))  # pyright: ignore[reportUnknownMemberType]
    debug_menu.addAction(shortcutfinger.home5("Show diff against current snapshot"), lambda: app.get_ui_utils().tool_tip(object_instance_tracker.create_transient_snapshot_against_last_snapshot().single_line_diff_report(), 10000))  # pyright: ignore[reportUnknownMemberType]  # pyright: ignore[reportUnknownMemberType]
    debug_menu.addAction(shortcutfinger.up1("Run GC and report"), local_note_updater.print_gc_status_and_collect)  # pyright: ignore[reportUnknownMemberType]
    debug_menu.addAction(shortcutfinger.up2("Reset"), app.reset)  # pyright: ignore[reportUnknownMemberType]
    add_menu_ui_action(debug_menu, shortcutfinger.down1("Refresh UI ('F5')"), refresh)

def build_config_menu(config_menu: QMenu) -> None:
    def build_feature_toggles_menu(_title: str) -> None:
        section_index = 0
        toggles_menu = non_optional(config_menu.addMenu(_title))
        for section_index, (section, toggles) in enumerate(app.config().feature_toggles):
            section_menu = non_optional(toggles_menu.addMenu(shortcutfinger.finger_by_priority_order(section_index, section)))
            ex_qmenu.build_checkbox_config_section_menu(section_menu, toggles)

        add_menu_ui_action(toggles_menu, shortcutfinger.finger_by_priority_order(section_index + 1, "Toggle all sentence auto yield compound last token flags (Ctrl+Shift+Alt+d)"), app.config().toggle_all_sentence_display_auto_yield_flags)

    build_feature_toggles_menu(shortcutfinger.home1("Feature Toggles"))
    non_optional(config_menu.addAction(shortcutfinger.home2("Readings mappings"), show_readings_mappings)).setShortcut("Ctrl+Shift+m")  # pyright: ignore[reportUnknownMemberType]
    non_optional(config_menu.addAction(shortcutfinger.home3("Options"), show_japanese_options)).setShortcut("Ctrl+Shift+s")  # pyright: ignore[reportUnknownMemberType]

def build_local_menu(local_menu: QMenu) -> None:
    def build_update_menu(update_menu: QMenu) -> None:
        add_menu_ui_action(update_menu, shortcutfinger.home1("Vocab"), local_note_updater.update_vocab)
        add_menu_ui_action(update_menu, shortcutfinger.home2("Kanji"), local_note_updater.update_kanji)
        add_menu_ui_action(update_menu, shortcutfinger.home3("Sentences"), local_note_updater.update_sentences)
        add_menu_ui_action(update_menu, shortcutfinger.home4("Tag note metadata"), local_note_updater.tag_note_metadata)
        add_menu_ui_action(update_menu, shortcutfinger.home5("All the above"), local_note_updater.update_all)
        add_menu_ui_action(update_menu, shortcutfinger.up1("Reparse sentences"), local_note_updater.reparse_all_sentences)
        add_menu_ui_action(update_menu, shortcutfinger.down1("All the above: Full rebuild"), local_note_updater.full_rebuild)


    build_update_menu(non_optional(local_menu.addMenu(shortcutfinger.home1("Update"))))

    add_menu_ui_action(local_menu, shortcutfinger.home2("Convert &Immersion Kit sentences"), local_note_updater.convert_immersion_kit_sentences)
    add_menu_ui_action(local_menu, shortcutfinger.home3("Update everyting except reparsing sentences"), local_note_updater.update_all)

def init() -> None:
    build_main_menu()
</file>

<file path="ui/web/__init__.py">
from __future__ import annotations

from ui.web import kanji, sentence, vocab


def init() -> None:
    kanji.init()
    vocab.init()
    sentence.init()
</file>

<file path="ui/web/kanji/__init__.py">
from __future__ import annotations


def init() -> None:
    from . import dependencies, kanji_list, mnemonic, readings, vocab_list
    dependencies.init()
    mnemonic.init()
    readings.init()
    vocab_list.init()
    kanji_list.init()
</file>

<file path="ui/web/kanji/dependencies.py">
from __future__ import annotations

import re

from aqt import gui_hooks
from note.kanjinote import KanjiNote
from sysutils import ex_str, kana_utils
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer


def render_dependencies_list(note: KanjiNote) -> str:
    readings = note.get_readings_clean()

    # noinspection DuplicatedCode
    def highlight_primary_reading_sources(text: str) -> str:
        for reading in readings:
            text = re.sub(rf"\b{re.escape(kana_utils.katakana_to_hiragana(reading))}\b", f"<primary-reading-source>{kana_utils.katakana_to_hiragana(reading)}</primary-reading-source>", text)
            text = re.sub(rf"\b{re.escape(kana_utils.hiragana_to_katakana(reading))}\b", f"<primary-reading-source>{kana_utils.hiragana_to_katakana(reading)}</primary-reading-source>", text)

        return text

    dependencies = note.get_radicals_notes()

    def format_readings(_kanji: KanjiNote) -> str:
        separator = """<span class="readingsSeparator">|</span>"""

        readings_on = ", ".join([kana_utils.hiragana_to_katakana(reading) for reading in _kanji.get_reading_on_list_html()])
        readings_kun = ", ".join(_kanji.get_reading_kun_list_html())

        return f"""{readings_on} {separator} {readings_kun}"""

    if dependencies:
        return f"""
<div id="dependencies_list" class="page_section">
    <div class="page_section_title">radicals</div>
{ex_str.newline.join(f'''
    <div class="dependency {" ".join(kanji.get_meta_tags())}">
        <div class="dependency_heading">
            <div class="dependency_character clipboard">{kanji.get_question()}</div>
            <div class="dependency_name clipboard">{kanji.get_answer()}</div>
            <div class="dependency_readings">{highlight_primary_reading_sources(format_readings(kanji))}</div>
        </div>
        <div class="dependency_mnemonic">{kanji.get_active_mnemonic()}</div>
    </div>
''' for kanji in dependencies)}
</div>
        """

    return ""


def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(KanjiNote, {"##DEPENDENCIES_LIST##": render_dependencies_list}).render)
</file>

<file path="ui/web/kanji/kanji_list.py">
from __future__ import annotations

import re
from typing import TYPE_CHECKING

from ankiutils import app
from aqt import gui_hooks
from note.kanjinote import KanjiNote
from sysutils import ex_str, kana_utils
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer
from viewmodels.kanji_list.sentence_kanji_viewmodel import KanjiViewModel

if TYPE_CHECKING:
    from note.jpnote import JPNote


def render_list(note:JPNote, kanjis:list[KanjiNote], kanji_readings:list[str]) -> str:
    if not kanjis:
        return ""

    # noinspection DuplicatedCode
    def highlight_inherited_reading(text: str) -> str:
        for reading in kanji_readings:
            text = re.sub(rf"\b{re.escape(kana_utils.katakana_to_hiragana(reading))}\b", f"<inherited-reading>{kana_utils.katakana_to_hiragana(reading)}</inherited-reading>", text)
            text = re.sub(rf"\b{re.escape(kana_utils.hiragana_to_katakana(reading))}\b", f"<inherited-reading>{kana_utils.hiragana_to_katakana(reading)}</inherited-reading>", text)

        return text

    def prefer__studying_kanji(kan: KanjiNote) -> int:
        return 0 if kan.is_studying() else 1

    kanjis = [kan for kan in kanjis if kan != note]
    kanjis = sorted(kanjis, key=lambda kan: prefer__studying_kanji(kan))

    viewmodels = [KanjiViewModel(kanji) for kanji in kanjis]

    return f"""
    <div id="kanji_list" class="page_section">
        <div class="page_section_title">kanji</div>
    {ex_str.newline.join(f'''
        <div class="kanji_item {" ".join(kanji.kanji.get_meta_tags())}">
            <div class="kanji_main">
                <span class="kanji_kanji clipboard">{kanji.question()}</span>
                <span class="kanji_readings">{highlight_inherited_reading(kanji.readings())}</span>
                <span class="kanji_answer">{kanji.answer()}</span>
            </div>
            <div class="kanji_mnemonic">{kanji.mnemonic()}</div>
        </div>
    ''' for kanji in viewmodels)}
    </div>
            """


def kanji_kanji_list(kanji:KanjiNote) -> str:
    kanjis = app.col().kanji.with_radical(kanji.get_question())
    kanji_readings = kanji.get_readings_clean()

    return render_list(kanji, kanjis, kanji_readings)


def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(KanjiNote, {"##KANJI_LIST##": kanji_kanji_list}).render)
</file>

<file path="ui/web/kanji/mnemonic.py">
from __future__ import annotations

from aqt import gui_hooks
from note.kanjinote import KanjiNote
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer


def render_mnemonic(note: KanjiNote) -> str:
    return note.get_active_mnemonic()

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(KanjiNote, {"##MNEMONIC##": render_mnemonic}).render)
</file>

<file path="ui/web/kanji/readings.py">
from __future__ import annotations

from aqt import gui_hooks
from note.kanjinote import KanjiNote
from sysutils import kana_utils
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer


def render_katakana_onyomi(kanji_note: KanjiNote) -> str:
    on_readings_list = [kana_utils.hiragana_to_katakana(x) for x in kanji_note.get_reading_on_list_html()]
    on_readings = ", ".join(f"""<span class="clipboard">{reading}</span>""" for reading in on_readings_list)
    kun_readings = ", ".join(f"""<span class="clipboard">{reading}</span>""" for reading in kanji_note.get_reading_kun_list_html())
    nan_readings = ", ".join(f"""<span class="clipboard">{reading}</span>""" for reading in kanji_note.get_reading_nan_list_html())

    return f"""
     <span class="reading">{on_readings}</span> <span class="readingsSeparator">|</span>
     <span class="reading">{kun_readings}</span> <span class="readingsSeparator">|</span>
     <span class="reading">{nan_readings}</span>
"""



def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(KanjiNote, {"##KANJI_READINGS##": render_katakana_onyomi}).render)
</file>

<file path="ui/web/kanji/vocab_list.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from aqt import gui_hooks
from note.kanjinote import KanjiNote
from sysutils.ex_str import newline
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer

if TYPE_CHECKING:
    from note.vocabulary.vocabnote import VocabNote


def generate_vocab_html_list(_kanji_note: KanjiNote) -> str:
    def _create_classes(_kanji: KanjiNote, _vocab: VocabNote) -> str:
        # noinspection DuplicatedCode
        tags = list(_vocab.meta_data.priority_spec().tags)
        tags.sort()
        classes = " ".join([f"""common_ness_{prio}""" for prio in tags])
        classes += f""" {_vocab.meta_data.priority_spec().priority_string}"""
        classes += " " + " ".join(_vocab.get_meta_tags())

        if _vocab.get_question() in primary_vocab or (_vocab.readings.get() and _vocab.readings.get()[0] in _kanji.get_primary_vocab()):
            classes += " primary_vocab" if has_real_primary_vocabs else " default_primary_vocab"

        if _kanji_note.get_question() not in _vocab.get_question():
            classes += " not_matching_kanji"

        return classes

    vocabs = _kanji_note.get_vocab_notes_sorted()
    primary_vocab = _kanji_note.get_primary_vocabs_or_defaults()
    has_real_primary_vocabs = any(_kanji_note.get_primary_vocab())

    if vocabs:
        return f'''
                <div class="kanjiVocabList page_section">
                    <div class="page_section_title">vocabulary</div>
                    <div>

                    {newline.join([f"""
                    <div class="kanjiVocabEntry {_create_classes(_kanji_note, _vocab_note)}">
                        <audio src="{_vocab_note.audio.get_primary_audio_path()}"></audio><a class="play-button"></a>
                        <span class="kanji clipboard">{_vocab_note.get_question()}</span>
                        (<span class="clipboard vocabReading">{", ".join(_kanji_note.tag_vocab_readings(_vocab_note))}</span>)
                        {_vocab_note.meta_data.meta_tags_html(True)}
                        <span class="meaning"> {_vocab_note.get_answer()}</span>
                    </div>
                    """ for _vocab_note in vocabs])}

                    </div>
                </div>
                '''
    return ""

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(KanjiNote, {"##VOCAB_LIST##": generate_vocab_html_list}).render)
</file>

<file path="ui/web/sentence/__init__.py">
from __future__ import annotations

from ui.web.sentence import question


def init() -> None:
    from . import ud_sentence_breakdown
    ud_sentence_breakdown.init()
    question.init()
</file>

<file path="ui/web/sentence/candidate_word_variant_viewmodel.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from sysutils.debug_repr_builder import SkipFalsyValuesDebugReprBuilder
from sysutils.weak_ref import WeakRef, WeakRefable
from ui.web.sentence.match_viewmodel import MatchViewModel

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.candidate_word_variant import CandidateWordVariant

class CandidateWordVariantViewModel(WeakRefable, AutoSlots):
    def __init__(self, variant: CandidateWordVariant) -> None:
        self.candidate_word: CandidateWordVariant = variant
        self.weakref: WeakRef[CandidateWordVariantViewModel] = WeakRef(self)
        self.is_shadowed: bool = variant.word.is_shadowed
        self.is_display_word: bool = variant in variant.word.analysis.display_word_variants
        self.matches: list[MatchViewModel] = [MatchViewModel(self.weakref, match) for match in variant.matches]
        self.display_matches: list[MatchViewModel] = [match for match in self.matches if match.match.is_displayed]
        self.has_perfect_match: bool = any(match.match_owns_form for match in self.matches if match.match.is_displayed)

        self.primary_display_forms: list[MatchViewModel] = [form for form in self.matches if form.match.is_displayed]
        self.matches = sorted(self.matches, key=lambda match_vm: 0 if match_vm.match.is_displayed else 1)

    @override
    def __repr__(self) -> str: return (
        SkipFalsyValuesDebugReprBuilder()
        .include(self.candidate_word.form)
        .flag("display_word", self.is_display_word)
        .flag("shadowed", self.is_shadowed).repr)
</file>

<file path="ui/web/sentence/compound_part_viewmodel.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from anki.notes import NoteId
    from note.sentences.sentence_configuration import SentenceConfiguration
    from note.vocabulary.vocabnote import VocabNote

class CompoundPartViewModel(AutoSlots):
    def __init__(self, vocab_note: VocabNote, depth: int, config: SentenceConfiguration) -> None:
        self.vocab_note: VocabNote = vocab_note
        self.depth: int = depth
        self.question: str = vocab_note.get_question()
        self.answer: str = vocab_note.get_answer()
        self.readings: str = ", ".join(vocab_note.readings.get())
        self.audio_path: str = vocab_note.audio.get_primary_audio_path()
        self.meta_tags_html: str = vocab_note.meta_data.meta_tags_html(no_sentense_statistics=True)
        self.display_readings: bool = self.question != self.readings
        self.is_highlighted: bool = self.question in config.highlighted_words

        self.meta_tags_string: str = " ".join(vocab_note.get_meta_tags())
        self.meta_tags_string += f""" depth_{depth}"""
        self.meta_tags_string += " highlighted" if self.is_highlighted else ""

    @classmethod
    def get_compound_parts_recursive(cls, vocab_note: VocabNote, config: SentenceConfiguration, depth: int = 0, visited: set[NoteId] | None = None) -> list[CompoundPartViewModel]:
        if not app.config().show_compound_parts_in_sentence_breakdown.get_value(): return []
        if visited is None: visited = set()
        if vocab_note.get_id() in visited: return []

        visited.add(vocab_note.get_id())

        result: list[CompoundPartViewModel] = []

        for part in vocab_note.compound_parts.primary_parts_notes(): #ex_sequence.flatten([app.col().vocab.with_form_prefer_exact_match(part) for part in vocab_note.compound_parts.primary()])
            wrapper = CompoundPartViewModel(part, depth, config)
            result.append(wrapper)
            nested_parts = cls.get_compound_parts_recursive(part, config, depth + 1, visited)
            result.extend(nested_parts)

        return result
</file>

<file path="ui/web/sentence/match_viewmodel.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ankiutils import app
from ex_autoslot import AutoSlots
from language_services.janome_ex.word_extraction.matches.vocab_match import VocabMatch
from queryablecollections.q_iterable import query
from sysutils import kana_utils, typed
from sysutils.debug_repr_builder import SkipFalsyValuesDebugReprBuilder
from sysutils.simple_string_list_builder import SimpleStringListBuilder
from ui.web.sentence.compound_part_viewmodel import CompoundPartViewModel

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.matches.match import Match
    from note.sentences.sentence_configuration import SentenceConfiguration
    from sysutils.weak_ref import WeakRef
    from ui.web.sentence.candidate_word_variant_viewmodel import CandidateWordVariantViewModel

class MatchViewModel(AutoSlots):
    def __init__(self, word_variant_vm: WeakRef[CandidateWordVariantViewModel], match: Match) -> None:
        self.match: Match = match
        self.vocab_match: VocabMatch | None = typed.try_cast(VocabMatch, match)
        self._config: SentenceConfiguration = word_variant_vm().candidate_word.word.analysis.configuration
        self.word_variant_vm: WeakRef[CandidateWordVariantViewModel] = word_variant_vm
        self.is_shadowed: bool = word_variant_vm().is_shadowed
        self.is_display_word: bool = word_variant_vm().is_display_word
        self.parsed_form: str = match.parsed_form
        self.answer: str = match.answer
        self.vocab_form: str = match.match_form
        self.compound_parts: list[CompoundPartViewModel] = []
        self.audio_path: str = ""
        self.is_highlighted: bool = self.parsed_form in self._config.highlighted_words or self.vocab_form in self._config.highlighted_words
        self.readings: str = ", ".join(match.readings)
        self.meta_tags_html: str = ""
        self._meta_tags: list[str] = []
        self.display_vocab_form: bool = self.parsed_form != self.vocab_form
        self.match_owns_form: bool = self.parsed_form == self.vocab_form
        self.display_readings: bool = self.parsed_form != self.readings
        if self.vocab_match is not None:
            self.compound_parts = CompoundPartViewModel.get_compound_parts_recursive(self.vocab_match.vocab, self._config)
            self.audio_path = self.vocab_match.vocab.audio.get_primary_audio_path()
            self._meta_tags = list(self.vocab_match.vocab.get_meta_tags())
            self.meta_tags_html = self.vocab_match.vocab.meta_data.meta_tags_html(display_extended_sentence_statistics=False)
            self.match_owns_form = self.vocab_match.vocab.forms.is_owned_form(self.parsed_form)
            if self.parsed_form != self.vocab_form:
                self.display_vocab_form = True
                self.display_readings = self.display_readings and self.vocab_form != self.readings

    @property
    def meta_tags_string(self) -> str: return " ".join(self.meta_tags_list)

    @property
    def meta_tags_list(self) -> list[str]:
        return (SimpleStringListBuilder()
                .concat(self._meta_tags)
                .append_if(self.is_highlighted, "highlighted")
                .concat(self.hiding_reasons)
                .value)

    @property
    def incorrect_reasons(self) -> list[str]: return list(self.match.failure_reasons)

    @property
    def hiding_reasons(self) -> list[str]: return list(self.match.hiding_reasons)

    @property
    def is_displayed(self) -> bool:
        if app.config().show_sentence_breakdown_in_edit_mode.get_value(): return True
        #todo: this absolutely does not belong here. This is a viewmodel for crying out loud, it should not be implementing core domain logic.
        return (not self.is_shadowed
                and self.is_display_word
                and self.match.is_displayed)

    @property
    def kanji(self) -> list[str]:
        sequence = kana_utils.extract_kanji(self.parsed_form + self.vocab_form)
        return query(sequence).distinct().to_list()

    @override
    def __repr__(self) -> str:
        return (
            SkipFalsyValuesDebugReprBuilder()
            .include(self.parsed_form)
            .flag("shadowed", self.is_shadowed)
            .flag("is_display_word", self.is_display_word)
            .flag("displayed", self.is_displayed).repr)
</file>

<file path="ui/web/sentence/question.py">
from __future__ import annotations

from ankiutils import app
from aqt import gui_hooks
from note.notefields.sentence_question_field import SentenceQuestionField
from note.sentences.sentencenote import SentenceNote
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer


def render_wbr(question: str) -> str:
    return question.replace(SentenceQuestionField.word_break_tag, "<span class='wbr_tag'>&lt;wbr&gt;</span>") \
        if app.config().show_sentence_breakdown_in_edit_mode.get_value() \
        else question

def render_user_question(note: SentenceNote) -> str:
    return render_wbr(note.user.question.value)

def render_source_question(note: SentenceNote) -> str:
    return render_wbr(note.source_question.value)

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(SentenceNote, {
        "##USER_QUESTION##": render_user_question,
        "##SOURCE_QUESTION##": render_source_question
    }).render)
</file>

<file path="ui/web/sentence/sentence_viewmodel.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from ui.web.sentence.text_analysis_viewmodel import TextAnalysisViewModel

if TYPE_CHECKING:
    from note.sentences.sentencenote import SentenceNote
    from ui.web.sentence.match_viewmodel import MatchViewModel

class SentenceViewModel(AutoSlots):
    def __init__(self, sentence: SentenceNote) -> None:
        self.sentence: SentenceNote = sentence
        self.analysis: TextAnalysisViewModel = TextAnalysisViewModel(sentence.create_analysis())
        self.displayed_matches:list[MatchViewModel] = self.analysis.displayed_matches
</file>

<file path="ui/web/sentence/text_analysis_viewmodel.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ex_autoslot import AutoSlots
from ui.web.sentence.candidate_word_variant_viewmodel import CandidateWordVariantViewModel

if TYPE_CHECKING:
    from language_services.janome_ex.word_extraction.text_analysis import TextAnalysis
    from queryablecollections.collections.q_list import QList
    from ui.web.sentence.match_viewmodel import MatchViewModel


class TextAnalysisViewModel(AutoSlots):
    def __init__(self, text_analysis: TextAnalysis) -> None:
        self.analysis: TextAnalysis = text_analysis
        self.candidate_words: QList[CandidateWordVariantViewModel] = text_analysis.indexing_word_variants.select(CandidateWordVariantViewModel).to_list() #[CandidateWordVariantViewModel(candidate_word) for candidate_word in text_analysis.all_word_variants]
        variant_view_models: QList[CandidateWordVariantViewModel] = self.candidate_words
        matches: QList[MatchViewModel] = variant_view_models.select_many(lambda variant_vm: variant_vm.matches).to_list() #ex_sequence.flatten([cand.matches for cand in variant_view_models])
        self.displayed_matches:QList[MatchViewModel] = matches.where(lambda match: match.is_displayed).to_list() #[display_form for display_form in matches if display_form.is_displayed]
</file>

<file path="ui/web/sentence/ud_sentence_breakdown.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app
from aqt import gui_hooks
from note.sentences.sentencenote import SentenceNote
from sysutils import ex_str
from sysutils.ex_str import newline
from ui.web.sentence.sentence_viewmodel import SentenceViewModel
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer
from viewmodels.kanji_list import sentence_kanji_list_viewmodel

if TYPE_CHECKING:
    from ui.web.sentence.match_viewmodel import MatchViewModel

def build_invalid_for_display_span(view_model: MatchViewModel) -> str:
    if not view_model.incorrect_reasons and not view_model.hiding_reasons: return ""

    incorrect_reasons = [f"""<div class="incorrect_reason">{reason}</div>""" for reason in view_model.incorrect_reasons]
    hiding_reasons = [f"""<div class="hiding_reason">{reason}</div>""" for reason in view_model.hiding_reasons]
    return f"""<span>{newline.join(incorrect_reasons + hiding_reasons)}</span>"""

def render_match_kanji(match: MatchViewModel) -> str:
    if not match.kanji or not app.config().show_kanji_in_sentence_breakdown.get_value():
        return ""

    viewmodel = sentence_kanji_list_viewmodel.create(match.kanji)

    return f"""
<div class="vocab_kanji_list">
{ex_str.newline.join(f'''
    <div class="kanji_item {" ".join(kanji.kanji.get_meta_tags())}">
        <div class="kanji_main">
            <span class="kanji_kanji clipboard">{kanji.question()}</span>
            <span class="kanji_answer">{kanji.answer()}</span>
            <span class="kanji_readings">{kanji.readings()}</span>
        </div>
        <div class="kanji_mnemonic">{kanji.mnemonic()}</div>
    </div>
''' for kanji in viewmodel.kanji_list)}
</div>
        """

def render_sentence_analysis(note: SentenceNote) -> str:
    sentence_analysis: SentenceViewModel = SentenceViewModel(note)
    html = """
    <div class="breakdown page_section">
        <div class="page_section_title">Sentence breakdown</div>
        <ul class="sentenceVocabList userExtra depth1">
    """

    for match in sentence_analysis.displayed_matches:
        html += f"""
                    <li class="sentenceVocabEntry depth1 word_priority_very_high {match.meta_tags_string}">
                        <div class="sentenceVocabEntryDiv">
                            {build_invalid_for_display_span(match)}
                            <audio src="{match.audio_path}"></audio><a class="play-button"></a>
                            <span class="vocabQuestion clipboard">{match.parsed_form}</span>
                            {f'''<span class="vocabHitForm clipboard">{match.vocab_form}</span>''' if match.display_vocab_form else ""}
                            {f'''<span class="vocabHitReadings clipboard">{match.readings}</span>''' if match.display_readings else ""}
                            {match.meta_tags_html}
                            <span class="vocabAnswer">{match.answer}</span>
                        </div>
                    </li>
                    """

        for compound_part in match.compound_parts:
            html += f"""
                        <li class="sentenceVocabEntry compound_part {compound_part.meta_tags_string}">
                            <div class="sentenceVocabEntryDiv">
                                <audio src="{compound_part.audio_path}"></audio><a class="play-button"></a>
                                <span class="vocabQuestion clipboard">{compound_part.question}</span>
                                {f'''<span class="vocabHitReadings clipboard">{compound_part.readings}</span>''' if compound_part.display_readings else ""}
                                {compound_part.meta_tags_html}
                                <span class="vocabAnswer">{compound_part.answer}</span>
                            </div>
                        </li>
                        """

        html += f"""
        <li class="sentenceVocabEntry depth1 word_priority_very_high {match.meta_tags_string}">
            {render_match_kanji(match)}
        </li>
"""
    html += """</ul>
            </div>
        """
    return html

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(SentenceNote, {
        "##SENTENCE_ANALYSIS##": render_sentence_analysis
    }).render)
</file>

<file path="ui/web/vocab/__init__.py">
from __future__ import annotations

from ui.web.vocab import vocab_kanji_list


def init() -> None:
    from . import compound_parts, related_vocabs, vocab_sentences
    vocab_sentences.init()
    related_vocabs.init()
    compound_parts.init()
    vocab_kanji_list.init()
</file>

<file path="ui/web/vocab/compound_parts.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from aqt import gui_hooks
from ex_autoslot import AutoSlots
from note.vocabulary.vocabnote import VocabNote
from sysutils.ex_str import newline
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer

if TYPE_CHECKING:
    from anki.notes import NoteId

class CompoundPart(AutoSlots):
    def __init__(self, vocab_note: VocabNote, depth: int = 0) -> None:
        self.vocab_note: VocabNote = vocab_note
        self.depth: int = depth

def _create_classes(_vocab: VocabNote, depth: int = 0) -> str:
    # noinspection DuplicatedCode
    tags = list(_vocab.meta_data.priority_spec().tags)
    tags.sort()
    classes = " ".join([f"""common_ness_{prio}""" for prio in tags])
    classes += f""" {_vocab.meta_data.priority_spec().priority_string}"""
    classes += " " + " ".join(_vocab.get_meta_tags())
    classes += f" compound_part_depth_{depth}"
    return classes

def render_vocab_list(vocab_list: list[CompoundPart], title: str, css_class: str, reading: bool = True) -> str:
    def render_readings(_vocab_wrapper: CompoundPart) -> str:
        _vocab_note = _vocab_wrapper.vocab_note
        readings = ", ".join(_vocab_note.readings.get())
        return f"""<span class="clipboard vocabReading">{readings}</span>""" if reading and readings != _vocab_note.get_question() else ""

    return f'''
             <div class="relatedVocabListDiv page_section {css_class}">
                <div class="page_section_title">{title}</div>
                <div class="vocabHomophonesList">
                    <div>
                        {newline.join([f"""
                        <div class="relatedVocab {_create_classes(_vocab_wrapper.vocab_note, _vocab_wrapper.depth)}">
                            <audio src="{_vocab_wrapper.vocab_note.audio.get_primary_audio_path()}"></audio><a class="play-button"></a>
                            <span class="question clipboard">{_vocab_wrapper.vocab_note.get_question()}</span>
                            {render_readings(_vocab_wrapper)}
                            {_vocab_wrapper.vocab_note.meta_data.meta_tags_html(no_sentense_statistics=True)}
                            <span class="meaning"> {_vocab_wrapper.vocab_note.get_answer()}</span>
                        </div>
                        """ for _vocab_wrapper in vocab_list])}
                    </div>
                </div>
            </div>
            '''

def get_compound_parts_recursive(vocab_note: VocabNote, depth: int = 0, visited: set[NoteId] | None = None) -> list[CompoundPart]:
    if visited is None: visited = set()
    if vocab_note.get_id() in visited: return []

    visited.add(vocab_note.get_id())

    compound_parts = vocab_note.compound_parts.primary_parts_notes()  # ex_sequence.flatten([app.col().vocab.with_form_prefer_exact_match(part) for part in vocab_note.compound_parts.primary()])

    result: list[CompoundPart] = []

    for part in compound_parts:
        wrapper = CompoundPart(part, depth)
        result.append(wrapper)
        nested_parts = get_compound_parts_recursive(part, depth + 1, visited)
        result.extend(nested_parts)

    return result

def generate_compounds(_vocab_note: VocabNote) -> str:
    compound_parts = get_compound_parts_recursive(_vocab_note)
    return render_vocab_list(compound_parts, "compound parts", css_class="compound_parts") if compound_parts else ""

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(VocabNote, {
            "##VOCAB_COMPOUNDS##": generate_compounds,
    }).render)
</file>

<file path="ui/web/vocab/related_vocabs.py">
from __future__ import annotations

import note.vocabulary.vocabnote_sorting
from ankiutils import app
from aqt import gui_hooks
from note.vocabulary.vocabnote import VocabNote
from queryablecollections.q_iterable import query
from sysutils.ex_str import newline
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer


def _create_classes(_vocab: VocabNote) -> str:
    # noinspection DuplicatedCode
    tags = list(_vocab.meta_data.priority_spec().tags)
    tags.sort()
    classes = " ".join([f"""common_ness_{prio}""" for prio in tags])
    classes += f""" {_vocab.meta_data.priority_spec().priority_string}"""
    classes += " " + " ".join(_vocab.get_meta_tags())
    return classes

def render_vocab_list(vocab_list: list[VocabNote], title: str, css_class: str, reading: bool = True, no_sentense_statistics: bool = True) -> str:
    def render_readings(_vocab_note: VocabNote) -> str:
        readings = ", ".join(_vocab_note.readings.get())
        return f"""<span class="clipboard vocabReading">{readings}</span>""" if reading and readings != _vocab_note.get_question() else ""

    if len(vocab_list) == 0: return ""

    return f'''
             <div class="relatedVocabListDiv page_section {css_class}">
                <div class="page_section_title">{title}</div>
                <div class="vocabHomophonesList">
                    <div>
                        {newline.join([f"""
                        <div class="relatedVocab {_create_classes(_vocab_note)}">
                            <audio src="{_vocab_note.audio.get_primary_audio_path()}"></audio><a class="play-button"></a>
                            <span class="question clipboard">{_vocab_note.get_question()}</span>
                            {render_readings(_vocab_note)}
                            {_vocab_note.meta_data.meta_tags_html(no_sentense_statistics=no_sentense_statistics)}
                            <span class="meaning"> {_vocab_note.get_answer()}</span>
                        </div>
                        """ for _vocab_note in vocab_list])}
                    </div>
                </div>
            </div>
            '''

def generate_homophones_html_list(vocab_note: VocabNote) -> str:
    homophone_notes = [vocab_note] + note.vocabulary.vocabnote_sorting.sort_vocab_list_by_studying_status(vocab_note.related_notes.homophones_notes())
    return render_vocab_list(homophone_notes, "homophones", css_class="homophones")

def generate_synonyms_meaning_html_list(_vocab_note: VocabNote) -> str:
    synonym_notes = _vocab_note.related_notes.synonyms.notes()
    perfect_synonyms = _vocab_note.related_notes.perfect_synonyms.notes().to_set()
    synonym_notes = query(synonym_notes).where(lambda synonym: synonym not in perfect_synonyms).to_list()
    synonym_notes = note.vocabulary.vocabnote_sorting.sort_vocab_list_by_studying_status(synonym_notes)

    return render_vocab_list(synonym_notes, "synonyms", css_class="similar")

def generate_perfect_synonyms_meaning_html_list(_vocab_note: VocabNote) -> str:
    perfect_synonym_notes = _vocab_note.related_notes.perfect_synonyms.notes()
    perfect_synonym_notes = note.vocabulary.vocabnote_sorting.sort_vocab_list_by_studying_status(perfect_synonym_notes)

    return render_vocab_list(perfect_synonym_notes, "perfect synonyms, answer automatically synced", css_class="similar")

def generate_antonyms_meaning_html_list(_vocab_note: VocabNote) -> str:
    antonym_notes = _vocab_note.related_notes.antonyms.notes()
    antonym_notes = note.vocabulary.vocabnote_sorting.sort_vocab_list_by_studying_status(antonym_notes)

    return render_vocab_list(antonym_notes, "antonyms", css_class="similar")

def generate_see_also_html_list(_vocab_note: VocabNote) -> str:
    see_also = _vocab_note.related_notes.see_also.notes()
    see_also = note.vocabulary.vocabnote_sorting.sort_vocab_list_by_studying_status(see_also)

    return render_vocab_list(see_also, "see also", css_class="similar")

def generate_confused_with_html_list(_vocab_note: VocabNote) -> str:
    vocabs = list(_vocab_note.related_notes.confused_with.get())
    confused_with = app.col().vocab.with_any_form_in_prefer_exact_match(vocabs)
    confused_with = note.vocabulary.vocabnote_sorting.sort_vocab_list_by_studying_status(confused_with)

    return render_vocab_list(confused_with, "confused with", css_class="confused_with")

def generate_ergative_twin_html(_vocab_note: VocabNote) -> str:
    ergative_twin = app.col().vocab.with_form_prefer_exact_match(_vocab_note.related_notes.ergative_twin.get())
    return render_vocab_list(ergative_twin, "ergative twin", css_class="ergative_twin")

def generate_derived_from(_vocab_note: VocabNote) -> str:
    part = _vocab_note.related_notes.derived_from.get()
    derived_from = app.col().vocab.with_form_prefer_exact_match(part)
    return render_vocab_list(derived_from, "derived from", css_class="derived_from")

def generate_in_compounds_list(_vocab_note: VocabNote) -> str:
    in_compounds = app.col().vocab.with_compound_part(_vocab_note.question.without_noise_characters)[:30]
    return render_vocab_list(in_compounds, "part of compound", css_class="in_compound_words")

def generate_stem_in_compounds_list(_vocab_note: VocabNote) -> str:
    if _vocab_note.question.stems().masu_stem() is None: return ""
    masu_stem_in_compounds = app.col().vocab.with_compound_part(_vocab_note.question.stems().masu_stem() or "")[:30]
    return render_vocab_list(masu_stem_in_compounds, "masu stem is part of compound", css_class="in_compound_words")

def generate_derived_list(_vocab_note: VocabNote) -> str:
    derived_vocabs = app.col().vocab.derived_from(_vocab_note.get_question())
    return render_vocab_list(derived_vocabs, "derived vocabulaty", css_class="derived_vocabulary")

def generate_stem_vocabs(_vocab_note: VocabNote) -> str:
    return render_vocab_list(_vocab_note.related_notes.stems_notes().to_list(), "conjugation forms", css_class="stem_vocabulary")

def generate_stem_of_vocabs(_vocab_note: VocabNote) -> str:
    return render_vocab_list(app.col().vocab.with_stem(_vocab_note.get_question()), "dictionary form", css_class="is_stem_of")

def generate_forms_list(vocab_note: VocabNote) -> str:
    forms = vocab_note.forms.all_list_notes_by_sentence_count()
    return render_vocab_list(forms, "forms", css_class="forms", no_sentense_statistics=False) if len(forms) > 1 else ""

def generate_meta_tags(vocab_note: VocabNote) -> str:
    return vocab_note.meta_data.meta_tags_html(True)

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(VocabNote, {
        "##FORMS_LIST##": generate_forms_list,
        "##IN_COMPOUNDS##": generate_in_compounds_list,
        "##STEM_IN_COMPOUNDS##": generate_stem_in_compounds_list,
        "##DERIVED_VOCABULARY##": generate_derived_list,
        "##ERGATIVE_TWIN##": generate_ergative_twin_html,
        "##DERIVED_FROM##": generate_derived_from,
        "##HOMOPHONES_LIST##": generate_homophones_html_list,
        "##PERFECT_SYNONYMS_LIST##": generate_perfect_synonyms_meaning_html_list,
        "##SYNONYMS_LIST##": generate_synonyms_meaning_html_list,
        "##SEE_ALSO_LIST##": generate_see_also_html_list,
        "##ANTONYMS_LIST##": generate_antonyms_meaning_html_list,
        "##CONFUSED_WITH_LIST##": generate_confused_with_html_list,
        "##VOCAB_META_TAGS_HTML##": generate_meta_tags,
        "##VOCAB_CLASSES##": _create_classes,
        "##STEM_VOCABULARY##": generate_stem_vocabs,
        "##IS_STEM_OF##": generate_stem_of_vocabs
    }).render)
</file>

<file path="ui/web/vocab/vocab_kanji_list.py">
from __future__ import annotations

from aqt import gui_hooks
from note.vocabulary.vocabnote import VocabNote
from sysutils import ex_str
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer
from viewmodels.kanji_list import sentence_kanji_list_viewmodel


def render_kanji_list_from_kanji(kanjis: list[str]) -> str:
    if not kanjis:
        return ""

    viewmodel = sentence_kanji_list_viewmodel.create(kanjis)

    return f"""
<div id="kanji_list" class="page_section">
    <div class="page_section_title">kanji</div>
{ex_str.newline.join(f'''
    <div class="kanji_item {" ".join(kanji.kanji.get_meta_tags())}">
        <div class="kanji_main">
            <span class="kanji_kanji clipboard">{kanji.question()}</span>
            <span class="kanji_answer">{kanji.answer()}</span>
            <span class="kanji_readings">{kanji.readings()}</span>
        </div>
        <div class="kanji_mnemonic">{kanji.mnemonic()}</div>
    </div>
''' for kanji in viewmodel.kanji_list)}
</div>
        """

def render_vocab_kanji_list(vocab: VocabNote) -> str:
    return render_kanji_list_from_kanji(vocab.kanji.extract_main_form_kanji())

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(VocabNote, {"##KANJI_LIST##": render_vocab_kanji_list}).render)
</file>

<file path="ui/web/vocab/vocab_sentences_vocab_sentence_view_model.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from note.sentences.parsed_word import ParsedMatch
    from note.sentences.parsing_result import ParsingResult
    from note.sentences.sentencenote import SentenceNote
    from note.vocabulary.vocabnote import VocabNote

class VocabSentenceMatchViewModel(AutoSlots):
    def __init__(self, match: ParsedMatch, sentence_view_model: VocabSentenceViewModel) -> None:
        self.match: ParsedMatch = match
        self.sentence_view_model: VocabSentenceViewModel = sentence_view_model

    @property
    def is_displayed(self) -> bool: return self.match.is_displayed
    @property
    def start_index(self) -> int: return self.match.start_index
    @property
    def end_index(self) -> int: return self.match.end_index
    def is_primary_form_of(self, vocab: VocabNote) -> bool: return self.match.parsed_form == vocab.question.without_noise_characters
    @property
    def shaded_by(self) -> ParsedMatch | None:
        shading = [match for match in reversed(self.sentence_view_model.result.parsed_words)
                   if match.is_displayed
                   and match.start_index <= self.start_index <= match.end_index]

        return shading[0] if shading else None

    @property
    def yields_to(self) -> ParsedMatch | None:
        yields_to = [match for match in self.sentence_view_model.result.parsed_words
                     if match.is_displayed
                     and self.start_index < match.start_index <= self.end_index < match.end_index]

        return yields_to[0] if yields_to else None

    @override
    def __repr__(self) -> str: return self.match.__repr__()

class VocabSentenceViewModel(AutoSlots):
    def __init__(self, _vocab_note: VocabNote, sentence_note: SentenceNote) -> None:
        self.vocab: VocabNote = _vocab_note
        self.sentence: SentenceNote = sentence_note
        self.result: ParsingResult = sentence_note.parsing_result.get()
        self.matches: list[VocabSentenceMatchViewModel] = [VocabSentenceMatchViewModel(match, self) for match in self.result.parsed_words if match.vocab_id == _vocab_note.get_id()]
        self.displayed_matches: list[VocabSentenceMatchViewModel] = [match for match in self.matches if match.is_displayed]
        self.highlighted_sentences: set[SentenceNote] = set(_vocab_note.sentences.user_highlighted())
        self.shaded_matches: list[VocabSentenceMatchViewModel] = [match for match in self.matches if not match.is_displayed]
        self.matched_vocab_ids: set[int] = {match.vocab_id for match in self.result.parsed_words}

    @property
    def primary_match(self) -> VocabSentenceMatchViewModel: return self.displayed_matches[0] if self.displayed_matches else self.matches[0]

    def format_sentence(self) -> str:
        result = self.result

        match = self.primary_match
        is_primary = "primary" if match.is_primary_form_of(self.vocab) else "secondary"
        match_class = f"{is_primary}FormMatch"

        if match.is_displayed:
            head = result.sentence[:match.start_index]
            match_range = result.sentence[match.start_index:match.end_index]
            tail = result.sentence[match.end_index:]
            return f"""{head}<span class="vocabInContext {match_class}">{match_range}</span>{tail}"""
        else:  # noqa: RET505
            covering_match = non_optional(match.shaded_by or match.yields_to)
            covering_match_class = "compound" if covering_match.vocab_id in self.vocab.related_notes.in_compound_ids else "shadingMatch"
            head = result.sentence[:min(covering_match.start_index, match.start_index)]
            shading_pre_range = result.sentence[covering_match.start_index:match.start_index]
            match_range = result.sentence[match.start_index:match.end_index]
            shading_post_range = result.sentence[match.end_index:covering_match.end_index]
            tail = result.sentence[max(covering_match.end_index, match.end_index):]
            return "".join([head,
                            f"""<span class="vocabInContext {match_class} {covering_match_class}">{shading_pre_range}</span>""",
                            f"""<span class="vocabInContext {match_class}">{match_range}</span>""",
                            f"""<span class="vocabInContext {match_class} {covering_match_class}">{shading_post_range}</span>""",
                            tail])

    def is_highlighted(self) -> bool: return self.sentence in self.highlighted_sentences

    @property
    def vocab_is_displayed(self) -> bool: return any(self.displayed_matches)

    def sentence_classes(self) -> str:
        classes = ""
        if self.sentence in self.highlighted_sentences: classes += "highlighted "
        classes += " ".join(self.sentence.get_meta_tags())
        return classes

    def contains_primary_form(self) -> bool:
        return any(match for match in self.displayed_matches if match.is_primary_form_of(self.vocab))
</file>

<file path="ui/web/vocab/vocab_sentences.py">
from __future__ import annotations

from aqt import gui_hooks
from note.note_constants import Tags
from note.vocabulary.vocabnote import VocabNote
from sysutils.ex_str import newline
from ui.web.vocab.vocab_sentences_vocab_sentence_view_model import VocabSentenceViewModel
from ui.web.web_utils.content_renderer import PrerenderingAnswerContentRenderer


def generate_sentences_list_html(_vocab_note: VocabNote) -> str:
    studying_sentences = set(_vocab_note.sentences.studying())

    def sort_sentences(_sentences: list[VocabSentenceViewModel]) -> list[VocabSentenceViewModel]:

        def prefer_highlighted(_sentence: VocabSentenceViewModel) -> int: return 0 if _sentence.is_highlighted() else 1
        def prefer_studying_read(_sentence: VocabSentenceViewModel) -> int: return 0 if _sentence.sentence.is_studying_read() else 1
        def prefer_studying_listening(_sentence: VocabSentenceViewModel) -> int: return 0 if _sentence.sentence.is_studying_listening() else 1
        def prefer_primary_form(_sentence: VocabSentenceViewModel) -> int: return 0 if _sentence.contains_primary_form() else 1
        def dislike_tts_sentences(_sentence: VocabSentenceViewModel) -> int: return 1 if _sentence.sentence.has_tag(Tags.TTSAudio) else 0
        def prefer_short_questions(_sentence: VocabSentenceViewModel) -> int: return len(_sentence.sentence.get_question())
        def prefer_lower_priority_tag_values(_sentence: VocabSentenceViewModel) -> int: return _sentence.sentence.priority_tag_value()
        def dislike_no_translation(_sentence: VocabSentenceViewModel) -> int: return 1 if not _sentence.sentence.get_answer() else 0

        def prefer_displayed(_sentence: VocabSentenceViewModel) -> int: return 0 if _sentence.vocab_is_displayed else 1

        return sorted(_sentences,
                      key=lambda x: (prefer_highlighted(x),
                                     prefer_studying_read(x),
                                     prefer_studying_listening(x),
                                     prefer_displayed(x),
                                     dislike_no_translation(x),
                                     prefer_lower_priority_tag_values(x),
                                     dislike_tts_sentences(x),
                                     prefer_primary_form(x),
                                     prefer_short_questions(x)))

    sentences = sort_sentences([VocabSentenceViewModel(_vocab_note, sentence_note) for sentence_note in _vocab_note.sentences.all()])
    primary_form_matches = len([x for x in sentences if x.contains_primary_form()])
    sentences = sentences[:30]

    return f'''
             <div id="highlightedSentencesSection" class="page_section {"" if studying_sentences else "no_studying_sentences"}">
                <div class="page_section_title" title="primary form hits: {primary_form_matches}">sentences: primary form hits: {primary_form_matches}, <span class="studing_sentence_count">studying: {len(studying_sentences)}</span></div>
                <div id="highlightedSentencesList">
                    <div>
                        {newline.join([f"""
                        <div class="highlightedSentenceDiv {_sentence.sentence_classes()}">
                            <audio src="{_sentence.sentence.audio.first_audiofile_path()}"></audio><a class="play-button"></a>
                            <div class="highlightedSentence">
                                <div class="sentenceQuestion"><span class="clipboard">{_sentence.format_sentence()}</span> <span class="deck_indicator">{_sentence.sentence.get_source_tag()}</div>
                                <div class="sentenceAnswer"> {_sentence.sentence.get_answer()}</span></div>
                            </div>
                        </div>
                        """ for _sentence in sentences])}
                    </div>
                </div>
            </div>
            ''' if sentences else ""

def init() -> None:
    gui_hooks.card_will_show.append(PrerenderingAnswerContentRenderer(VocabNote, {"##HIGHLIGHTED_SENTENCES##": generate_sentences_list_html}).render)
</file>

<file path="ui/web/web_utils/content_renderer.py">
from __future__ import annotations

from typing import TYPE_CHECKING

from ankiutils import app, ui_utils
from ex_autoslot import AutoSlots
from note.jpnote import JPNote
from note.note_constants import Mine
from sysutils import app_thread_pool
from sysutils.timeutil import StopWatch
from sysutils.typed import non_optional

if TYPE_CHECKING:
    from collections.abc import Callable
    from concurrent.futures import Future

    from anki.cards import Card

class PrerenderingAnswerContentRenderer[TNote: JPNote](AutoSlots):
    def __init__(self, cls: type[TNote], render_methods: dict[str, Callable[[TNote], str]]) -> None:
        self._cls: type[TNote] = cls
        self._render_methods: dict[str, Callable[[TNote], str]] = render_methods
        self._promises: dict[str, Future[str]] | None = None

    @staticmethod
    def _schedule_render_method(_render_method: Callable[[TNote], str], tag: str, note: TNote) -> Future[str]:
        def run_render_method() -> str:
            with StopWatch.log_warning_if_slower_than(0.5, f"rendering:{tag}"):
                return _render_method(note)

        return app_thread_pool.pool.submit(run_render_method if app.is_initialized() else lambda: Mine.app_still_loading_message)

    def render(self, html: str, card: Card, type_of_display: str) -> str:
        note = JPNote.note_from_card(card)

        if isinstance(note, self._cls):
            def schedule_all() -> None:
                self._promises = {tag_: self._schedule_render_method(render_method_, tag_, note) for tag_, render_method_ in self._render_methods.items()}

            def render_scheduled(_html: str) -> str:
                for tag_, content in non_optional(self._promises).items():
                    _html = _html.replace(tag_, content.result())
                self._promises = None
                return _html

            if ui_utils.is_displaytype_displaying_review_question(type_of_display):
                schedule_all()
            elif ui_utils.is_displaytype_displaying_review_answer(type_of_display) and self._promises:
                with StopWatch.log_warning_if_slower_than(0.01, "fetching_results"):
                    html = render_scheduled(html)
            elif ui_utils.is_displaytype_displaying_answer(type_of_display):
                with StopWatch.log_warning_if_slower_than(0.5, "live_rendering"):
                    for tag, render_method in self._render_methods.items():
                        with StopWatch.log_warning_if_slower_than(0.001, f"rendering:{tag}"):
                            html = html.replace(tag, render_method(note) if app.is_initialized() else Mine.app_still_loading_message)

        return html
</file>

<file path="viewmodels/kanji_list/kanji_list_viewmodel.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots

if TYPE_CHECKING:
    from viewmodels.kanji_list.sentence_kanji_viewmodel import KanjiViewModel


class KanjiListViewModel(AutoSlots):
    def __init__(self, kanji_list: list[KanjiViewModel]) -> None:
        self.kanji_list: list[KanjiViewModel] = kanji_list

    @override
    def __str__(self) -> str: return "\n".join([str(kan) for kan in self.kanji_list])
</file>

<file path="viewmodels/kanji_list/sentence_kanji_list_viewmodel.py">
from __future__ import annotations

from ankiutils import app
from viewmodels.kanji_list.kanji_list_viewmodel import KanjiListViewModel
from viewmodels.kanji_list.sentence_kanji_viewmodel import KanjiViewModel


def create(kanji: list[str]) -> KanjiListViewModel:
    kanji_notes = app.col().kanji.with_any_kanji_in(kanji)
    kanji_viewmodels = [KanjiViewModel(note) for note in kanji_notes]
    return KanjiListViewModel(kanji_viewmodels)
</file>

<file path="viewmodels/kanji_list/sentence_kanji_viewmodel.py">
from __future__ import annotations

from typing import TYPE_CHECKING, override

from ex_autoslot import AutoSlots
from sysutils import ex_str, kana_utils

if TYPE_CHECKING:
    from note.kanjinote import KanjiNote


class KanjiViewModel(AutoSlots):
    def __init__(self, kanji: KanjiNote) -> None:
        self.kanji: KanjiNote = kanji

    def question(self) -> str: return self.kanji.get_question()
    def answer(self) -> str: return self.kanji.get_answer()
    def readings(self) -> str:
        readings = f"""{kana_utils.hiragana_to_katakana(self.kanji.get_reading_on_html())} <span class="readingsSeparator">|</span> {self.kanji.get_reading_kun_html()}"""
        if self.kanji.get_reading_nan_html():
            readings += f""" <span class="readingsSeparator">|</span> {self.kanji.get_reading_nan_html()}"""

        return readings

    def mnemonic(self) -> str:
        return self.kanji.get_active_mnemonic()

    @override
    def __str__(self) -> str:
        return f"{self.question()}      {ex_str.pad_to_length(self.answer(), 60)}: {self.readings()}"
</file>

</files>
